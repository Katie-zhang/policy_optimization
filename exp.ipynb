{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from utils.logger import Logger\n",
    "\n",
    "    \n",
    "\n",
    "from utils.io_utils import save_code\n",
    "from utils.logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_dir = f\"./logs/test_{current_time}\"\n",
    "save_code(log_dir)\n",
    "\n",
    "logger = Logger(log_dir)\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 1\n",
    "actions = [-10,0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple(\n",
    "    \"Transition\", [\"state\", \"action_0\", \"action_1\", \"pref\", \"chosen_probs\"]\n",
    ")\n",
    "\n",
    "\n",
    "def sigmoid(x: float):\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "class NonMonotonicScalarToVectorNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NonMonotonicScalarToVectorNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)     # Input layer (1 -> 10)\n",
    "        self.fc2 = nn.Linear(10, 20)    # Hidden layer (10 -> 20)\n",
    "        self.fc3 = nn.Linear(20, 2)     # Output layer (20 -> 2)      \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))      # Using sin activation for non-monotonicity\n",
    "        x = torch.tanh(self.fc2(x))     # Tanh activation adds more non-linearity\n",
    "        x = self.fc3(x)                 # Output layer\n",
    "        return x\n",
    "        \n",
    "\n",
    "def get_score(action_one,action_two,feature_func):\n",
    "    feature_one = feature_func(torch.tensor([[action_one]], dtype=torch.float))[0].detach().numpy()\n",
    "    feature_two = feature_func(torch.tensor([[action_two]], dtype=torch.float))[0].detach().numpy()\n",
    "    score_param =  np.array([[0.0, -1.0],[1, 0]], np.float32) \n",
    "    score = feature_one@score_param@feature_two\n",
    "    return score\n",
    "\n",
    "def get_p(action_one,action_two,feature_func):\n",
    "    feature_one = feature_func(torch.tensor([[action_one]], dtype=torch.float))[0].detach().numpy()\n",
    "    feature_two = feature_func(torch.tensor([[action_two]], dtype=torch.float))[0].detach().numpy()\n",
    "    score_param =  np.array([[0.0, -1.0],[1, 0]], np.float32) \n",
    "    score = feature_one@score_param@feature_two \n",
    "    \n",
    "    temperature = 0.1\n",
    "    p = 1 / (1 + np.exp(-score / temperature))\n",
    "   \n",
    "    return p\n",
    "\n",
    "        \n",
    "        \n",
    "def collect_preference_data(\n",
    "    actions:np.ndarray,\n",
    "    sample_size: int,\n",
    ") -> List[Transition]:\n",
    "    pref_dataset = []\n",
    "    actions = actions\n",
    "    cur_state = np.array([0])   \n",
    "    feature_func = NonMonotonicScalarToVectorNN()\n",
    "    \n",
    "    p_list = np.zeros([len(actions),len(actions)])\n",
    "    for i in range(len(actions)):\n",
    "        for j in range(len(actions)):\n",
    "            action_one = actions[i]\n",
    "            action_two = actions[j]\n",
    "            p = get_p(action_one,action_two,feature_func)\n",
    "            if i==j:\n",
    "                assert p==0.5\n",
    "            p_list[i][j] = p\n",
    "            \n",
    "    for i in range(sample_size):\n",
    "        idx_one, idx_two = np.random.choice(len(actions), 2, replace=False)\n",
    "        action_one = actions[idx_one]\n",
    "        action_two = actions[idx_two]\n",
    "        \n",
    "        bernoulli_param = p_list[idx_one][idx_two]\n",
    "        \n",
    "        if np.random.random() < bernoulli_param:  \n",
    "            transition = Transition(\n",
    "                cur_state, action_one, action_two, 0, p_list[idx_one][idx_two]\n",
    "            )\n",
    "        else:\n",
    "            transition = Transition(\n",
    "                cur_state, action_two, action_one, 1, p_list[idx_two][idx_one]\n",
    "            )\n",
    "        pref_dataset.append(transition)\n",
    "\n",
    "           \n",
    "    return pref_dataset,p_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(5)\n",
    "pref_dataset, p_list = collect_preference_data(actions,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.75489888, 0.44118223],\n",
       "       [0.24510112, 0.5       , 0.83795365],\n",
       "       [0.55881777, 0.16204635, 0.5       ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transition(state=array([0]), action_0=0, action_1=-10, pref=1, chosen_probs=0.24510111981513732),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=10, pref=0, chosen_probs=0.44118222864585355),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=1, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=-10, action_1=10, pref=1, chosen_probs=0.44118222864585355),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=-10, pref=1, chosen_probs=0.24510111981513732),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=-10, pref=1, chosen_probs=0.24510111981513732),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=1, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=0, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=0, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=10, action_1=0, pref=0, chosen_probs=0.16204634591644665),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=10, pref=1, chosen_probs=0.44118222864585355),\n",
       " Transition(state=array([0]), action_0=-10, action_1=10, pref=1, chosen_probs=0.44118222864585355),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=1, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=1, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=1, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=1, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=-10, action_1=10, pref=0, chosen_probs=0.44118222864585355),\n",
       " Transition(state=array([0]), action_0=0, action_1=-10, pref=0, chosen_probs=0.24510111981513732),\n",
       " Transition(state=array([0]), action_0=10, action_1=-10, pref=1, chosen_probs=0.5588177713541465),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=1, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=0, action_1=10, pref=0, chosen_probs=0.8379536540835533),\n",
       " Transition(state=array([0]), action_0=-10, action_1=10, pref=1, chosen_probs=0.44118222864585355),\n",
       " Transition(state=array([0]), action_0=0, action_1=-10, pref=0, chosen_probs=0.24510111981513732),\n",
       " Transition(state=array([0]), action_0=-10, action_1=0, pref=0, chosen_probs=0.7548988801848626)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_index = {-10: 0, 0: 1, 10: 2}\n",
    "\n",
    "states = torch.cat([torch.tensor(x.state) for x in pref_dataset], dim=0)\n",
    "\n",
    "positive_actions = torch.cat(\n",
    "    [torch.tensor(action_to_index[x.action_1] if x.pref == 1 else action_to_index[x.action_0]).unsqueeze(0) for x in pref_dataset],\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "negative_actions = torch.cat(\n",
    "    [torch.tensor(action_to_index[x.action_0] if x.pref == 1 else action_to_index[x.action_1]).unsqueeze(0) for x in pref_dataset],\n",
    "    dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pref_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 1, 2, 2, 0, 0, 0, 2, 1, 1, 2, 1, 0, 1, 0, 2, 0, 0, 1, 2, 2, 2,\n",
       "        2, 2, 1, 0, 1, 2, 2, 1, 1, 0, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 2, 1, 1, 2,\n",
       "        0, 1, 0, 0, 1, 0, 2, 2, 1, 2, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 2, 1, 1, 2, 1, 2, 0, 2, 2, 1, 0, 1, 2, 1, 1, 1, 2, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 2, 0, 0, 0, 2, 1, 2, 1, 0, 1, 1, 1, 2, 2, 0, 2, 1, 2, 0, 1,\n",
       "        1, 2, 2, 2, 0, 2, 1, 1, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, ('-10', '0', '10'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check transition\n",
    "import itertools\n",
    "\n",
    "def check_cyclic_order(matrix):\n",
    "    items = [str(i) for i in actions]\n",
    "    # Generate all permutations of the items\n",
    "    for perm in itertools.permutations(items):\n",
    "        # Extract preferences based on the current permutation\n",
    "        if (matrix[items.index(perm[0])][items.index(perm[1])] > 0.5 and\n",
    "            matrix[items.index(perm[1])][items.index(perm[2])] > 0.5 and\n",
    "            matrix[items.index(perm[2])][items.index(perm[0])] > 0.5):\n",
    "            return True, perm  # Found a cyclic order\n",
    "    return False, None  # No cyclic order found\n",
    "check_cyclic_order(p_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_data_consistency(pref_dataset):\n",
    "    consistent = 0\n",
    "    total = len(pref_dataset)\n",
    "    \n",
    "    for t in pref_dataset:\n",
    "      \n",
    "        if (t.chosen_probs > 0.5):\n",
    "            consistent += 1\n",
    "    \n",
    "    return consistent / total\n",
    "\n",
    "check_data_consistency(pref_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        actions,\n",
    "        action_feature_extractor: nn.Module = None,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        if action_feature_extractor is None:\n",
    "            action_feature_extractor = [\n",
    "                nn.Linear(len(actions), hidden_dim),\n",
    "                nn.Tanh(),\n",
    "            ]\n",
    "            for _ in range(num_layers - 1):\n",
    "                action_feature_extractor.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                action_feature_extractor.append(nn.Tanh())\n",
    "        self.action_feature_extractor = nn.Sequential(*action_feature_extractor).to(self.device)\n",
    "\n",
    "        self.predict_layer = nn.Linear(hidden_dim , 1).to(self.device)\n",
    "\n",
    "    def forward(self, state: torch.tensor, action: torch.tensor) -> torch.tensor:\n",
    "        assert len(state.shape) == len(action.shape)\n",
    "        assert torch.all(action >= 0) and torch.all(action <= 1), f\"{action}\"\n",
    "\n",
    "        \n",
    "        action = torch.as_tensor(action, dtype=torch.float32, device=self.device)\n",
    "\n",
    "       \n",
    "        ha = self.action_feature_extractor(action)\n",
    "\n",
    "        rew = self.predict_layer(ha)\n",
    "       \n",
    "        return rew\n",
    "    \n",
    "\n",
    "\n",
    "class MaximumLikelihoodEstimator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actions: np.ndarray,\n",
    "        reward_model: nn.Module,\n",
    "        learning_rate: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        batch_size: int = 64,\n",
    "        logger: Logger = None,\n",
    "    ):\n",
    "        self.actions = actions\n",
    "        self.reward_model = reward_model\n",
    "        self.batch_size = batch_size\n",
    "        self.logger = logger\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.reward_model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    def optimize_one_epoch(self, states, positive_actions, negative_actions):\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "\n",
    "        k = 0\n",
    "        for i in range(0, len(states), self.batch_size):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            _states = states[i : i + self.batch_size]\n",
    "            _positive_actions = positive_actions[i : i + self.batch_size]\n",
    "            _negative_actions = negative_actions[i : i + self.batch_size]\n",
    "\n",
    "            _positive_actions = F.one_hot(\n",
    "                _positive_actions, num_classes=len(self.actions)\n",
    "            )\n",
    "            _negative_actions = F.one_hot(\n",
    "                _negative_actions, num_classes=len(self.actions)\n",
    "            )\n",
    "            \n",
    "         \n",
    "            _states = _states.unsqueeze(1) if _states.dim() == 1 else _states\n",
    "            \n",
    "\n",
    "            positive_rews = self.reward_model(_states, _positive_actions)\n",
    "            negative_rews = self.reward_model(_states, _negative_actions)\n",
    "\n",
    "            loss = -torch.log(torch.sigmoid(positive_rews - negative_rews)).mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            acc = (positive_rews > negative_rews).float().mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "            k += 1\n",
    "\n",
    "        return total_loss / k, total_acc / k\n",
    "\n",
    "    def optimize(self, states, positive_actions, negative_actions, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            loss, acc = self.optimize_one_epoch(\n",
    "                states, positive_actions, negative_actions\n",
    "            )\n",
    "            if self.logger:\n",
    "                if epoch % 2 == 0:\n",
    "                    self.logger.info(\n",
    "                        f\"[Reward] Epoch {epoch} loss: {loss:.4f} acc: {acc:.2f}\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_reward_model = RewardModel(\n",
    "    state_dim,\n",
    "    actions,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_learner = MaximumLikelihoodEstimator(\n",
    "    actions,\n",
    "    learned_reward_model,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=64,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:22:14,314 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 0 loss: 0.6915 acc: 0.52\n",
      "2024-11-12 09:22:14,320 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 2 loss: 0.6910 acc: 0.52\n",
      "2024-11-12 09:22:14,326 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 4 loss: 0.6905 acc: 0.52\n",
      "2024-11-12 09:22:14,331 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 6 loss: 0.6902 acc: 0.52\n",
      "2024-11-12 09:22:14,336 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 8 loss: 0.6898 acc: 0.55\n",
      "2024-11-12 09:22:14,341 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 10 loss: 0.6896 acc: 0.55\n",
      "2024-11-12 09:22:14,347 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 12 loss: 0.6894 acc: 0.55\n",
      "2024-11-12 09:22:14,352 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 14 loss: 0.6892 acc: 0.55\n",
      "2024-11-12 09:22:14,356 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 16 loss: 0.6891 acc: 0.55\n",
      "2024-11-12 09:22:14,362 - /tmp/ipykernel_3369283/3626585618.py[line:107] - INFO: [Reward] Epoch 18 loss: 0.6890 acc: 0.55\n"
     ]
    }
   ],
   "source": [
    "import time  # for seed\n",
    "\n",
    "torch.manual_seed(time.time())\n",
    "mle_learner.optimize(\n",
    "    states, positive_actions, negative_actions, num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        actions: np.ndarray,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_num = len(actions)\n",
    "\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        network = [nn.Linear(state_dim, hidden_dim), nn.ReLU()]\n",
    "        for _ in range(num_layers - 1):\n",
    "            network.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            network.append(nn.ReLU())\n",
    "        network.append(nn.Linear(hidden_dim, self.action_num))\n",
    "\n",
    "        self.network = nn.Sequential(*network)\n",
    "\n",
    "    def forward(self, state: torch.tensor) -> torch.tensor:\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "        logits = self.network(state)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "class UniformPolicyModel(nn.Module):\n",
    "    def __init__(self, action_num: int, device: str = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.action_num = action_num\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "    def forward(self, state: torch.tensor) -> torch.tensor:\n",
    "        logits = torch.zeros(\n",
    "            [len(state), self.action_num], dtype=torch.float32, device=self.device\n",
    "        )\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "#TODO: other ref models       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: nn.Module,\n",
    "        reward_model: nn.Module,\n",
    "        ref_policy: nn.Module,  # Uniform policy\n",
    "        learning_rate: float = 1e-3,\n",
    "        batch_size: int = 64,\n",
    "        logger: Logger = None,\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.reward_model = reward_model\n",
    "        self.ref_policy = ref_policy\n",
    "        self.batch_size = batch_size\n",
    "        self.logger = logger\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.policy.parameters(), lr=learning_rate\n",
    "        )\n",
    "\n",
    "    def optimize_one_epoch(self, states):\n",
    "        total_loss = 0.0\n",
    "        k = 0\n",
    "        for i in range(0, len(states), self.batch_size):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            _states = states[i : i + self.batch_size]\n",
    "           \n",
    "            distributions = self.policy(_states)\n",
    "            ref_distributions = self.ref_policy(_states)\n",
    "            \n",
    "            \n",
    "            rewards = self.reward_model(_states, distributions)\n",
    "            ref_rewards = self.reward_model(_states, ref_distributions)\n",
    "            \n",
    "            \n",
    "            loss = -torch.sum(distributions * rewards, dim=-1).mean()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            k += 1\n",
    "            \n",
    "        return total_loss / k, rewards.mean().item(), ref_rewards.mean().item()\n",
    "\n",
    "    def optimize(self, states, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            loss, reward, ref_reward = self.optimize_one_epoch(states)\n",
    "            if epoch % 2 == 0:\n",
    "                if self.logger:\n",
    "                    self.logger.info(\n",
    "                        f\"[Policy] Epoch {epoch} \"\n",
    "                        f\"loss: {loss:.4f} \"\n",
    "                        f\"reward: {reward:.4f} \"\n",
    "                        f\"ref_reward: {ref_reward:.4f} \"\n",
    "                        f\"improvement: {(reward-ref_reward)/abs(ref_reward):.2%}\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pg\n",
    "policy = PolicyModel(\n",
    "    state_dim,\n",
    "    actions,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    device=device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_policy = UniformPolicyModel(\n",
    "    action_num=len(actions),\n",
    "    device=device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_policy(torch.tensor([[0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = torch.zeros(len(pref_dataset), 1, dtype=torch.float32).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = PolicyGradientOptimizer(\n",
    "    policy=policy,\n",
    "    reward_model=learned_reward_model,\n",
    "    ref_policy=ref_policy,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:22:14,460 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 0 loss: -0.2123 reward: 0.2123 ref_reward: 0.1991 improvement: 6.59%\n",
      "2024-11-12 09:22:14,468 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 2 loss: -0.2176 reward: 0.2176 ref_reward: 0.1991 improvement: 9.25%\n",
      "2024-11-12 09:22:14,475 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 4 loss: -0.2228 reward: 0.2228 ref_reward: 0.1991 improvement: 11.88%\n",
      "2024-11-12 09:22:14,482 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 6 loss: -0.2282 reward: 0.2282 ref_reward: 0.1991 improvement: 14.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:22:14,489 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 8 loss: -0.2337 reward: 0.2337 ref_reward: 0.1991 improvement: 17.35%\n",
      "2024-11-12 09:22:14,496 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 10 loss: -0.2393 reward: 0.2393 ref_reward: 0.1991 improvement: 20.18%\n",
      "2024-11-12 09:22:14,502 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 12 loss: -0.2450 reward: 0.2450 ref_reward: 0.1991 improvement: 23.05%\n",
      "2024-11-12 09:22:14,509 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 14 loss: -0.2507 reward: 0.2507 ref_reward: 0.1991 improvement: 25.91%\n",
      "2024-11-12 09:22:14,515 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 16 loss: -0.2564 reward: 0.2564 ref_reward: 0.1991 improvement: 28.75%\n",
      "2024-11-12 09:22:14,522 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 18 loss: -0.2619 reward: 0.2619 ref_reward: 0.1991 improvement: 31.53%\n",
      "2024-11-12 09:22:14,529 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 20 loss: -0.2673 reward: 0.2673 ref_reward: 0.1991 improvement: 34.22%\n",
      "2024-11-12 09:22:14,535 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 22 loss: -0.2724 reward: 0.2724 ref_reward: 0.1991 improvement: 36.78%\n",
      "2024-11-12 09:22:14,542 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 24 loss: -0.2772 reward: 0.2772 ref_reward: 0.1991 improvement: 39.18%\n",
      "2024-11-12 09:22:14,549 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 26 loss: -0.2816 reward: 0.2816 ref_reward: 0.1991 improvement: 41.39%\n",
      "2024-11-12 09:22:14,555 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 28 loss: -0.2855 reward: 0.2855 ref_reward: 0.1991 improvement: 43.37%\n",
      "2024-11-12 09:22:14,562 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 30 loss: -0.2890 reward: 0.2890 ref_reward: 0.1991 improvement: 45.12%\n",
      "2024-11-12 09:22:14,569 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 32 loss: -0.2920 reward: 0.2920 ref_reward: 0.1991 improvement: 46.64%\n",
      "2024-11-12 09:22:14,575 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 34 loss: -0.2946 reward: 0.2946 ref_reward: 0.1991 improvement: 47.95%\n",
      "2024-11-12 09:22:14,582 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 36 loss: -0.2968 reward: 0.2968 ref_reward: 0.1991 improvement: 49.05%\n",
      "2024-11-12 09:22:14,588 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 38 loss: -0.2987 reward: 0.2987 ref_reward: 0.1991 improvement: 49.97%\n",
      "2024-11-12 09:22:14,595 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 40 loss: -0.3002 reward: 0.3002 ref_reward: 0.1991 improvement: 50.74%\n",
      "2024-11-12 09:22:14,602 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 42 loss: -0.3014 reward: 0.3014 ref_reward: 0.1991 improvement: 51.37%\n",
      "2024-11-12 09:22:14,608 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 44 loss: -0.3025 reward: 0.3025 ref_reward: 0.1991 improvement: 51.90%\n",
      "2024-11-12 09:22:14,615 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 46 loss: -0.3033 reward: 0.3033 ref_reward: 0.1991 improvement: 52.33%\n",
      "2024-11-12 09:22:14,621 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 48 loss: -0.3041 reward: 0.3041 ref_reward: 0.1991 improvement: 52.68%\n",
      "2024-11-12 09:22:14,628 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 50 loss: -0.3046 reward: 0.3046 ref_reward: 0.1991 improvement: 52.98%\n",
      "2024-11-12 09:22:14,634 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 52 loss: -0.3051 reward: 0.3051 ref_reward: 0.1991 improvement: 53.22%\n",
      "2024-11-12 09:22:14,641 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 54 loss: -0.3055 reward: 0.3055 ref_reward: 0.1991 improvement: 53.43%\n",
      "2024-11-12 09:22:14,648 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 56 loss: -0.3059 reward: 0.3059 ref_reward: 0.1991 improvement: 53.60%\n",
      "2024-11-12 09:22:14,654 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 58 loss: -0.3062 reward: 0.3062 ref_reward: 0.1991 improvement: 53.74%\n",
      "2024-11-12 09:22:14,661 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 60 loss: -0.3064 reward: 0.3064 ref_reward: 0.1991 improvement: 53.86%\n",
      "2024-11-12 09:22:14,667 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 62 loss: -0.3066 reward: 0.3066 ref_reward: 0.1991 improvement: 53.97%\n",
      "2024-11-12 09:22:14,674 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 64 loss: -0.3068 reward: 0.3068 ref_reward: 0.1991 improvement: 54.06%\n",
      "2024-11-12 09:22:14,680 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 66 loss: -0.3069 reward: 0.3069 ref_reward: 0.1991 improvement: 54.14%\n",
      "2024-11-12 09:22:14,687 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 68 loss: -0.3071 reward: 0.3071 ref_reward: 0.1991 improvement: 54.20%\n",
      "2024-11-12 09:22:14,694 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 70 loss: -0.3072 reward: 0.3072 ref_reward: 0.1991 improvement: 54.26%\n",
      "2024-11-12 09:22:14,700 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 72 loss: -0.3073 reward: 0.3073 ref_reward: 0.1991 improvement: 54.31%\n",
      "2024-11-12 09:22:14,707 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 74 loss: -0.3074 reward: 0.3074 ref_reward: 0.1991 improvement: 54.36%\n",
      "2024-11-12 09:22:14,713 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 76 loss: -0.3075 reward: 0.3075 ref_reward: 0.1991 improvement: 54.40%\n",
      "2024-11-12 09:22:14,720 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 78 loss: -0.3075 reward: 0.3075 ref_reward: 0.1991 improvement: 54.44%\n",
      "2024-11-12 09:22:14,727 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 80 loss: -0.3076 reward: 0.3076 ref_reward: 0.1991 improvement: 54.47%\n",
      "2024-11-12 09:22:14,733 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 82 loss: -0.3077 reward: 0.3077 ref_reward: 0.1991 improvement: 54.50%\n",
      "2024-11-12 09:22:14,740 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 84 loss: -0.3077 reward: 0.3077 ref_reward: 0.1991 improvement: 54.53%\n",
      "2024-11-12 09:22:14,746 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 86 loss: -0.3078 reward: 0.3078 ref_reward: 0.1991 improvement: 54.55%\n",
      "2024-11-12 09:22:14,753 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 88 loss: -0.3078 reward: 0.3078 ref_reward: 0.1991 improvement: 54.58%\n",
      "2024-11-12 09:22:14,759 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 90 loss: -0.3079 reward: 0.3079 ref_reward: 0.1991 improvement: 54.60%\n",
      "2024-11-12 09:22:14,766 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 92 loss: -0.3079 reward: 0.3079 ref_reward: 0.1991 improvement: 54.62%\n",
      "2024-11-12 09:22:14,772 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 94 loss: -0.3079 reward: 0.3079 ref_reward: 0.1991 improvement: 54.63%\n",
      "2024-11-12 09:22:14,779 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 96 loss: -0.3080 reward: 0.3080 ref_reward: 0.1991 improvement: 54.65%\n",
      "2024-11-12 09:22:14,785 - /tmp/ipykernel_3369283/201659581.py[line:51] - INFO: [Policy] Epoch 98 loss: -0.3080 reward: 0.3080 ref_reward: 0.1991 improvement: 54.67%\n"
     ]
    }
   ],
   "source": [
    "policy_optimizer.optimize(states=states, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3088], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor([0.1832], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor([0.0883], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "state_0 = torch.tensor([0.0], dtype=torch.float32).to(device)\n",
    "action_0 = torch.tensor([1,0,0], dtype=torch.float32).to(device)\n",
    "action_1 = torch.tensor([0,0,1], dtype=torch.float32).to(device)\n",
    "action_2 = torch.tensor([0,1,0], dtype=torch.float32).to(device)\n",
    "print(learned_reward_model(state_0,action_0))\n",
    "print(learned_reward_model(state_0,action_1))\n",
    "print(learned_reward_model(state_0,action_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(action=-10) = 0.993\n",
      "P(action=0) = 0.003\n",
      "P(action=10) = 0.004\n",
      "\n",
      "Uniform policy probabilities:\n",
      "P(action=-10) = 0.333\n",
      "P(action=0) = 0.333\n",
      "P(action=10) = 0.333\n"
     ]
    }
   ],
   "source": [
    "# output the probabilities of each action from the learned policy\n",
    "test_state = torch.tensor([[0.0]], dtype=torch.float32).to(device)  \n",
    "\n",
    "with torch.no_grad():\n",
    "    action_probs = policy(test_state)\n",
    "    action_probs = action_probs.cpu().numpy()[0]  \n",
    "\n",
    "actions = [-10, 0, 10]  \n",
    "for action, prob in zip(actions, action_probs):\n",
    "    print(f\"P(action={action}) = {prob:.3f}\")\n",
    "\n",
    "# output the probabilities of each action from the reference policy\n",
    "ref_probs = ref_policy(test_state).cpu().numpy()[0]\n",
    "print(\"\\nUniform policy probabilities:\")\n",
    "for action, prob in zip(actions, ref_probs):\n",
    "    print(f\"P(action={action}) = {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectPreferenceOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: nn.Module,\n",
    "        ref_policy: nn.Module,\n",
    "        learning_rate: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        batch_size: int = 64,\n",
    "        beta: float = 1.0,\n",
    "        logger: Logger = None,\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.ref_policy = ref_policy\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.beta = beta\n",
    "        self.logger = logger\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.policy.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    def optimize_one_epoch(\n",
    "        self,\n",
    "        states: torch.tensor,\n",
    "        positive_actions: torch.tensor,\n",
    "        negative_actions: torch.tensor,\n",
    "    ):\n",
    "        total_loss = 0.0\n",
    "        total_gradient_norm = 0.0\n",
    "        k = 0\n",
    "        for i in range(0, len(states), self.batch_size):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            _states = states[i : i + self.batch_size]\n",
    "            distributions = self.policy(_states)\n",
    "            ref_distributions = self.ref_policy(_states)\n",
    "\n",
    "            _positive_actions = positive_actions[i : i + self.batch_size]\n",
    "            _negative_actions = negative_actions[i : i + self.batch_size]\n",
    "\n",
    "            pi_positive_logprobs = distributions[\n",
    "                np.arange(len(_states)), _positive_actions\n",
    "            ]\n",
    "            pi_negative_logprobs = distributions[\n",
    "                np.arange(len(_states)), _negative_actions\n",
    "            ]\n",
    "\n",
    "            ref_positive_logprobs = ref_distributions[\n",
    "                np.arange(len(_states)), _positive_actions\n",
    "            ]\n",
    "            ref_negative_logprobs = ref_distributions[\n",
    "                np.arange(len(_states)), _negative_actions\n",
    "            ]\n",
    "\n",
    "            pi_log_ratios = pi_positive_logprobs - pi_negative_logprobs\n",
    "            ref_log_ratios = ref_positive_logprobs - ref_negative_logprobs\n",
    "\n",
    "            log_ratios = pi_log_ratios - ref_log_ratios\n",
    "\n",
    "            loss = -F.logsigmoid(self.beta * log_ratios).mean()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gradient_norm = 0.0\n",
    "            for p in self.policy.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                gradient_norm += param_norm.item() ** 2\n",
    "            gradient_norm = gradient_norm**0.5\n",
    "            total_gradient_norm += gradient_norm\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        return total_loss / k, total_gradient_norm / k\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        states: torch.tensor,\n",
    "        positive_actions: torch.tensor = None,\n",
    "        negative_actions: torch.tensor = None,\n",
    "        num_epochs: int = 10,\n",
    "    ):\n",
    "        eval_epoch_interval = 5\n",
    "        for epoch in range(num_epochs):\n",
    "            loss, gradient_norm = self.optimize_one_epoch(\n",
    "                states, positive_actions, negative_actions\n",
    "            )\n",
    "            if epoch % eval_epoch_interval == 0:\n",
    "                if self.logger:\n",
    "                    self.logger.info(\n",
    "                        f\"[Policy] Epoch: {epoch} loss: {loss:.4f} grad norm: {gradient_norm:.4f} \"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy2 = PolicyModel(\n",
    "    state_dim,\n",
    "    actions,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    device=device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_optimizer = DirectPreferenceOptimizer(\n",
    "    policy=policy2,\n",
    "    ref_policy=ref_policy,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    beta=1.0,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:22:14,970 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 0 loss: 0.6942 grad norm: 0.0433 \n",
      "2024-11-12 09:22:14,988 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 5 loss: 0.6908 grad norm: 0.0258 \n",
      "2024-11-12 09:22:15,003 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 10 loss: 0.6893 grad norm: 0.0121 \n",
      "2024-11-12 09:22:15,019 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 15 loss: 0.6888 grad norm: 0.0023 \n",
      "2024-11-12 09:22:15,034 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 20 loss: 0.6889 grad norm: 0.0062 \n",
      "2024-11-12 09:22:15,049 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 25 loss: 0.6889 grad norm: 0.0065 \n",
      "2024-11-12 09:22:15,064 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 30 loss: 0.6889 grad norm: 0.0047 \n",
      "2024-11-12 09:22:15,079 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 35 loss: 0.6889 grad norm: 0.0034 \n",
      "2024-11-12 09:22:15,094 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 40 loss: 0.6888 grad norm: 0.0017 \n",
      "2024-11-12 09:22:15,110 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 45 loss: 0.6888 grad norm: 0.0001 \n",
      "2024-11-12 09:22:15,125 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 50 loss: 0.6888 grad norm: 0.0011 \n",
      "2024-11-12 09:22:15,140 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 55 loss: 0.6888 grad norm: 0.0013 \n",
      "2024-11-12 09:22:15,155 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 60 loss: 0.6888 grad norm: 0.0010 \n",
      "2024-11-12 09:22:15,170 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 65 loss: 0.6888 grad norm: 0.0006 \n",
      "2024-11-12 09:22:15,186 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 70 loss: 0.6888 grad norm: 0.0003 \n",
      "2024-11-12 09:22:15,201 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 75 loss: 0.6888 grad norm: 0.0004 \n",
      "2024-11-12 09:22:15,216 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 80 loss: 0.6888 grad norm: 0.0005 \n",
      "2024-11-12 09:22:15,231 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 85 loss: 0.6888 grad norm: 0.0003 \n",
      "2024-11-12 09:22:15,246 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 90 loss: 0.6888 grad norm: 0.0001 \n",
      "2024-11-12 09:22:15,261 - /tmp/ipykernel_3369283/3860663475.py[line:94] - INFO: [Policy] Epoch: 95 loss: 0.6888 grad norm: 0.0001 \n"
     ]
    }
   ],
   "source": [
    "dpo_optimizer.optimize(\n",
    "    states=states,\n",
    "    positive_actions=positive_actions,\n",
    "    negative_actions=negative_actions,\n",
    "    num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned policy probabilities:\n",
      "P(action=-10) = 0.475\n",
      "P(action=0) = 0.215\n",
      "P(action=10) = 0.310\n"
     ]
    }
   ],
   "source": [
    "test_state = torch.zeros(1, 1, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    action_probs = policy2(test_state)\n",
    "    print(\"\\nLearned policy probabilities:\")\n",
    "    for action, prob in zip(actions, action_probs[0].cpu().numpy()):\n",
    "        print(f\"P(action={action}) = {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy3 = PolicyModel(\n",
    "    state_dim,\n",
    "    actions,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    device=device,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayPreferenceOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: nn.Module,\n",
    "        ref_policy: nn.Module,\n",
    "        learning_rate: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        batch_size: int = 64,\n",
    "        eta: float = 1e-4,\n",
    "        logger: Logger = None,\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.ref_policy = ref_policy\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.eta = eta\n",
    "        self.logger = logger\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.policy.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    def optimize_one_epoch(\n",
    "        self,\n",
    "        states: torch.tensor,\n",
    "        positive_actions: torch.tensor,\n",
    "        negative_actions: torch.tensor,\n",
    "        chsoen_probs: torch.tensor,\n",
    "    ):\n",
    "        total_loss = 0.0\n",
    "        total_gradient_norm = 0.0\n",
    "        k = 0\n",
    "        for i in range(0, len(states), self.batch_size):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            _states = states[i : i + self.batch_size]\n",
    "            distributions = self.policy(_states)\n",
    "            ref_distributions = self.ref_policy(_states)\n",
    "\n",
    "            _positive_actions = positive_actions[i : i + self.batch_size]\n",
    "            _negative_actions = negative_actions[i : i + self.batch_size]\n",
    "\n",
    "            pi_positive_logprobs = distributions[\n",
    "                np.arange(len(_states)), _positive_actions\n",
    "            ]\n",
    "            pi_negative_logprobs = distributions[\n",
    "                np.arange(len(_states)), _negative_actions\n",
    "            ]\n",
    "\n",
    "            ref_positive_logprobs = ref_distributions[\n",
    "                np.arange(len(_states)), _positive_actions\n",
    "            ]\n",
    "            ref_negative_logprobs = ref_distributions[\n",
    "                np.arange(len(_states)), _negative_actions\n",
    "            ]\n",
    "\n",
    "            \n",
    "            square_log_w = ((pi_positive_logprobs - ref_positive_logprobs) - self.eta * (chsoen_probs - 1 /2))**2\n",
    "            square_log_l = ((pi_negative_logprobs - ref_negative_logprobs) - self.eta * (1 - chsoen_probs - 1 /2))**2\n",
    "\n",
    "            loss = (square_log_w + square_log_l).mean()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            gradient_norm = 0.0\n",
    "            for p in self.policy.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                gradient_norm += param_norm.item() ** 2\n",
    "            gradient_norm = gradient_norm**0.5\n",
    "            total_gradient_norm += gradient_norm\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        return total_loss / k, total_gradient_norm / k\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        states: torch.tensor,\n",
    "        positive_actions: torch.tensor = None,\n",
    "        negative_actions: torch.tensor = None,\n",
    "        chosen_probs: torch.tensor = None,\n",
    "        num_epochs: int = 10,\n",
    "    ):\n",
    "        eval_epoch_interval = 5\n",
    "        for epoch in range(num_epochs):\n",
    "            loss, gradient_norm = self.optimize_one_epoch(\n",
    "                states, positive_actions, negative_actions,chosen_probs\n",
    "            )\n",
    "            if epoch % eval_epoch_interval == 0:\n",
    "                if self.logger:\n",
    "                    self.logger.info(\n",
    "                        f\"[Policy] Epoch: {epoch} loss: {loss:.4f} grad norm: {gradient_norm:.4f} \"\n",
    "                    )\n",
    "            if epoch % 20 == 0:\n",
    "                self.ref_policy = copy.deepcopy(self.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_probs = torch.tensor([x.chosen_probs for x in pref_dataset], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sppo_optimizer = SelfPlayPreferenceOptimizer(\n",
    "    policy=policy3,\n",
    "    ref_policy=ref_policy,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    eta=1e-4,  \n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 09:22:15,348 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 0 loss: 0.0066 grad norm: 0.0893 \n",
      "2024-11-12 09:22:15,372 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 5 loss: 0.0003 grad norm: 0.0207 \n",
      "2024-11-12 09:22:15,393 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 10 loss: 0.0000 grad norm: 0.0076 \n",
      "2024-11-12 09:22:15,415 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 15 loss: 0.0000 grad norm: 0.0074 \n",
      "2024-11-12 09:22:15,436 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 20 loss: 0.0001 grad norm: 0.0081 \n",
      "2024-11-12 09:22:15,459 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 25 loss: 0.0000 grad norm: 0.0036 \n",
      "2024-11-12 09:22:15,480 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 30 loss: 0.0000 grad norm: 0.0025 \n",
      "2024-11-12 09:22:15,502 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0011 \n",
      "2024-11-12 09:22:15,523 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0020 \n",
      "2024-11-12 09:22:15,546 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0007 \n",
      "2024-11-12 09:22:15,567 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0004 \n",
      "2024-11-12 09:22:15,588 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0004 \n",
      "2024-11-12 09:22:15,610 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0003 \n",
      "2024-11-12 09:22:15,632 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0002 \n",
      "2024-11-12 09:22:15,654 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0001 \n",
      "2024-11-12 09:22:15,675 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0001 \n",
      "2024-11-12 09:22:15,696 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0000 \n",
      "2024-11-12 09:22:15,719 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0001 \n",
      "2024-11-12 09:22:15,740 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0000 \n",
      "2024-11-12 09:22:15,762 - /tmp/ipykernel_3369283/2634107125.py[line:94] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0001 \n"
     ]
    }
   ],
   "source": [
    "sppo_optimizer.optimize(\n",
    "    states=states,\n",
    "    positive_actions=positive_actions,\n",
    "    negative_actions=negative_actions,\n",
    "    chosen_probs=chosen_probs,\n",
    "    num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned policy probabilities:\n",
      "P(action=-10) = 0.255\n",
      "P(action=0) = 0.378\n",
      "P(action=10) = 0.367\n"
     ]
    }
   ],
   "source": [
    "test_state = torch.zeros(1, 1, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    action_probs = policy3(test_state)\n",
    "    print(\"\\nLearned policy probabilities:\")\n",
    "    for action, prob in zip(actions, action_probs[0].cpu().numpy()):\n",
    "        print(f\"P(action={action}) = {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_ref2 = torch.tensor([[0, 0, 1]], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPPO Closed-form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPOClosedForm:\n",
    "   def __init__(\n",
    "       self,\n",
    "       policy: nn.Module,\n",
    "       ref_policy: nn.Module,\n",
    "       eta: float = 1e-4,\n",
    "       batch_size: int = 64,\n",
    "       logger: Logger = None,\n",
    "   ):\n",
    "       self.policy = policy\n",
    "       self.ref_policy = ref_policy\n",
    "       self.eta = eta\n",
    "       self.batch_size = batch_size\n",
    "       self.logger = logger\n",
    "\n",
    "   def compute_pi(\n",
    "       self,\n",
    "       states: torch.tensor,\n",
    "       positive_actions: torch.tensor,\n",
    "       p_list:List[List[float]],\n",
    "   ):\n",
    "    \n",
    "       with torch.no_grad():\n",
    "            new_distributions = []\n",
    "            for i in range(0, len(states)):\n",
    "               state = states[i]\n",
    "               ref_distribution = self.ref_policy(state).squeeze(0)\n",
    "                \n",
    "               p_yi_pi = torch.zeros_like(ref_distribution)\n",
    "               \n",
    "               pref_action = positive_actions[i]\n",
    "               p_pref = ref_distribution[positive_actions[i]]\n",
    "               \n",
    "\n",
    "               p_y_yi = []\n",
    "               for j in range(len(actions) - 1):\n",
    "                   if j != positive_actions[i]:\n",
    "                        p_y_yi.append(p_list[pref_action][j] * ref_distribution[j])\n",
    "                \n",
    "               p_y_yi.append(p_pref * 0.5)     \n",
    "               p_y_yi = sum(p_y_yi)\n",
    "               \n",
    "               exp_term = torch.exp(self.eta * p_yi_pi)\n",
    "                    \n",
    "               Z = torch.sum(ref_distribution * exp_term, dim=-1, keepdim=True)\n",
    "               new_distribution = ref_distribution * exp_term / Z\n",
    "               new_distributions.append(new_distribution) \n",
    "               \n",
    "               \n",
    "            new_distributions = torch.stack(new_distributions)  \n",
    "            \n",
    "               \n",
    "            \n",
    "       return new_distributions\n",
    "   \n",
    "   def optimize(\n",
    "       self,\n",
    "       states: torch.tensor,\n",
    "       positive_actions: torch.tensor,\n",
    "       p_list:List[List[float]],\n",
    "       num_iters: int = 3,\n",
    "   ):\n",
    "       \n",
    "        for iter in range(num_iters):\n",
    "           \n",
    "           new_distributions = self.compute_pi(states, positive_actions, p_list)\n",
    "\n",
    "           ref_distributions = self.ref_policy(states)\n",
    "           print(iter)\n",
    "           print(new_distributions)\n",
    "          \n",
    "        #    if self.logger:\n",
    "        #         self.logger.info(\n",
    "        #             f\"[Policy] Iteration {iter} \"\n",
    "        #             f\"mean policy prob: {new_distributions.mean().item():.4f} \"\n",
    "        #             f\"mean ref prob: {ref_distributions.mean().item():.4f}\"\n",
    "        #         )\n",
    "\n",
    "          \n",
    "           self.ref_policy = copy.deepcopy(self.policy)\n",
    "\n",
    "        return new_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy4 = PolicyModel(\n",
    "    state_dim=1,\n",
    "    actions=actions,\n",
    "    hidden_dim=128,\n",
    "    device=device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sppo = SPPOClosedForm(\n",
    "    policy=policy,\n",
    "    ref_policy=ref_policy,\n",
    "    eta=1e-4,\n",
    "    batch_size=64,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]], device='cuda:0')\n",
      "1\n",
      "tensor([[0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044]], device='cuda:0')\n",
      "2\n",
      "tensor([[0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044],\n",
      "        [0.9929, 0.0027, 0.0044]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sppo_closed_form = sppo.optimize(\n",
    "    states=states,\n",
    "    positive_actions=positive_actions,\n",
    "    p_list=p_list,\n",
    "    num_iters=3\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned policy probabilities(RLHF):\n",
      "P(action=-10) = 0.993\n",
      "P(action=0) = 0.003\n",
      "P(action=10) = 0.004\n",
      "\n",
      "Learned policy probabilities(DPO):\n",
      "P(action=-10) = 0.475\n",
      "P(action=0) = 0.215\n",
      "P(action=10) = 0.310\n",
      "\n",
      "Learned policy probabilities(SPPO):\n",
      "P(action=-10) = 0.255\n",
      "P(action=0) = 0.378\n",
      "P(action=10) = 0.367\n",
      "\n",
      "Learned policy probabilities(SPPO-ClosedForm):\n",
      "P(action=-10) = 0.302\n",
      "P(action=0) = 0.393\n",
      "P(action=10) = 0.305\n"
     ]
    }
   ],
   "source": [
    "test_state = torch.zeros(1, 1, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    action_probs = policy(test_state)\n",
    "    print(\"\\nLearned policy probabilities(RLHF):\")\n",
    "    for action, prob in zip(actions, action_probs[0].cpu().numpy()):\n",
    "        print(f\"P(action={action}) = {prob:.3f}\")\n",
    "\n",
    "    action_probs2 = policy2(test_state)\n",
    "    print(\"\\nLearned policy probabilities(DPO):\")\n",
    "    for action, prob in zip(actions, action_probs2[0].cpu().numpy()):\n",
    "        print(f\"P(action={action}) = {prob:.3f}\")\n",
    "        \n",
    "    action_probs3 = policy3(test_state)\n",
    "    print(\"\\nLearned policy probabilities(SPPO):\")\n",
    "    for action, prob in zip(actions, action_probs3[0].cpu().numpy()):\n",
    "        print(f\"P(action={action}) = {prob:.3f}\")\n",
    "    \n",
    "    action_probs4 = policy4(test_state)\n",
    "    print(\"\\nLearned policy probabilities(SPPO-ClosedForm):\")\n",
    "    for action, prob in zip(actions, action_probs4[0].cpu().numpy()):\n",
    "        print(f\"P(action={action}) = {prob:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pref_with_policy(ret_action_prob, dataset: List[Transition]) -> float:\n",
    "        \"\"\"\n",
    "        Compare the preferences predicted by the policy with the actual preferences in the dataset.\n",
    "        Returns the accuracy of the policy's predictions.\n",
    "        \"\"\"\n",
    "        correct_predictions = 0\n",
    "        for transition in dataset:\n",
    "            state, action_one, action_two, actual_pref, _ = (\n",
    "                transition.state,\n",
    "                transition.action_0,\n",
    "                transition.action_1,\n",
    "                transition.pref,\n",
    "                transition.chosen_probs\n",
    "            )\n",
    "\n",
    "            # Get index of the preferred action and the non-preferred action\n",
    "            action_one_idx = actions.index(action_one)\n",
    "            action_two_idx = actions.index(action_two)\n",
    "           \n",
    "            # Get action probabilities for both actions from the current policy\n",
    "            action_probs = ret_action_prob(state)\n",
    "            prob_one = action_probs[action_one_idx]\n",
    "            prob_two = action_probs[action_two_idx]\n",
    "\n",
    "            # Determine the predicted preferred action\n",
    "            predicted_pref = 1 if prob_two > prob_one else 0\n",
    "\n",
    "            # Compare with actual preference\n",
    "            if predicted_pref == actual_pref:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        # Return the accuracy as the percentage of correct predictions\n",
    "        accuracy = correct_predictions / len(dataset)\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "def plot_model_accuracies(model_names: List[str], accuracies: List[float], filename) -> None:\n",
    "    \"\"\"\n",
    "    Plot the accuracies of different models.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = ['red', 'green', 'blue', 'purple']\n",
    "    plt.bar(model_names, accuracies, color=colors)\n",
    "    plt.ylim(0, 1)  \n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Model Accuracy Comparison\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF accuracy: 0.55\n",
      "DPO accuracy: 0.55\n",
      "SPPO accuracy: 0.45\n",
      "SPPO closed form accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "rlhf_accuracy = compare_pref_with_policy(policy, pref_dataset)\n",
    "print(f\"RLHF accuracy: {rlhf_accuracy:.2f}\")\n",
    "\n",
    "DPO_accuracy = compare_pref_with_policy(policy2, pref_dataset)\n",
    "print(f\"DPO accuracy: {DPO_accuracy:.2f}\")\n",
    "\n",
    "SPPO_accuracy = compare_pref_with_policy(policy3, pref_dataset)\n",
    "print(f\"SPPO accuracy: {SPPO_accuracy:.2f}\")\n",
    "\n",
    "SPPO_closed_form_accuracy = compare_pref_with_policy(policy4, pref_dataset)\n",
    "print(f\"SPPO closed form accuracy: {SPPO_closed_form_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDyUlEQVR4nO3de3zO9f/H8ee188nmsNnMd8z5EDFk5JSaRlqUHKuxkBwiK4kwOhhKlpDyDX2/OSWHzgoliVIOUUkhTWVDsmnY1vb5/dHP9e3q2rSx7fLW4367XTeu9+f9+Xxen+v62J7e1+fzvmyWZVkCAAAADOTm6gIAAACAi0WYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFUKZsNpsmT55c7PUOHz4sm82mxYsXl3hNQEGuu+46XXfdda4uA8DfIMwC/0CLFy+WzWaTzWbTli1bnJZblqWIiAjZbDbdfPPNLqiwZLz99tuy2WwKDw9Xfn6+q8sxTmZmpqZMmaImTZooICBAvr6+atSokcaOHauff/7Z1eUBgCTJw9UFAHAdHx8fLV26VG3btnVo//DDD/Xjjz/K29vbRZWVjCVLligyMlKHDx/W+++/r5iYGFeXZIxDhw4pJiZGqamp6tmzp+655x55eXlpz549evHFF7VmzRp9++23ri6zVL333nuuLgFAETAyC/yD3XTTTVq5cqV+//13h/alS5eqefPmCgsLc1Flly4rK0uvvfaaEhMTFRUVpSVLlri6pEJlZWW5ugQHv//+u2677Talp6dr06ZNWrZsmYYPH67Bgwfr2Wef1aFDh9SzZ09Xl1lqzpw5I0ny8vKSl5eXi6sB8HcIs8A/WN++ffXLL79o/fr19racnBy9+uqr6tevX4HrZGVl6YEHHlBERIS8vb1Vr149PfXUU7Isy6Ffdna2Ro8erZCQEJUrV0633HKLfvzxxwK3+dNPP+nuu+9WaGiovL29ddVVV2nhwoWXdGxr1qzR2bNn1bNnT/Xp00erV6/WuXPnnPqdO3dOkydPVt26deXj46MqVarotttu08GDB+198vPz9cwzz6hx48by8fFRSEiIOnfurM8//1zSha/n/es1wpMnT5bNZtPXX3+tfv36qUKFCvaR8T179mjAgAGqWbOmfHx8FBYWprvvvlu//PJLga/ZwIEDFR4eLm9vb9WoUUNDhw5VTk6ODh06JJvNplmzZjmtt3XrVtlsNi1btqzQ127VqlX64osv9MgjjziN2ktSYGCgnnjiCYe2lStXqnnz5vL19VVwcLDuvPNO/fTTTw59BgwYoICAAKWmpurmm29WQECAqlatqrlz50qS9u7dq+uvv17+/v6qXr26li5d6rD++ctjNm/erCFDhqhSpUoKDAxUfHy8fv31V4e+r732mrp27Wp/fWrVqqXHHntMeXl5Dv2uu+46NWrUSDt27FD79u3l5+en8ePH25f99ZrZZ599VldddZX8/PxUoUIFtWjRwqnOXbt2qUuXLgoMDFRAQIBuuOEGffLJJwUey8cff6zExESFhITI399ft956q44fP17Q2wKgEIRZ4B8sMjJSrVu3dgg277zzjjIyMtSnTx+n/pZl6ZZbbtGsWbPUuXNnPf3006pXr57GjBmjxMREh76DBg1SSkqKbrzxRk2bNk2enp7q2rWr0zbT09PVqlUrbdiwQSNGjNAzzzyj2rVra+DAgUpJSbnoY1uyZIk6duyosLAw9enTR6dPn9Ybb7zh0CcvL08333yzpkyZoubNm2vmzJkaNWqUMjIy9OWXX9r7DRw4UPfff78iIiI0ffp0Pfzww/Lx8XEKKMXRs2dPnTlzRlOnTtXgwYMlSevXr9ehQ4eUkJCgZ599Vn369NHy5ct10003Ofxn4eeff1bLli21fPly9e7dW7Nnz9Zdd92lDz/8UGfOnFHNmjXVpk2bAkejlyxZonLlyqlbt26F1vb6669Lku66664iHcvixYvVq1cvubu7Kzk5WYMHD9bq1avVtm1bnTp1yqFvXl6eunTpooiICM2YMUORkZEaMWKEFi9erM6dO6tFixaaPn26ypUrp/j4eH3//fdO+xsxYoT27dunyZMnKz4+XkuWLFH37t0dXqPFixcrICBAiYmJeuaZZ9S8eXNNmjRJDz/8sNP2fvnlF3Xp0kVNmzZVSkqKOnbsWOBxLliwQCNHjlTDhg2VkpKiKVOmqGnTpvr000/tfb766iu1a9dOX3zxhR566CFNnDhR33//va677jqHfufdd999+uKLL5SUlKShQ4fqjTfe0IgRI4r0ugP4fxaAf5xFixZZkqzPPvvMmjNnjlWuXDnrzJkzlmVZVs+ePa2OHTtalmVZ1atXt7p27Wpfb+3atZYk6/HHH3fY3u23327ZbDbrwIEDlmVZ1u7duy1J1rBhwxz69evXz5JkJSUl2dsGDhxoValSxTpx4oRD3z59+lhBQUH2ur7//ntLkrVo0aK/Pb709HTLw8PDWrBggb3t2muvtbp16+bQb+HChZYk6+mnn3baRn5+vmVZlvX+++9bkqyRI0cW2udCtf31eJOSkixJVt++fZ36nj/WP1u2bJklydq8ebO9LT4+3nJzc7M+++yzQmt6/vnnLUnWvn377MtycnKs4OBgq3///k7r/VlUVJQVFBR0wT5/3mblypWtRo0aWWfPnrW3v/nmm5Yka9KkSfa2/v37W5KsqVOn2tt+/fVXy9fX17LZbNby5cvt7d98843Ta3f+vG3evLmVk5Njb58xY4YlyXrttdfsbQW9lkOGDLH8/Pysc+fO2ds6dOhgSbLmz5/v1L9Dhw5Whw4d7M+7detmXXXVVRd8Pbp37255eXlZBw8etLf9/PPPVrly5az27ds7HUtMTIz9PbMsyxo9erTl7u5unTp16oL7AfA/jMwC/3C9evXS2bNn9eabb+r06dN68803C73E4O2335a7u7tGjhzp0P7AAw/Isiy988479n6SnPrdf//9Ds8ty9KqVasUFxcny7J04sQJ+yM2NlYZGRnauXNnsY9p+fLlcnNzU48ePextffv21TvvvOPwcfSqVasUHBys++67z2kbNpvN3sdmsykpKanQPhfj3nvvdWrz9fW1//3cuXM6ceKEWrVqJUn21yE/P19r165VXFycWrRoUWhNvXr1ko+Pj8Po7LvvvqsTJ07ozjvvvGBtmZmZKleuXJGO4/PPP9exY8c0bNgw+fj42Nu7du2q+vXr66233nJaZ9CgQfa/ly9fXvXq1ZO/v7969eplb69Xr57Kly+vQ4cOOa1/zz33yNPT0/586NCh8vDwsJ93kuNrefr0aZ04cULt2rXTmTNn9M033zhsz9vbWwkJCX97rOXLl9ePP/6ozz77rMDleXl5eu+999S9e3fVrFnT3l6lShX169dPW7ZsUWZmptOx/Pk8ateunfLy8vTDDz/8bT0A/kCYBf7hQkJCFBMTo6VLl2r16tXKy8vT7bffXmDfH374QeHh4U5Bp0GDBvbl5/90c3NTrVq1HPrVq1fP4fnx48d16tQpvfDCCwoJCXF4nA8Xx44dK/Yxvfzyy2rZsqV++eUXHThwQAcOHFBUVJRycnK0cuVKe7+DBw+qXr168vAofGKXgwcPKjw8XBUrVix2HRdSo0YNp7aTJ09q1KhRCg0Nla+vr0JCQuz9MjIyJP3xmmVmZqpRo0YX3H758uUVFxfncD3nkiVLVLVqVV1//fUXXDcwMFCnT58u0nGcf8//+t5KUv369Z1C2flrjv8sKChI//rXv5z+cxAUFOR0Lawk1alTx+F5QECAqlSposOHD9vbvvrqK916660KCgpSYGCgQkJC7CH+/Gt5XtWqVYt0o9fYsWMVEBCgli1bqk6dOho+fLg+/vhj+/Ljx4/rzJkzBb4WDRo0UH5+vo4cOeLQXq1aNYfnFSpUkKQCjxtAwZiaC4D69eunwYMHKy0tTV26dFH58uXLZL/n536988471b9//wL7XH311cXa5nfffWcfOftr6JH+CHT33HNPMSu9sMJGaP96s9Gf/Xnk8LxevXpp69atGjNmjJo2baqAgADl5+erc+fOFzVPbnx8vFauXKmtW7eqcePGev311zVs2DC5uV14HKN+/fratWuXjhw5ooiIiGLv90Lc3d2L1W795cbCojh16pQ6dOigwMBAPfroo6pVq5Z8fHy0c+dOjR071um1LOi9KEiDBg20f/9+vfnmm1q3bp1WrVqlefPmadKkSZoyZUqx65RK9riBfyrCLADdeuutGjJkiD755BOtWLGi0H7Vq1fXhg0bdPr0aYfR2fMf21avXt3+Z35+vn3k87z9+/c7bO/8TAd5eXklNgfskiVL5Onpqf/+979OQWHLli2aPXu2UlNTVa1aNdWqVUuffvqpcnNzHT62/rNatWrp3Xff1cmTJwsdnT0/mvbXm52K81Hxr7/+qo0bN2rKlCmaNGmSvf27775z6BcSEqLAwECHG9QK07lzZ4WEhGjJkiWKjo7WmTNninRTV1xcnJYtW6aXX35Z48aNu2Df8+/5/v37nUZ89+/fb19ekr777juHm7R+++03HT16VDfddJMkadOmTfrll1+0evVqtW/f3t6voJvJisvf31+9e/dW7969lZOTo9tuu01PPPGExo0bp5CQEPn5+Tmd59If/0bc3NxK/D8HALjMAID++Jj2ueee0+TJkxUXF1dov5tuukl5eXmaM2eOQ/usWbNks9nUpUsXSbL/OXv2bId+f52dwN3dXT169NCqVasKDGcXM0XRkiVL1K5dO/Xu3Vu33367w2PMmDGSZJ+9oUePHjpx4oTT8Uj/Gxnr0aOHLMsqcOTtfJ/AwEAFBwdr8+bNDsvnzZtX5LrPB++/jsj99TVzc3NT9+7d9cYbb9inBiuoJkny8PBQ37599corr2jx4sVq3LhxkUa6b7/9djVu3FhPPPGEtm3b5rT89OnTeuSRRyRJLVq0UOXKlTV//nxlZ2fb+7zzzjvat29fgTNYXKoXXnhBubm59ufPPfecfv/9d/t5V9BrmZOTU6z3oyB/nSLNy8tLDRs2lGVZys3Nlbu7u2688Ua99tprDpc8pKen27+cJDAw8JJqAOCMkVkAklTox/x/FhcXp44dO+qRRx7R4cOH1aRJE7333nt67bXXdP/999uvkW3atKn69u2refPmKSMjQ9dee602btyoAwcOOG1z2rRp+uCDDxQdHa3BgwerYcOGOnnypHbu3KkNGzbo5MmTRT6GTz/9VAcOHCh0aqOqVauqWbNmWrJkicaOHav4+Hj95z//UWJiorZv36527dopKytLGzZs0LBhw9StWzd17NhRd911l2bPnq3vvvvO/pH/Rx99pI4dO9r3NWjQIE2bNk2DBg1SixYttHnz5mJ9Q1ZgYKDat2+vGTNmKDc3V1WrVtV7771X4Gji1KlT9d5776lDhw6655571KBBAx09elQrV67Uli1bHC4TiY+P1+zZs/XBBx9o+vTpRarF09NTq1evVkxMjNq3b69evXqpTZs28vT01FdffaWlS5eqQoUKeuKJJ+Tp6anp06crISFBHTp0UN++fZWenq5nnnlGkZGRGj16dJFfg6LKycnRDTfcoF69emn//v2aN2+e2rZtq1tuuUWSdO2116pChQrq37+/Ro4cKZvNpv/+97+X/NH9jTfeqLCwMLVp00ahoaHat2+f5syZo65du9o/qXj88ce1fv16tW3bVsOGDZOHh4eef/55ZWdna8aMGZd87AAK4JI5FAC41J+n5rqQv07NZVmWdfr0aWv06NFWeHi45enpadWpU8d68sknHaYXsizLOnv2rDVy5EirUqVKlr+/vxUXF2cdOXLEaboly/pjKq3hw4dbERERlqenpxUWFmbdcMMN1gsvvGDvU5Spue677z5LksO0SH81efJkS5L1xRdfWJb1xxROjzzyiFWjRg37vm+//XaHbfz+++/Wk08+adWvX9/y8vKyQkJCrC5dulg7duyw9zlz5ow1cOBAKygoyCpXrpzVq1cv69ixY4VOzXX8+HGn2n788Ufr1ltvtcqXL28FBQVZPXv2tH7++ecCX7MffvjBio+Pt0JCQixvb2+rZs2a1vDhw63s7Gyn7V511VWWm5ub9eOPPxb6uhTk119/tSZNmmQ1btzY8vPzs3x8fKxGjRpZ48aNs44ePerQd8WKFVZUVJTl7e1tVaxY0brjjjuc9te/f3/L39/faT8dOnQocMqrv55/58/bDz/80LrnnnusChUqWAEBAdYdd9xh/fLLLw7rfvzxx1arVq0sX19fKzw83HrooYesd99915JkffDBB3+77/PL/jw11/PPP2+1b9/eqlSpkuXt7W3VqlXLGjNmjJWRkeGw3s6dO63Y2FgrICDA8vPzszp27Ght3brVoU9h/wY/+OADpxoBXJjNsrjKHACuZFFRUapYsaI2btzo6lIuyeLFi5WQkKDPPvuswGnJAPwzcc0sAFzBPv/8c+3evVvx8fGuLgUASgXXzALAFejLL7/Ujh07NHPmTFWpUkW9e/d2dUkAUCoYmQWAK9Crr76qhIQE5ebmatmyZQ7fzgUAVxKXhtnNmzcrLi5O4eHhstlsWrt27d+us2nTJjVr1kze3t6qXbu2Fi9eXOp1AoBpJk+erPz8fO3bt08dOnRwdTklYsCAAbIsi+tlAThwaZjNyspSkyZNNHfu3CL1//7779W1a1d17NhRu3fv1v33369Bgwbp3XffLeVKAQAAcDm6bGYzsNlsWrNmjbp3715on7Fjx+qtt95ymFy9T58+OnXqlNatW1cGVQIAAOByYtQNYNu2bXP6ysvY2Fjdf//9ha6TnZ3t8K00+fn5OnnypCpVqlTo96kDAADAdSzL0unTpxUeHi43twtfSGBUmE1LS1NoaKhDW2hoqDIzM3X27Fn5+vo6rZOcnFzg11ACAADg8nbkyBH961//umAfo8LsxRg3bpwSExPtzzMyMlStWjUdOXKE78gGAAC4DGVmZioiIsL+VdEXYlSYDQsLU3p6ukNbenq6AgMDCxyVlSRvb295e3s7tQcGBhJmAQAALmNFuSTUqHlmW7du7fR1jOvXr1fr1q1dVBEAAABcyaVh9rffftPu3bu1e/duSX9MvbV7926lpqZK+uMSgT9/BeO9996rQ4cO6aGHHtI333yjefPm6ZVXXtHo0aNdUT4AAABczKVh9vPPP1dUVJSioqIkSYmJiYqKitKkSZMkSUePHrUHW0mqUaOG3nrrLa1fv15NmjTRzJkz9e9//1uxsbEuqR8AAACuddnMM1tWMjMzFRQUpIyMDK6ZBQAAuAwVJ68Zdc0sAAAA8GeEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWC4Ps3PnzlVkZKR8fHwUHR2t7du3X7B/SkqK6tWrJ19fX0VERGj06NE6d+5cGVULAACAy4lLw+yKFSuUmJiopKQk7dy5U02aNFFsbKyOHTtWYP+lS5fq4YcfVlJSkvbt26cXX3xRK1as0Pjx48u4cgAAAFwOXBpmn376aQ0ePFgJCQlq2LCh5s+fLz8/Py1cuLDA/lu3blWbNm3Ur18/RUZG6sYbb1Tfvn3/djQXAAAAVyaXhdmcnBzt2LFDMTEx/yvGzU0xMTHatm1bgetce+212rFjhz28Hjp0SG+//bZuuummQveTnZ2tzMxMhwcAAACuDB6u2vGJEyeUl5en0NBQh/bQ0FB98803Ba7Tr18/nThxQm3btpVlWfr999917733XvAyg+TkZE2ZMqVEawcAAMDlweU3gBXHpk2bNHXqVM2bN087d+7U6tWr9dZbb+mxxx4rdJ1x48YpIyPD/jhy5EgZVgwAAIDS5LKR2eDgYLm7uys9Pd2hPT09XWFhYQWuM3HiRN11110aNGiQJKlx48bKysrSPffco0ceeURubs7Z3NvbW97e3iV/AAAAAHA5l43Menl5qXnz5tq4caO9LT8/Xxs3blTr1q0LXOfMmTNOgdXd3V2SZFlW6RULAACAy5LLRmYlKTExUf3791eLFi3UsmVLpaSkKCsrSwkJCZKk+Ph4Va1aVcnJyZKkuLg4Pf3004qKilJ0dLQOHDigiRMnKi4uzh5qAQAA8M/h0jDbu3dvHT9+XJMmTVJaWpqaNm2qdevW2W8KS01NdRiJnTBhgmw2myZMmKCffvpJISEhiouL0xNPPOGqQwAAAIAL2ax/2OfzmZmZCgoKUkZGhgIDA11dDgAAAP6iOHnNqNkMAAAAgD8jzAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFguD7Nz585VZGSkfHx8FB0dre3bt1+w/6lTpzR8+HBVqVJF3t7eqlu3rt5+++0yqhYAAACXEw9X7nzFihVKTEzU/PnzFR0drZSUFMXGxmr//v2qXLmyU/+cnBx16tRJlStX1quvvqqqVavqhx9+UPny5cu+eAAAALiczbIsy1U7j46O1jXXXKM5c+ZIkvLz8xUREaH77rtPDz/8sFP/+fPn68knn9Q333wjT0/Pi9pnZmamgoKClJGRocDAwEuqHwAAACWvOHnNZZcZ5OTkaMeOHYqJiflfMW5uiomJ0bZt2wpc5/XXX1fr1q01fPhwhYaGqlGjRpo6dary8vIK3U92drYyMzMdHgAAALgyuCzMnjhxQnl5eQoNDXVoDw0NVVpaWoHrHDp0SK+++qry8vL09ttva+LEiZo5c6Yef/zxQveTnJysoKAg+yMiIqJEjwMAAACu4/IbwIojPz9flStX1gsvvKDmzZurd+/eeuSRRzR//vxC1xk3bpwyMjLsjyNHjpRhxQAAAChNLrsBLDg4WO7u7kpPT3doT09PV1hYWIHrVKlSRZ6ennJ3d7e3NWjQQGlpacrJyZGXl5fTOt7e3vL29i7Z4gEAAHBZcNnIrJeXl5o3b66NGzfa2/Lz87Vx40a1bt26wHXatGmjAwcOKD8/39727bffqkqVKgUGWQAAAFzZXHqZQWJiohYsWKCXXnpJ+/bt09ChQ5WVlaWEhARJUnx8vMaNG2fvP3ToUJ08eVKjRo3St99+q7feektTp07V8OHDXXUIAAAAcCGXzjPbu3dvHT9+XJMmTVJaWpqaNm2qdevW2W8KS01NlZvb//J2RESE3n33XY0ePVpXX321qlatqlGjRmns2LGuOgQAAAC4kEvnmXUF5pkFAAC4vBkxzywAAABwqQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwVrHDbGRkpB599FGlpqaWRj0AAABAkRU7zN5///1avXq1atasqU6dOmn58uXKzs4ujdoAAACAC7qoMLt7925t375dDRo00H333acqVapoxIgR2rlzZ2nUCAAAABTIZlmWdSkbyM3N1bx58zR27Fjl5uaqcePGGjlypBISEmSz2UqqzhKTmZmpoKAgZWRkKDAw0NXlAAAA4C+Kk9c8LnYnubm5WrNmjRYtWqT169erVatWGjhwoH788UeNHz9eGzZs0NKlSy928wAAAMDfKnaY3blzpxYtWqRly5bJzc1N8fHxmjVrlurXr2/vc+utt+qaa64p0UIBAACAvyp2mL3mmmvUqVMnPffcc+revbs8PT2d+tSoUUN9+vQpkQKvCJfh5RYoIZd2lc5Fs03hnLoSWUmuOZ+AkjTFNsXVJaCUJFlJri6hQMUOs4cOHVL16tUv2Mff31+LFi266KIAAACAoij2bAbHjh3Tp59+6tT+6aef6vPPPy+RogAAAICiKHaYHT58uI4cOeLU/tNPP2n48OElUhQAAABQFMUOs19//bWaNWvm1B4VFaWvv/66RIoCAAAAiqLYYdbb21vp6elO7UePHpWHx0XP9AUAAAAUW7HD7I033qhx48YpIyPD3nbq1CmNHz9enTp1KtHiAAAAgAsp9lDqU089pfbt26t69eqKioqSJO3evVuhoaH673//W+IFAgAAAIUpdpitWrWq9uzZoyVLluiLL76Qr6+vEhIS1Ldv3wLnnAUAAABKy0Vd5Orv76977rmnpGsBAAAAiuWi79j6+uuvlZqaqpycHIf2W2655ZKLAgAAAIrior4B7NZbb9XevXtls9lk/f/Xedr+/ytb8/LySrZCAAAAoBDFns1g1KhRqlGjho4dOyY/Pz999dVX2rx5s1q0aKFNmzaVQokAAABAwYo9Mrtt2za9//77Cg4Olpubm9zc3NS2bVslJydr5MiR2rVrV2nUCQAAADgp9shsXl6eypUrJ0kKDg7Wzz//LEmqXr269u/fX7LVAQAAABdQ7JHZRo0a6YsvvlCNGjUUHR2tGTNmyMvLSy+88IJq1qxZGjUCAAAABSp2mJ0wYYKysrIkSY8++qhuvvlmtWvXTpUqVdKKFStKvEAAAACgMMUOs7Gxsfa/165dW998841OnjypChUq2Gc0AAAAAMpCsa6Zzc3NlYeHh7788kuH9ooVKxJkAQAAUOaKFWY9PT1VrVo15pIFAADAZaHYsxk88sgjGj9+vE6ePFka9QAAAABFVuxrZufMmaMDBw4oPDxc1atXl7+/v8PynTt3llhxAAAAwIUUO8x27969FMoAAAAAiq/YYTYpKak06gAAAACKrdjXzAIAAACXi2KPzLq5uV1wGi5mOgAAAEBZKXaYXbNmjcPz3Nxc7dq1Sy+99JKmTJlSYoUBAAAAf6fYYbZbt25ObbfffruuuuoqrVixQgMHDiyRwgAAAIC/U2LXzLZq1UobN24sqc0BAAAAf6tEwuzZs2c1e/ZsVa1atSQ2BwAAABRJsS8zqFChgsMNYJZl6fTp0/Lz89PLL79cosUBAAAAF1LsMDtr1iyHMOvm5qaQkBBFR0erQoUKJVocAAAAcCHFDrMDBgwohTIAAACA4iv2NbOLFi3SypUrndpXrlypl156qUSKAgAAAIqi2GE2OTlZwcHBTu2VK1fW1KlTS6QoAAAAoCiKHWZTU1NVo0YNp/bq1asrNTW1RIoCAAAAiqLYYbZy5cras2ePU/sXX3yhSpUqlUhRAAAAQFEUO8z27dtXI0eO1AcffKC8vDzl5eXp/fff16hRo9SnT5/SqBEAAAAoULFnM3jsscd0+PBh3XDDDfLw+GP1/Px8xcfHc80sAAAAylSxw6yXl5dWrFihxx9/XLt375avr68aN26s6tWrl0Z9AAAAQKGKHWbPq1OnjurUqVOStQAAAADFUuxrZnv06KHp06c7tc+YMUM9e/YskaIAAACAoih2mN28ebNuuukmp/YuXbpo8+bNJVIUAAAAUBTFDrO//fabvLy8nNo9PT2VmZlZIkUBAAAARVHsMNu4cWOtWLHCqX358uVq2LBhiRQFAAAAFEWxbwCbOHGibrvtNh08eFDXX3+9JGnjxo1aunSpXn311RIvEAAAAChMscNsXFyc1q5dq6lTp+rVV1+Vr6+vmjRpovfff18VK1YsjRoBAKXIZnN1BSgtluXqCoDSd1FTc3Xt2lVdu3aVJGVmZmrZsmV68MEHtWPHDuXl5ZVogQAAAEBhin3N7HmbN29W//79FR4erpkzZ+r666/XJ598UpK1AQAAABdUrJHZtLQ0LV68WC+++KIyMzPVq1cvZWdna+3atdz8BQAAgDJX5JHZuLg41atXT3v27FFKSop+/vlnPfvss6VZGwAAAHBBRR6ZfeeddzRy5EgNHTqUr7EFAADAZaHII7NbtmzR6dOn1bx5c0VHR2vOnDk6ceJEadYGAAAAXFCRw2yrVq20YMECHT16VEOGDNHy5csVHh6u/Px8rV+/XqdPny7NOgEAAAAnxZ7NwN/fX3fffbe2bNmivXv36oEHHtC0adNUuXJl3XLLLaVRIwAAAFCgi56aS5Lq1aunGTNm6Mcff9SyZctKqiYAAACgSC4pzJ7n7u6u7t276/XXXy+JzQEAAABFUiJhFgAAAHAFwiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAY10WYXbu3LmKjIyUj4+PoqOjtX379iKtt3z5ctlsNnXv3r10CwQAAMBlyeVhdsWKFUpMTFRSUpJ27typJk2aKDY2VseOHbvgeocPH9aDDz6odu3alVGlAAAAuNy4PMw+/fTTGjx4sBISEtSwYUPNnz9ffn5+WrhwYaHr5OXl6Y477tCUKVNUs2bNMqwWAAAAlxOXhtmcnBzt2LFDMTEx9jY3NzfFxMRo27Ztha736KOPqnLlyho4cODf7iM7O1uZmZkODwAAAFwZXBpmT5w4oby8PIWGhjq0h4aGKi0trcB1tmzZohdffFELFiwo0j6Sk5MVFBRkf0RERFxy3QAAALg8uPwyg+I4ffq07rrrLi1YsEDBwcFFWmfcuHHKyMiwP44cOVLKVQIAAKCseLhy58HBwXJ3d1d6erpDe3p6usLCwpz6Hzx4UIcPH1ZcXJy9LT8/X5Lk4eGh/fv3q1atWg7reHt7y9vbuxSqBwAAgKu5dGTWy8tLzZs318aNG+1t+fn52rhxo1q3bu3Uv379+tq7d692795tf9xyyy3q2LGjdu/ezSUEAAAA/zAuHZmVpMTERPXv318tWrRQy5YtlZKSoqysLCUkJEiS4uPjVbVqVSUnJ8vHx0eNGjVyWL98+fKS5NQOAACAK5/Lw2zv3r11/PhxTZo0SWlpaWratKnWrVtnvyksNTVVbm5GXdoLAACAMmKzLMtydRFlKTMzU0FBQcrIyFBgYGDZ7NRmK5v9oOy56J+PbQrn1JXISnLR+cTpdMVyxY+oKbYpZb9TlIkkK6nM9lWcvMaQJwAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjHVZhNm5c+cqMjJSPj4+io6O1vbt2wvtu2DBArVr104VKlRQhQoVFBMTc8H+AAAAuHK5PMyuWLFCiYmJSkpK0s6dO9WkSRPFxsbq2LFjBfbftGmT+vbtqw8++EDbtm1TRESEbrzxRv30009lXDkAAABczeVh9umnn9bgwYOVkJCghg0bav78+fLz89PChQsL7L9kyRINGzZMTZs2Vf369fXvf/9b+fn52rhxYxlXDgAAAFdzaZjNycnRjh07FBMTY29zc3NTTEyMtm3bVqRtnDlzRrm5uapYsWKBy7Ozs5WZmenwAAAAwJXBpWH2xIkTysvLU2hoqEN7aGio0tLSirSNsWPHKjw83CEQ/1lycrKCgoLsj4iIiEuuGwAAAJcHl19mcCmmTZum5cuXa82aNfLx8Smwz7hx45SRkWF/HDlypIyrBAAAQGnxcOXOg4OD5e7urvT0dIf29PR0hYWFXXDdp556StOmTdOGDRt09dVXF9rP29tb3t7eJVIvAAAALi8uHZn18vJS8+bNHW7eOn8zV+vWrQtdb8aMGXrssce0bt06tWjRoixKBQAAwGXIpSOzkpSYmKj+/furRYsWatmypVJSUpSVlaWEhARJUnx8vKpWrark5GRJ0vTp0zVp0iQtXbpUkZGR9mtrAwICFBAQ4LLjAAAAQNlzeZjt3bu3jh8/rkmTJiktLU1NmzbVunXr7DeFpaamys3tfwPIzz33nHJycnT77bc7bCcpKUmTJ08uy9IBAADgYi4Ps5I0YsQIjRgxosBlmzZtcnh++PDh0i8IAAAARjB6NgMAAAD8sxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADGIswCAADAWIRZAAAAGIswCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAAAAYxFmAQAAYCzCLAAAAIxFmAUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgrMsizM6dO1eRkZHy8fFRdHS0tm/ffsH+K1euVP369eXj46PGjRvr7bffLqNKAQAAcDlxeZhdsWKFEhMTlZSUpJ07d6pJkyaKjY3VsWPHCuy/detW9e3bVwMHDtSuXbvUvXt3de/eXV9++WUZVw4AAABXc3mYffrppzV48GAlJCSoYcOGmj9/vvz8/LRw4cIC+z/zzDPq3LmzxowZowYNGuixxx5Ts2bNNGfOnDKuHAAAAK7m4cqd5+TkaMeOHRo3bpy9zc3NTTExMdq2bVuB62zbtk2JiYkObbGxsVq7dm2B/bOzs5WdnW1/npGRIUnKzMy8xOoBSa46j865ZrcoXfxcQklzxSl1jh9QV6yy/Bl1fl+WZf1tX5eG2RMnTigvL0+hoaEO7aGhofrmm28KXCctLa3A/mlpaQX2T05O1pQpU5zaIyIiLrJq4E+CglxdAa4gQdM4n1Cy+BGFkjQtaFqZ7/P06dMK+psT2aVhtiyMGzfOYSQ3Pz9fJ0+eVKVKlWSz2VxY2ZUpMzNTEREROnLkiAIDA11dDgzH+YSSxPmEksY5VXosy9Lp06cVHh7+t31dGmaDg4Pl7u6u9PR0h/b09HSFhYUVuE5YWFix+nt7e8vb29uhrXz58hdfNIokMDCQf9goMZxPKEmcTyhpnFOl4+9GZM9z6Q1gXl5eat68uTZu3Ghvy8/P18aNG9W6desC12ndurVDf0lav359of0BAABw5XL5ZQaJiYnq37+/WrRooZYtWyolJUVZWVlKSEiQJMXHx6tq1apKTk6WJI0aNUodOnTQzJkz1bVrVy1fvlyff/65XnjhBVceBgAAAFzA5WG2d+/eOn78uCZNmqS0tDQ1bdpU69ats9/klZqaKje3/w0gX3vttVq6dKkmTJig8ePHq06dOlq7dq0aNWrkqkPAn3h7eyspKcnp0g7gYnA+oSRxPqGkcU5dHmxWUeY8AAAAAC5DLv/SBAAAAOBiEWYBAABgLMIsAAAAjEWYBQAAkLRp0ybZbDadOnXqH12DaQizcDBgwADZbDbZbDZ5enqqRo0aeuihh3Tu3P++a9tms2nt2rUFrn+hf4SRkZFKSUlxeH5+X+cf//rXv0r4iHC5+Ou5FRoaqk6dOmnhwoXKz8+39/vzeeHv769mzZpp5cqVDts6efKk7r//flWvXl1eXl4KDw/X3XffrdTU1LI+LJSx48ePa+jQoapWrZq8vb0VFham2NhYffzxx5L+/vyZPHmyfbmHh4ciIyM1evRo/fbbbw77eemll3TNNdfIz89P5cqVU4cOHfTmm2+W6bGa5O/eF+nyeG927dqlnj17KjQ0VD4+PqpTp44GDx6sb7/9tmReiDLC709HhFk46dy5s44ePapDhw5p1qxZev7555WUlFQq+3r00Ud19OhR+2PXrl2lsh9cHs6fW4cPH9Y777yjjh07atSoUbr55pv1+++/2/udPy927dqla665Rr1799bWrVsl/RFkW7VqpQ0bNmj+/Pk6cOCAli9frgMHDuiaa67RoUOHXHV4KAM9evTQrl279NJLL+nbb7/V66+/ruuuu06//PKLvc+Fzh9Juuqqq+zn4fTp0/XCCy/ogQcesC9/8MEHNWTIEPXu3Vt79uzR9u3b1bZtW3Xr1k1z5swp0+M1RVHeF8m1782bb76pVq1aKTs7W0uWLNG+ffv08ssvKygoSBMnTizZF6QMlOTvz9zc3BKszAUs4E/69+9vdevWzaHttttus6KiouzPJVlr1qwpcP0PPvjAkmT9+uuvTsuqV69uzZo1q9DnuLIVdG5ZlmVt3LjRkmQtWLDAsizn8yI3N9fy8/OzHn74YcuyLOvee++1/P39raNHjzps58yZM1bVqlWtzp07l9oxwLV+/fVXS5K1adOmQvv83fmTlJRkNWnSxGGdwYMHW2FhYZZlWda2bdssSdbs2bOdtp2YmGh5enpaqampl34wV5CivC+W5dr3JisrywoODra6d+9e6DFYVsG/w1599VWrYcOGlpeXl1W9enXrqaeeclh37ty5Vu3atS1vb2+rcuXKVo8ePezL8vLyrKlTp1qRkZGWj4+PdfXVV1srV650WP+tt96y6tSpY/n4+FjXXXedtWjRokJ/j573d78/582bZ9WsWdPy9PS06tata/3nP/9xWC7JmjdvnhUXF2f5+flZSUlJ9tf/xRdftCIiIix/f39r6NCh1u+//25Nnz7dCg0NtUJCQqzHH3+80P26CiOzuKAvv/xSW7dulZeXl6tLwRXq+uuvV5MmTbR69eoCl3t4eMjT01M5OTnKz8/X8uXLdccddygsLMyhn6+vr4YNG6Z3331XJ0+eLIvSUcYCAgIUEBCgtWvXKjs7u0jr/Pn8KYyvr699+bJlyxQQEKAhQ4Y49XvggQeUm5urVatWXdwBXKEu5n2Ryva9effdd3XixAk99NBDBS4vX758ge07duxQr1691KdPH+3du1eTJ0/WxIkTtXjxYknS559/rpEjR+rRRx/V/v37tW7dOrVv396+fnJysv7zn/9o/vz5+uqrrzR69Gjdeeed+vDDDyVJR44c0W233aa4uDjt3r1bgwYN0sMPP1zo61EUa9as0ahRo/TAAw/oyy+/1JAhQ5SQkKAPPvjAod/kyZN16623au/evbr77rslSQcPHtQ777yjdevWadmyZXrxxRfVtWtX/fjjj/rwww81ffp0TZgwQZ9++ukl1VjSCLNw8uabbyogIEA+Pj5q3Lixjh07pjFjxhRrG//617/sP+DOPwq6nnHs2LEOfWbPnl1ShwGD1K9fX4cPH3Zqz8nJUXJysjIyMnT99dfr+PHjOnXqlBo0aFDgdho0aCDLsnTgwIFSrhiu4OHhocWLF+ull15S+fLl1aZNG40fP1579uwpsP9fz5+C7NixQ0uXLrUv//bbb1WrVq0C/wMfHh6uwMBA466vLG3FfV+ksn9vvvvuO0l//Kwpjqefflo33HCDJk6cqLp162rAgAEaMWKEnnzySUl/fEupv7+/br75ZlWvXl1RUVEaOXKkJCk7O1tTp07VwoULFRsbq5o1a2rAgAG688479fzzz0uSnnvuOdWqVUszZ85UvXr1dMcdd2jAgAFFqq2w359PPfWUBgwYoGHDhqlu3bpKTEzUbbfdpqeeesph/X79+ikhIUE1a9ZUtWrVJEn5+flauHChGjZsqLi4OHXs2FH79+9XSkqK6tWrp4SEBNWrV88pGLuay7/OFpefjh076rnnnlNWVpZmzZolDw8P9ejRo1jb+Oijj1SuXDmHtuuuu86p35gxYxz+4QYHB19MyTCcZVmy2Wz252PHjtWECRN07tw5BQQEaNq0aeratavS09Pt/fHP1KNHD3Xt2lUfffSRPvnkE73zzjuaMWOG/v3vf9t/lhR2/py3d+9eBQQEKC8vTzk5OeratavD9ZacX8VXlPdFct17c7Hr7du3T926dXNoa9OmjVJSUpSXl6dOnTqpevXqqlmzpjp37qzOnTvr1ltvlZ+fnw4cOKAzZ86oU6dODuvn5OQoKirKvv3o6GiH5a1bty5SbYX9/ty3b5/uuecep5qfeeYZh7YWLVo4bTMyMtLhd3doaKjc3d3l5ubm0Hbs2LEi1VhWCLNw4u/vr9q1a0uSFi5cqCZNmujFF1/UwIEDi7yNGjVqOH1s4+HhfLoFBwfb94V/rn379qlGjRr25+d/SAcEBCg0NNQedENCQlS+fHnt27ev0O3YbDbOqSucj4+POnXqpE6dOmnixIkaNGiQkpKS7L/YCzt/zqtXr55ef/11eXh4KDw83GGkr27dutqyZYtycnKcRgB//vlnZWZmqm7duqV+jCb6u/dFct17c779m2++KXJYLIpy5cpp586d2rRpk9577z1NmjRJkydP1meffWafheGtt95S1apVHdbz9va+5H1f6u9Pf39/pzZPT0+H5+dnn/lr259noLkccJkBLsjNzU3jx4/XhAkTdPbsWVeXgyvQ+++/r7179zqM/p//IR0WFubwy87NzU29evXS0qVLlZaW5rCds2fPat68eYqNjVXFihXLrH64XsOGDZWVlWV/Xtj5c56Xl5dq166tyMhIp1DUp08f/fbbb/aPgf/sqaeekqenZ7E/qfqn+uv7IrnuvbnxxhsVHBysGTNmFLi8sDldGzRo4DC9mCR9/PHHqlu3rtzd3SX9MVATExOjGTNmaM+ePTp8+LDef/99NWzYUN7e3kpNTVXt2rUdHhEREfbtb9++3WH7n3zySYG1FFVhNTds2PCStns5Y2QWf6tnz54aM2aM5s6dqwcffFCS9P3332v37t0O/erUqeOC6mCS7OxspaWlKS8vT+np6Vq3bp2Sk5N18803Kz4+vkjbmDp1qjZu3KhOnTppxowZatSokb7//ntNmDBBubm5mjt3bikfBVzll19+Uc+ePXX33Xfr6quvVrly5fT5559rxowZTh8FX6zWrVtr1KhRGjNmjHJyctS9e3fl5ubq5Zdf1jPPPKOUlBR7EMEfyuJ9kS7tvfH399e///1v9ezZU7fccotGjhyp2rVr68SJE3rllVeUmpqq5cuXO633wAMP6JprrtFjjz2m3r17a9u2bZozZ47mzZsn6Y97TA4dOqT27durQoUKevvtt5Wfn6969eqpXLlyevDBBzV69Gjl5+erbdu2ysjI0Mcff6zAwED1799f9957r2bOnKkxY8Zo0KBB2rFjh/3msos1ZswY9erVS1FRUYqJidEbb7yh1atXa8OGDZe03csZYRZ/y8PDQyNGjNCMGTM0dOhQSVJiYqJTv48++qisS4Nh1q1bpypVqsjDw0MVKlRQkyZNNHv2bPXv39/hmqwLqVSpkj755BM9+uijGjJkiNLS0lSxYkV16dJFL7/8sv1GBlx5AgICFB0drVmzZungwYPKzc1VRESEBg8erPHjx5fYflJSUnT11Vdr3rx5mjBhgtzd3dWsWTOtXbtWcXFxJbafK0VZvS/Spb033bp109atW5WcnKx+/fopMzNTERERuv766/X4448XuE6zZs30yiuvaNKkSXrsscdUpUoVPfroo/ZLJ8qXL6/Vq1dr8uTJOnfunOrUqaNly5bpqquukiQ99thjCgkJUXJysg4dOqTy5curWbNm9telWrVqWrVqlUaPHq1nn31WLVu21NSpU+2zC1yM7t2765lnntFTTz2lUaNGqUaNGlq0aFGB961cKWwWV7oDAADAUFwzCwAAAGMRZgEAAGAswiwAAACMRZgFAACAsQizAAAAMBZhFgAAAMYizAIAAMBYhFkAAAAYizALAFeITZs2yWazFfo98wWJjIxUSkpKqdUEAKWNMAsAZWTAgAGy2Wy69957nZYNHz5cNpvN/jWZAICiIcwCQBmKiIjQ8uXLdfbsWXvbuXPntHTpUlWrVs2FlQGAmQizAFCGmjVrpoiICK1evdretnr1alWrVk1RUVH2tuzsbI0cOVKVK1eWj4+P2rZtq88++8xhW2+//bbq1q0rX19fdezYUYcPH3ba35YtW9SuXTv5+voqIiJCI0eOVFZWVoG1WZalyZMnq1q1avL29lZ4eLhGjhxZMgcOAKWEMAsAZezuu+/WokWL7M8XLlyohIQEhz4PPfSQVq1apZdeekk7d+5U7dq1FRsbq5MnT0qSjhw5ottuu01xcXHavXu3Bg0apIcffthhGwcPHlTnzp3Vo0cP7dmzRytWrNCWLVs0YsSIAutatWqVZs2apeeff17fffed1q5dq8aNG5fw0QNAySLMAkAZu/POO7Vlyxb98MMP+uGHH/Txxx/rzjvvtC/PysrSc889pyeffFJdunRRw4YNtWDBAvn6+urFF1+UJD333HOqVauWZs6cqXr16umOO+5wut42OTlZd9xxh+6//37VqVNH1157rWbPnq3//Oc/OnfunFNdqampCgsLU0xMjKpVq6aWLVtq8ODBpfpaAMClIswCQBkLCQlR165dtXjxYi1atEhdu3ZVcHCwffnBgweVm5urNm3a2Ns8PT3VsmVL7du3T5K0b98+RUdHO2y3devWDs+/+OILLV68WAEBAfZHbGys8vPz9f333zvV1bNnT509e1Y1a9bU4MGDtWbNGv3+++8leegAUOI8XF0AAPwT3X333faP++fOnVsq+/jtt980ZMiQAq97Lehms4iICO3fv18bNmzQ+vXrNWzYMD355JP68MMP5enpWSo1AsClYmQWAFygc+fOysnJUW5urmJjYx2W1apVS15eXvr444/tbbm5ufrss8/UsGFDSVKDBg20fft2h/U++eQTh+fNmjXT119/rdq1azs9vLy8CqzL19dXcXFxmj17tjZt2qRt27Zp7969JXHIAFAqGJkFABdwd3e3XzLg7u7usMzf319Dhw7VmDFjVLFiRVWrVk0zZszQmTNnNHDgQEnSvffeq5kzZ2rMmDEaNGiQduzYocWLFztsZ+zYsWrVqpVGjBihQYMGyd/fX19//bXWr1+vOXPmONW0ePFi5eXlKTo6Wn5+fnr55Zfl6+ur6tWrl86LAAAlgJFZAHCRwMBABQYGFrhs2rRp6tGjh+666y41a9ZMBw4c0LvvvqsKFSpI+uMygVWrVmnt2rVq0qSJ5s+fr6lTpzps4+qrr9aHH36ob7/9Vu3atVNUVJQmTZqk8PDwAvdZvnx5LViwQG3atNHVV1+tDRs26I033lClSpVK9sABoATZLMuyXF0EAAAAcDEYmQUAAICxCLMAAAAwFmEWAAAAxiLMAgAAwFiEWQAAABiLMAsAAABjEWYBAABgLMIsAAAAjEWYBQAAgLEIswAAADAWYRYAAADG+j+uOvSntvobdAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_accuracies(\n",
    "    model_names=[\"RLHF\", \"DPO\", \"SPPO\", \"SPPO Closed Form\"],\n",
    "    accuracies=[rlhf_accuracy, DPO_accuracy, SPPO_accuracy, SPPO_closed_form_accuracy],\n",
    "    filename=\"model_accuracies.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
