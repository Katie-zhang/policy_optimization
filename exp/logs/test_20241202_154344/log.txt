2024-12-02 15:43:44,921 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-02 15:43:44,928 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-02 15:43:44,934 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-02 15:43:44,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-02 15:43:44,947 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-02 15:43:44,954 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-02 15:43:44,960 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-02 15:43:44,966 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-02 15:43:44,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-02 15:43:44,979 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-02 15:43:45,241 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 0 loss: -0.2646 reward: 0.2646 ref_reward: 0.2734 improvement: -3.20%
2024-12-02 15:43:45,661 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 2 loss: -0.2704 reward: 0.2704 ref_reward: 0.2734 improvement: -1.11%
2024-12-02 15:43:46,082 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 4 loss: -0.2759 reward: 0.2759 ref_reward: 0.2734 improvement: 0.91%
2024-12-02 15:43:46,500 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 6 loss: -0.2813 reward: 0.2813 ref_reward: 0.2734 improvement: 2.87%
2024-12-02 15:43:46,921 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 8 loss: -0.2867 reward: 0.2867 ref_reward: 0.2734 improvement: 4.85%
2024-12-02 15:43:47,298 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 10 loss: -0.2920 reward: 0.2920 ref_reward: 0.2734 improvement: 6.79%
2024-12-02 15:43:47,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 12 loss: -0.2973 reward: 0.2973 ref_reward: 0.2734 improvement: 8.74%
2024-12-02 15:43:47,905 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 14 loss: -0.3026 reward: 0.3026 ref_reward: 0.2734 improvement: 10.69%
2024-12-02 15:43:48,203 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 16 loss: -0.3080 reward: 0.3080 ref_reward: 0.2734 improvement: 12.65%
2024-12-02 15:43:48,503 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 18 loss: -0.3133 reward: 0.3133 ref_reward: 0.2734 improvement: 14.59%
2024-12-02 15:43:48,797 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 20 loss: -0.3185 reward: 0.3185 ref_reward: 0.2734 improvement: 16.50%
2024-12-02 15:43:49,097 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 22 loss: -0.3237 reward: 0.3237 ref_reward: 0.2734 improvement: 18.38%
2024-12-02 15:43:49,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 24 loss: -0.3287 reward: 0.3287 ref_reward: 0.2734 improvement: 20.22%
2024-12-02 15:43:49,691 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 26 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 22.00%
2024-12-02 15:43:49,987 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 28 loss: -0.3382 reward: 0.3382 ref_reward: 0.2734 improvement: 23.70%
2024-12-02 15:43:50,284 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 30 loss: -0.3426 reward: 0.3426 ref_reward: 0.2734 improvement: 25.30%
2024-12-02 15:43:50,583 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 32 loss: -0.3467 reward: 0.3467 ref_reward: 0.2734 improvement: 26.82%
2024-12-02 15:43:50,879 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 34 loss: -0.3506 reward: 0.3506 ref_reward: 0.2734 improvement: 28.23%
2024-12-02 15:43:51,176 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 36 loss: -0.3542 reward: 0.3542 ref_reward: 0.2734 improvement: 29.55%
2024-12-02 15:43:51,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 38 loss: -0.3575 reward: 0.3575 ref_reward: 0.2734 improvement: 30.77%
2024-12-02 15:43:51,772 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 40 loss: -0.3606 reward: 0.3606 ref_reward: 0.2734 improvement: 31.90%
2024-12-02 15:43:52,068 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 42 loss: -0.3635 reward: 0.3635 ref_reward: 0.2734 improvement: 32.95%
2024-12-02 15:43:52,365 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 44 loss: -0.3661 reward: 0.3661 ref_reward: 0.2734 improvement: 33.92%
2024-12-02 15:43:52,664 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 46 loss: -0.3685 reward: 0.3685 ref_reward: 0.2734 improvement: 34.79%
2024-12-02 15:43:52,961 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 48 loss: -0.3707 reward: 0.3707 ref_reward: 0.2734 improvement: 35.59%
2024-12-02 15:43:53,259 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 50 loss: -0.3726 reward: 0.3726 ref_reward: 0.2734 improvement: 36.30%
2024-12-02 15:43:53,555 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 52 loss: -0.3743 reward: 0.3743 ref_reward: 0.2734 improvement: 36.92%
2024-12-02 15:43:53,852 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 54 loss: -0.3758 reward: 0.3758 ref_reward: 0.2734 improvement: 37.47%
2024-12-02 15:43:54,148 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 56 loss: -0.3771 reward: 0.3771 ref_reward: 0.2734 improvement: 37.95%
2024-12-02 15:43:54,442 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 58 loss: -0.3783 reward: 0.3783 ref_reward: 0.2734 improvement: 38.36%
2024-12-02 15:43:54,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 60 loss: -0.3792 reward: 0.3792 ref_reward: 0.2734 improvement: 38.71%
2024-12-02 15:43:55,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 62 loss: -0.3801 reward: 0.3801 ref_reward: 0.2734 improvement: 39.01%
2024-12-02 15:43:55,331 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 64 loss: -0.3808 reward: 0.3808 ref_reward: 0.2734 improvement: 39.27%
2024-12-02 15:43:55,628 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 66 loss: -0.3814 reward: 0.3814 ref_reward: 0.2734 improvement: 39.49%
2024-12-02 15:43:55,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 68 loss: -0.3819 reward: 0.3819 ref_reward: 0.2734 improvement: 39.67%
2024-12-02 15:43:56,221 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 70 loss: -0.3823 reward: 0.3823 ref_reward: 0.2734 improvement: 39.83%
2024-12-02 15:43:56,517 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 72 loss: -0.3827 reward: 0.3827 ref_reward: 0.2734 improvement: 39.97%
2024-12-02 15:43:56,814 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 74 loss: -0.3830 reward: 0.3830 ref_reward: 0.2734 improvement: 40.08%
2024-12-02 15:43:57,111 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 76 loss: -0.3833 reward: 0.3833 ref_reward: 0.2734 improvement: 40.18%
2024-12-02 15:43:57,409 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 78 loss: -0.3835 reward: 0.3835 ref_reward: 0.2734 improvement: 40.27%
2024-12-02 15:43:57,706 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 80 loss: -0.3837 reward: 0.3837 ref_reward: 0.2734 improvement: 40.35%
2024-12-02 15:43:58,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 82 loss: -0.3839 reward: 0.3839 ref_reward: 0.2734 improvement: 40.41%
2024-12-02 15:43:58,299 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 84 loss: -0.3840 reward: 0.3840 ref_reward: 0.2734 improvement: 40.47%
2024-12-02 15:43:58,596 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 86 loss: -0.3842 reward: 0.3842 ref_reward: 0.2734 improvement: 40.52%
2024-12-02 15:43:58,892 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 88 loss: -0.3843 reward: 0.3843 ref_reward: 0.2734 improvement: 40.56%
2024-12-02 15:43:59,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 90 loss: -0.3844 reward: 0.3844 ref_reward: 0.2734 improvement: 40.60%
2024-12-02 15:43:59,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 92 loss: -0.3845 reward: 0.3845 ref_reward: 0.2734 improvement: 40.64%
2024-12-02 15:43:59,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 94 loss: -0.3846 reward: 0.3846 ref_reward: 0.2734 improvement: 40.67%
2024-12-02 15:44:00,082 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 96 loss: -0.3847 reward: 0.3847 ref_reward: 0.2734 improvement: 40.70%
2024-12-02 15:44:00,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 98 loss: -0.3848 reward: 0.3848 ref_reward: 0.2734 improvement: 40.73%
2024-12-02 15:44:01,097 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 0.7063 grad norm: 0.4910 
2024-12-02 15:44:01,835 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 0.6647 grad norm: 0.3745 
2024-12-02 15:44:02,571 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.6374 grad norm: 0.2940 
2024-12-02 15:44:03,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.6194 grad norm: 0.2297 
2024-12-02 15:44:04,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.6076 grad norm: 0.1606 
2024-12-02 15:44:04,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.6009 grad norm: 0.0852 
2024-12-02 15:44:05,519 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.5989 grad norm: 0.0166 
2024-12-02 15:44:06,254 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.5993 grad norm: 0.0437 
2024-12-02 15:44:06,993 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.5999 grad norm: 0.0629 
2024-12-02 15:44:07,729 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.5996 grad norm: 0.0566 
2024-12-02 15:44:08,465 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.5992 grad norm: 0.0378 
2024-12-02 15:44:09,205 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.5989 grad norm: 0.0183 
2024-12-02 15:44:09,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0064 
2024-12-02 15:44:10,679 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0107 
2024-12-02 15:44:11,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5989 grad norm: 0.0134 
2024-12-02 15:44:12,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0116 
2024-12-02 15:44:12,889 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0071 
2024-12-02 15:44:13,631 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0024 
2024-12-02 15:44:14,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0022 
2024-12-02 15:44:15,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0037 
2024-12-02 15:44:16,048 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 1.0518 grad norm: 0.8963 
2024-12-02 15:44:16,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 0.9682 grad norm: 0.8279 
2024-12-02 15:44:17,530 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.8979 grad norm: 0.8010 
2024-12-02 15:44:18,269 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.8361 grad norm: 0.7987 
2024-12-02 15:44:19,010 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.7776 grad norm: 0.7846 
2024-12-02 15:44:19,749 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.7219 grad norm: 0.7328 
2024-12-02 15:44:20,489 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.6721 grad norm: 0.6286 
2024-12-02 15:44:21,227 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.6335 grad norm: 0.4704 
2024-12-02 15:44:21,968 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.6097 grad norm: 0.2785 
2024-12-02 15:44:22,708 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.6002 grad norm: 0.1039 
2024-12-02 15:44:23,449 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.5994 grad norm: 0.0777 
2024-12-02 15:44:24,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.6008 grad norm: 0.1306 
2024-12-02 15:44:24,930 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.6014 grad norm: 0.1440 
2024-12-02 15:44:25,670 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.6009 grad norm: 0.1265 
2024-12-02 15:44:26,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5998 grad norm: 0.0884 
2024-12-02 15:44:27,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5990 grad norm: 0.0427 
2024-12-02 15:44:27,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0081 
2024-12-02 15:44:28,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5989 grad norm: 0.0208 
2024-12-02 15:44:29,362 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5989 grad norm: 0.0281 
2024-12-02 15:44:30,104 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5989 grad norm: 0.0263 
2024-12-02 15:44:31,068 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 9.1717 grad norm: 0.9847 
2024-12-02 15:44:31,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 9.0820 grad norm: 0.9093 
2024-12-02 15:44:32,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 8.9980 grad norm: 0.9230 
2024-12-02 15:44:33,287 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 8.9196 grad norm: 0.9831 
2024-12-02 15:44:34,026 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 8.8410 grad norm: 1.0845 
2024-12-02 15:44:34,764 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 8.7552 grad norm: 1.2320 
2024-12-02 15:44:35,503 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 8.6550 grad norm: 1.3978 
2024-12-02 15:44:36,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 8.5371 grad norm: 1.5797 
2024-12-02 15:44:36,977 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 8.3990 grad norm: 1.7768 
2024-12-02 15:44:37,715 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 8.2385 grad norm: 1.9890 
2024-12-02 15:44:38,452 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 8.0534 grad norm: 2.2163 
2024-12-02 15:44:39,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 7.8414 grad norm: 2.4586 
2024-12-02 15:44:39,928 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 7.6002 grad norm: 2.7162 
2024-12-02 15:44:40,671 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 7.3278 grad norm: 2.9891 
2024-12-02 15:44:41,410 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 7.0217 grad norm: 3.2775 
2024-12-02 15:44:42,147 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 6.6799 grad norm: 3.5816 
2024-12-02 15:44:42,887 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 6.2999 grad norm: 3.9015 
2024-12-02 15:44:43,629 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 5.8797 grad norm: 4.2373 
2024-12-02 15:44:44,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 5.4168 grad norm: 4.5893 
2024-12-02 15:44:45,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 4.9090 grad norm: 4.9575 
2024-12-02 15:44:46,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 0.6342 grad norm: 0.2712 
2024-12-02 15:44:46,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 0.6123 grad norm: 0.1796 
2024-12-02 15:44:47,555 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.6012 grad norm: 0.0804 
2024-12-02 15:44:48,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.5990 grad norm: 0.0266 
2024-12-02 15:44:49,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.6006 grad norm: 0.0825 
2024-12-02 15:44:49,768 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.6001 grad norm: 0.0682 
2024-12-02 15:44:50,505 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.5989 grad norm: 0.0216 
2024-12-02 15:44:51,240 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.5989 grad norm: 0.0154 
2024-12-02 15:44:51,977 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.5991 grad norm: 0.0285 
2024-12-02 15:44:52,711 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.5990 grad norm: 0.0220 
2024-12-02 15:44:53,450 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.5988 grad norm: 0.0054 
2024-12-02 15:44:54,186 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0090 
2024-12-02 15:44:54,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.5989 grad norm: 0.0119 
2024-12-02 15:44:55,659 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0054 
2024-12-02 15:44:56,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0023 
2024-12-02 15:44:57,132 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0051 
2024-12-02 15:44:57,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0032 
2024-12-02 15:44:58,606 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0004 
2024-12-02 15:44:59,341 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0022 
2024-12-02 15:45:00,076 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0016 
2024-12-02 15:45:01,672 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 0.0427 grad norm: 0.7967 
2024-12-02 15:45:02,423 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0032 grad norm: 0.1763 
2024-12-02 15:45:03,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0006 grad norm: 0.0646 
2024-12-02 15:45:03,917 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0013 grad norm: 0.0952 
2024-12-02 15:45:04,663 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0009 grad norm: 0.0673 
2024-12-02 15:45:05,412 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0003 grad norm: 0.0390 
2024-12-02 15:45:06,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0004 grad norm: 0.0475 
2024-12-02 15:45:06,906 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0065 
2024-12-02 15:45:07,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0055 
2024-12-02 15:45:08,404 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0046 
2024-12-02 15:45:09,152 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0039 
2024-12-02 15:45:09,900 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0029 
2024-12-02 15:45:10,651 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0023 
2024-12-02 15:45:11,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0005 
2024-12-02 15:45:12,147 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0003 
2024-12-02 15:45:12,895 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0001 
2024-12-02 15:45:13,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0002 
2024-12-02 15:45:14,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0003 
2024-12-02 15:45:15,143 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0003 
2024-12-02 15:45:15,891 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0003 
2024-12-02 15:45:16,850 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 1.7154 grad norm: 3.8191 
2024-12-02 15:45:17,594 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0511 grad norm: 0.8749 
2024-12-02 15:45:18,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0038 grad norm: 0.1696 
2024-12-02 15:45:19,079 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0131 grad norm: 0.2689 
2024-12-02 15:45:19,821 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0054 grad norm: 0.1773 
2024-12-02 15:45:20,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0001 grad norm: 0.0247 
2024-12-02 15:45:21,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0022 grad norm: 0.1316 
2024-12-02 15:45:22,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0128 
2024-12-02 15:45:22,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0047 
2024-12-02 15:45:23,538 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0075 
2024-12-02 15:45:24,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0033 
2024-12-02 15:45:25,028 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0042 
2024-12-02 15:45:25,771 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0016 
2024-12-02 15:45:26,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0023 
2024-12-02 15:45:27,266 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0004 
2024-12-02 15:45:28,013 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0014 
2024-12-02 15:45:28,760 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:45:29,504 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:45:30,247 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:45:30,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0002 
2024-12-02 15:45:31,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 670.1270 grad norm: 70.4579 
2024-12-02 15:45:32,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.7254 grad norm: 6.9555 
2024-12-02 15:45:33,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.4196 grad norm: 4.8470 
2024-12-02 15:45:34,188 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0031 grad norm: 0.2855 
2024-12-02 15:45:34,933 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0919 grad norm: 1.5206 
2024-12-02 15:45:35,681 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0348 grad norm: 0.8672 
2024-12-02 15:45:36,422 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0054 grad norm: 0.3326 
2024-12-02 15:45:37,162 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0007 grad norm: 0.1182 
2024-12-02 15:45:37,903 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0013 grad norm: 0.1627 
2024-12-02 15:45:38,648 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0009 grad norm: 0.1399 
2024-12-02 15:45:39,389 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0003 grad norm: 0.0790 
2024-12-02 15:45:40,134 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0160 
2024-12-02 15:45:40,875 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0288 
2024-12-02 15:45:41,621 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0166 
2024-12-02 15:45:42,366 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0235 
2024-12-02 15:45:43,111 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0185 
2024-12-02 15:45:43,857 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0080 
2024-12-02 15:45:44,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0023 
2024-12-02 15:45:45,349 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0069 
2024-12-02 15:45:46,095 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0014 
2024-12-02 15:45:47,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 2.0406 grad norm: 5.4401 
2024-12-02 15:45:47,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0536 grad norm: 0.8386 
2024-12-02 15:45:48,543 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0022 grad norm: 0.1206 
2024-12-02 15:45:49,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0018 grad norm: 0.0886 
2024-12-02 15:45:50,029 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0034 grad norm: 0.1089 
2024-12-02 15:45:50,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0024 grad norm: 0.0878 
2024-12-02 15:45:51,518 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0013 grad norm: 0.0669 
2024-12-02 15:45:52,263 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0001 grad norm: 0.0236 
2024-12-02 15:45:53,006 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0002 grad norm: 0.0270 
2024-12-02 15:45:53,751 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0025 
2024-12-02 15:45:54,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0112 
2024-12-02 15:45:55,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0064 
2024-12-02 15:45:55,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0054 
2024-12-02 15:45:56,724 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0051 
2024-12-02 15:45:57,467 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0007 
2024-12-02 15:45:58,212 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0032 
2024-12-02 15:45:58,956 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0013 
2024-12-02 15:45:59,698 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0019 
2024-12-02 15:46:00,442 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:46:01,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0028 
2024-12-02 15:46:02,631 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-02 15:46:02,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-02 15:46:02,761 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-02 15:46:02,826 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-02 15:46:02,891 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-02 15:46:02,956 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-02 15:46:03,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-02 15:46:03,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-02 15:46:03,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-02 15:46:03,215 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-02 15:46:03,280 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-02 15:46:03,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-02 15:46:03,409 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-02 15:46:03,474 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-02 15:46:03,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-02 15:46:03,604 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-02 15:46:03,671 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-02 15:46:03,735 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-02 15:46:03,800 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-02 15:46:03,865 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-02 15:46:03,930 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-02 15:46:03,994 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-02 15:46:04,059 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-02 15:46:04,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-02 15:46:04,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-02 15:46:04,254 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-02 15:46:04,319 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-02 15:46:04,383 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-02 15:46:04,448 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-02 15:46:04,513 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-02 15:46:04,578 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-02 15:46:04,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-02 15:46:04,707 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-02 15:46:04,772 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-02 15:46:04,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-02 15:46:04,897 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-02 15:46:04,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-02 15:46:05,026 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-02 15:46:05,091 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-02 15:46:05,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-02 15:46:05,221 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-02 15:46:05,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-02 15:46:05,350 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-02 15:46:05,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-02 15:46:05,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-02 15:46:05,544 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-02 15:46:05,608 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-02 15:46:05,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-02 15:46:05,738 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-02 15:46:05,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-02 15:46:05,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-02 15:46:05,932 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-02 15:46:05,997 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-02 15:46:06,062 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-02 15:46:06,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-02 15:46:06,192 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-02 15:46:06,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-02 15:46:06,321 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-02 15:46:06,386 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-02 15:46:06,451 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-02 15:46:06,515 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-02 15:46:06,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-02 15:46:06,644 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-02 15:46:06,709 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-02 15:46:06,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-02 15:46:06,839 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-02 15:46:06,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-02 15:46:06,968 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-02 15:46:07,033 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-02 15:46:07,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-02 15:46:07,163 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-02 15:46:07,228 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-02 15:46:07,292 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-02 15:46:07,355 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-02 15:46:07,420 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-02 15:46:07,485 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-02 15:46:07,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-02 15:46:07,615 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-02 15:46:07,680 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-02 15:46:07,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-02 15:46:07,809 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-02 15:46:07,873 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-02 15:46:07,938 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-02 15:46:08,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-02 15:46:08,068 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-02 15:46:08,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-02 15:46:08,198 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-02 15:46:08,263 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-02 15:46:08,328 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-02 15:46:08,393 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-02 15:46:08,459 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-02 15:46:08,521 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-02 15:46:08,586 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-02 15:46:08,650 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-02 15:46:08,715 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-02 15:46:08,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-02 15:46:08,845 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-02 15:46:08,910 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-02 15:46:08,974 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-02 15:46:09,039 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-02 15:46:09,317 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-02 15:46:09,382 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-02 15:46:09,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-02 15:46:09,511 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-02 15:46:09,575 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-02 15:46:09,640 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-02 15:46:09,704 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-02 15:46:09,769 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-02 15:46:09,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-02 15:46:09,898 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-02 15:46:09,963 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-02 15:46:10,027 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-02 15:46:10,092 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-02 15:46:10,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-02 15:46:10,221 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-02 15:46:10,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-02 15:46:10,350 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-02 15:46:10,414 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-02 15:46:10,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-02 15:46:10,543 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-02 15:46:10,608 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-02 15:46:10,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-02 15:46:10,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-02 15:46:10,802 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-02 15:46:10,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-02 15:46:10,931 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-02 15:46:10,996 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-02 15:46:11,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-02 15:46:11,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-02 15:46:11,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-02 15:46:11,254 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-02 15:46:11,319 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-02 15:46:11,384 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-02 15:46:11,448 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-02 15:46:11,513 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-02 15:46:11,578 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-02 15:46:11,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-02 15:46:11,707 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-02 15:46:11,771 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-02 15:46:11,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-02 15:46:11,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-02 15:46:11,965 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-02 15:46:12,030 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-02 15:46:12,095 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-02 15:46:12,160 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-02 15:46:12,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-02 15:46:12,289 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-02 15:46:12,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-02 15:46:12,418 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-02 15:46:12,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-02 15:46:12,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-02 15:46:12,613 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-02 15:46:12,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-02 15:46:12,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-02 15:46:12,807 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-02 15:46:12,871 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-02 15:46:12,936 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-02 15:46:13,000 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-02 15:46:13,065 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-02 15:46:13,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-02 15:46:13,194 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-02 15:46:13,259 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-02 15:46:13,323 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-02 15:46:13,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-02 15:46:13,453 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-02 15:46:13,517 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-02 15:46:13,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-02 15:46:13,646 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-02 15:46:13,711 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-02 15:46:13,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-02 15:46:13,840 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-02 15:46:13,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-02 15:46:13,969 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-02 15:46:14,033 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-02 15:46:14,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-02 15:46:14,162 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-02 15:46:14,227 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-02 15:46:14,291 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-02 15:46:14,356 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-02 15:46:14,420 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-02 15:46:14,485 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-02 15:46:14,549 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-02 15:46:14,614 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-02 15:46:14,679 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-02 15:46:14,743 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-02 15:46:14,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-02 15:46:14,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-02 15:46:14,933 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-02 15:46:14,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-02 15:46:15,063 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-02 15:46:15,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-02 15:46:15,192 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-02 15:46:15,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-02 15:46:15,321 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-02 15:46:15,385 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-02 15:46:15,449 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-02 15:46:15,514 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-02 15:46:15,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-02 15:46:15,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-02 15:46:15,708 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-02 15:46:15,988 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,053 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,117 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,182 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,247 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,441 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,505 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,570 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,635 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,699 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,764 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,828 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:16,957 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,022 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,151 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,216 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,280 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,346 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,411 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,476 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,603 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,732 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,796 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,861 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:17,990 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,055 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,119 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,184 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,441 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,508 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,573 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,703 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,767 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,832 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,896 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:18,960 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,025 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,154 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,412 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,476 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,541 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,670 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,735 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,799 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,864 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,929 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:19,993 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,122 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,187 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,316 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,381 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,445 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,510 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,575 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,704 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,768 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,897 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:20,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,026 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,091 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,309 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,525 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,761 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,878 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:21,996 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,114 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,232 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,350 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,585 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,702 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,820 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:22,932 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:23,039 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:23,141 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:23,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:46:23,619 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.1000, 0.3000, 0.6000], device='cuda:0'), new_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0')
2024-12-02 15:46:23,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0'), new_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0')
2024-12-02 15:46:23,759 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0'), new_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0')
2024-12-02 15:46:23,825 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0'), new_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0')
2024-12-02 15:46:23,890 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0'), new_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0')
2024-12-02 15:46:23,956 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0'), new_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0')
2024-12-02 15:46:24,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0'), new_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0')
2024-12-02 15:46:24,087 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0'), new_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0')
2024-12-02 15:46:24,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0'), new_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0')
2024-12-02 15:46:24,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0'), new_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0')
2024-12-02 15:46:24,284 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0'), new_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0')
2024-12-02 15:46:24,349 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0'), new_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0')
2024-12-02 15:46:24,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0'), new_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0')
2024-12-02 15:46:24,480 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0'), new_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0')
2024-12-02 15:46:24,547 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0'), new_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0')
2024-12-02 15:46:24,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0'), new_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0')
2024-12-02 15:46:24,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0'), new_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0')
2024-12-02 15:46:24,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0'), new_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0')
2024-12-02 15:46:24,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0'), new_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0')
2024-12-02 15:46:24,871 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0'), new_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0')
2024-12-02 15:46:24,936 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0'), new_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0')
2024-12-02 15:46:25,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0'), new_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0')
2024-12-02 15:46:25,068 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0'), new_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0')
2024-12-02 15:46:25,134 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0'), new_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0')
2024-12-02 15:46:25,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0'), new_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0')
2024-12-02 15:46:25,265 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0'), new_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0')
2024-12-02 15:46:25,330 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0'), new_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0')
2024-12-02 15:46:25,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0'), new_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0')
2024-12-02 15:46:25,460 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0'), new_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0')
2024-12-02 15:46:25,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0'), new_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0')
2024-12-02 15:46:25,591 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0'), new_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0')
2024-12-02 15:46:25,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0'), new_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0')
2024-12-02 15:46:25,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0'), new_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0')
2024-12-02 15:46:25,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0'), new_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0')
2024-12-02 15:46:25,856 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0'), new_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0')
2024-12-02 15:46:25,922 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0'), new_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0')
2024-12-02 15:46:25,988 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0'), new_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0')
2024-12-02 15:46:26,053 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0'), new_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0')
2024-12-02 15:46:26,120 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0'), new_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0')
2024-12-02 15:46:26,185 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0'), new_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0')
2024-12-02 15:46:26,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0'), new_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0')
2024-12-02 15:46:26,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0'), new_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0')
2024-12-02 15:46:26,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0'), new_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0')
2024-12-02 15:46:26,445 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0'), new_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0')
2024-12-02 15:46:26,510 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0'), new_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0')
2024-12-02 15:46:26,576 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0'), new_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0')
2024-12-02 15:46:26,641 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0'), new_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0')
2024-12-02 15:46:26,707 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0'), new_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0')
2024-12-02 15:46:26,772 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0'), new_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0')
2024-12-02 15:46:26,838 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0'), new_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0')
2024-12-02 15:46:26,903 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0'), new_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0')
2024-12-02 15:46:26,969 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0'), new_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0')
2024-12-02 15:46:27,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0'), new_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0')
2024-12-02 15:46:27,100 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0'), new_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0')
2024-12-02 15:46:27,166 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0'), new_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0')
2024-12-02 15:46:27,232 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0'), new_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0')
2024-12-02 15:46:27,297 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0'), new_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0')
2024-12-02 15:46:27,363 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0'), new_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0')
2024-12-02 15:46:27,428 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0'), new_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0')
2024-12-02 15:46:27,494 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0'), new_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0')
2024-12-02 15:46:27,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0'), new_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0')
2024-12-02 15:46:27,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0'), new_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0')
2024-12-02 15:46:27,693 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0'), new_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0')
2024-12-02 15:46:27,758 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0'), new_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0')
2024-12-02 15:46:27,821 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0'), new_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0')
2024-12-02 15:46:27,886 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0'), new_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0')
2024-12-02 15:46:27,952 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0'), new_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0')
2024-12-02 15:46:28,018 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0'), new_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0')
2024-12-02 15:46:28,083 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0'), new_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0')
2024-12-02 15:46:28,149 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0'), new_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0')
2024-12-02 15:46:28,214 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0'), new_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0')
2024-12-02 15:46:28,280 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0'), new_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0')
2024-12-02 15:46:28,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0'), new_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0')
2024-12-02 15:46:28,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0'), new_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0')
2024-12-02 15:46:28,474 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0'), new_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0')
2024-12-02 15:46:28,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0'), new_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0')
2024-12-02 15:46:28,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0'), new_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0')
2024-12-02 15:46:28,670 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0'), new_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0')
2024-12-02 15:46:28,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0'), new_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0')
2024-12-02 15:46:28,801 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0'), new_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0')
2024-12-02 15:46:28,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0'), new_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0')
2024-12-02 15:46:28,932 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0'), new_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0')
2024-12-02 15:46:28,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0'), new_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0')
2024-12-02 15:46:29,063 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0'), new_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0')
2024-12-02 15:46:29,129 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0'), new_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0')
2024-12-02 15:46:29,194 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0'), new_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0')
2024-12-02 15:46:29,260 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0'), new_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0')
2024-12-02 15:46:29,325 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0'), new_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0')
2024-12-02 15:46:29,391 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0'), new_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0')
2024-12-02 15:46:29,456 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0'), new_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0')
2024-12-02 15:46:29,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0'), new_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0')
2024-12-02 15:46:29,587 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0'), new_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0')
2024-12-02 15:46:29,652 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0'), new_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0')
2024-12-02 15:46:29,718 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0'), new_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0')
2024-12-02 15:46:29,783 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0'), new_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0')
2024-12-02 15:46:29,849 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0'), new_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0')
2024-12-02 15:46:29,914 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0'), new_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0')
2024-12-02 15:46:29,980 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0'), new_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0')
2024-12-02 15:46:30,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0'), new_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0')
2024-12-02 15:46:30,111 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0'), new_distribution = tensor([0.1051, 0.3544, 0.5405], device='cuda:0')
