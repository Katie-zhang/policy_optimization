2024-12-04 16:31:33,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-04 16:31:33,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-04 16:31:33,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-04 16:31:33,398 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-04 16:31:33,403 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-04 16:31:33,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-04 16:31:33,413 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-04 16:31:33,418 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-04 16:31:33,423 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-04 16:31:33,429 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-04 16:31:33,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: -0.2456 reward: 0.2456 ref_reward: 0.2734 improvement: -10.15%
2024-12-04 16:31:34,216 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: -0.2587 reward: 0.2587 ref_reward: 0.2734 improvement: -5.36%
2024-12-04 16:31:34,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: -0.2685 reward: 0.2685 ref_reward: 0.2734 improvement: -1.80%
2024-12-04 16:31:35,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: -0.2756 reward: 0.2756 ref_reward: 0.2734 improvement: 0.80%
2024-12-04 16:31:35,445 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: -0.2803 reward: 0.2803 ref_reward: 0.2734 improvement: 2.51%
2024-12-04 16:31:35,772 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: -0.2828 reward: 0.2828 ref_reward: 0.2734 improvement: 3.42%
2024-12-04 16:31:36,062 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 16:31:36,352 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: -0.2828 reward: 0.2828 ref_reward: 0.2734 improvement: 3.45%
2024-12-04 16:31:36,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: -0.2820 reward: 0.2820 ref_reward: 0.2734 improvement: 3.14%
2024-12-04 16:31:36,928 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: -0.2815 reward: 0.2815 ref_reward: 0.2734 improvement: 2.95%
2024-12-04 16:31:37,223 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: -0.2814 reward: 0.2814 ref_reward: 0.2734 improvement: 2.93%
2024-12-04 16:31:37,512 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: -0.2817 reward: 0.2817 ref_reward: 0.2734 improvement: 3.05%
2024-12-04 16:31:37,804 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: -0.2823 reward: 0.2823 ref_reward: 0.2734 improvement: 3.24%
2024-12-04 16:31:38,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: -0.2828 reward: 0.2828 ref_reward: 0.2734 improvement: 3.42%
2024-12-04 16:31:38,645 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: -0.2831 reward: 0.2831 ref_reward: 0.2734 improvement: 3.56%
2024-12-04 16:31:39,197 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.63%
2024-12-04 16:31:39,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 16:31:40,276 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.63%
2024-12-04 16:31:40,805 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.61%
2024-12-04 16:31:41,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 16:31:41,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 16:31:41,882 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 16:31:42,168 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.61%
2024-12-04 16:31:42,459 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.63%
2024-12-04 16:31:42,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.64%
2024-12-04 16:31:43,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 16:31:43,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:43,587 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:43,868 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:44,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:44,437 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:44,720 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:45,001 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:45,282 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:45,563 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:45,843 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:46,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:46,405 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:46,685 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:46,966 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:47,247 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:47,527 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:47,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:31:48,090 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:31:48,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:31:48,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:31:49,175 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:31:49,701 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:31:50,228 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:31:50,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:31:51,437 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: 0.0932 reward: -0.0932 ref_reward: 0.3541 improvement: -126.33%
2024-12-04 16:31:51,719 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: 0.0452 reward: -0.0452 ref_reward: 0.3541 improvement: -112.77%
2024-12-04 16:31:52,000 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: -0.0010 reward: 0.0010 ref_reward: 0.3541 improvement: -99.71%
2024-12-04 16:31:52,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: -0.0452 reward: 0.0452 ref_reward: 0.3541 improvement: -87.24%
2024-12-04 16:31:52,562 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: -0.0847 reward: 0.0847 ref_reward: 0.3541 improvement: -76.08%
2024-12-04 16:31:52,843 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: -0.1217 reward: 0.1217 ref_reward: 0.3541 improvement: -65.64%
2024-12-04 16:31:53,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: -0.1550 reward: 0.1550 ref_reward: 0.3541 improvement: -56.22%
2024-12-04 16:31:53,410 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: -0.1851 reward: 0.1851 ref_reward: 0.3541 improvement: -47.73%
2024-12-04 16:31:53,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: -0.2136 reward: 0.2136 ref_reward: 0.3541 improvement: -39.69%
2024-12-04 16:31:53,979 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: -0.2400 reward: 0.2400 ref_reward: 0.3541 improvement: -32.21%
2024-12-04 16:31:54,266 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: -0.2633 reward: 0.2633 ref_reward: 0.3541 improvement: -25.63%
2024-12-04 16:31:54,551 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: -0.2838 reward: 0.2838 ref_reward: 0.3541 improvement: -19.84%
2024-12-04 16:31:54,834 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: -0.3012 reward: 0.3012 ref_reward: 0.3541 improvement: -14.92%
2024-12-04 16:31:55,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: -0.3153 reward: 0.3153 ref_reward: 0.3541 improvement: -10.95%
2024-12-04 16:31:55,400 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: -0.3263 reward: 0.3263 ref_reward: 0.3541 improvement: -7.84%
2024-12-04 16:31:55,683 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: -0.3346 reward: 0.3346 ref_reward: 0.3541 improvement: -5.51%
2024-12-04 16:31:55,968 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: -0.3405 reward: 0.3405 ref_reward: 0.3541 improvement: -3.83%
2024-12-04 16:31:56,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: -0.3447 reward: 0.3447 ref_reward: 0.3541 improvement: -2.66%
2024-12-04 16:31:56,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: -0.3476 reward: 0.3476 ref_reward: 0.3541 improvement: -1.84%
2024-12-04 16:31:56,820 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: -0.3497 reward: 0.3497 ref_reward: 0.3541 improvement: -1.25%
2024-12-04 16:31:57,102 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: -0.3513 reward: 0.3513 ref_reward: 0.3541 improvement: -0.79%
2024-12-04 16:31:57,384 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: -0.3526 reward: 0.3526 ref_reward: 0.3541 improvement: -0.41%
2024-12-04 16:31:57,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: -0.3538 reward: 0.3538 ref_reward: 0.3541 improvement: -0.07%
2024-12-04 16:31:57,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: -0.3549 reward: 0.3549 ref_reward: 0.3541 improvement: 0.23%
2024-12-04 16:31:58,233 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: -0.3558 reward: 0.3558 ref_reward: 0.3541 improvement: 0.49%
2024-12-04 16:31:58,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: -0.3565 reward: 0.3565 ref_reward: 0.3541 improvement: 0.68%
2024-12-04 16:31:58,800 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.80%
2024-12-04 16:31:59,083 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 16:31:59,366 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 16:31:59,650 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 16:31:59,933 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.78%
2024-12-04 16:32:00,216 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: -0.3567 reward: 0.3567 ref_reward: 0.3541 improvement: 0.75%
2024-12-04 16:32:00,500 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.3567 reward: 0.3567 ref_reward: 0.3541 improvement: 0.73%
2024-12-04 16:32:00,783 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.3567 reward: 0.3567 ref_reward: 0.3541 improvement: 0.74%
2024-12-04 16:32:01,066 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.3568 reward: 0.3568 ref_reward: 0.3541 improvement: 0.76%
2024-12-04 16:32:01,349 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.3568 reward: 0.3568 ref_reward: 0.3541 improvement: 0.78%
2024-12-04 16:32:01,632 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.80%
2024-12-04 16:32:01,917 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.82%
2024-12-04 16:32:02,201 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.82%
2024-12-04 16:32:02,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 16:32:02,766 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 16:32:03,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 16:32:03,333 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 16:32:03,616 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 16:32:03,899 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 16:32:04,183 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 16:32:04,467 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 16:32:04,753 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 16:32:05,040 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.3572 reward: 0.3572 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 16:32:05,325 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.3572 reward: 0.3572 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 16:32:05,831 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: 13.5719 reward: -13.5719 ref_reward: 0.3861 improvement: -3615.48%
2024-12-04 16:32:06,122 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: 13.1194 reward: -13.1194 ref_reward: 0.3861 improvement: -3498.27%
2024-12-04 16:32:06,406 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: 12.6574 reward: -12.6574 ref_reward: 0.3861 improvement: -3378.59%
2024-12-04 16:32:06,691 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: 12.1896 reward: -12.1896 ref_reward: 0.3861 improvement: -3257.43%
2024-12-04 16:32:06,974 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: 11.7356 reward: -11.7356 ref_reward: 0.3861 improvement: -3139.84%
2024-12-04 16:32:07,257 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: 11.2722 reward: -11.2722 ref_reward: 0.3861 improvement: -3019.79%
2024-12-04 16:32:07,542 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: 10.7785 reward: -10.7785 ref_reward: 0.3861 improvement: -2891.91%
2024-12-04 16:32:07,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: 10.2529 reward: -10.2529 ref_reward: 0.3861 improvement: -2755.77%
2024-12-04 16:32:08,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: 9.6949 reward: -9.6949 ref_reward: 0.3861 improvement: -2611.22%
2024-12-04 16:32:08,390 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: 9.1149 reward: -9.1149 ref_reward: 0.3861 improvement: -2461.00%
2024-12-04 16:32:08,672 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: 8.5286 reward: -8.5286 ref_reward: 0.3861 improvement: -2309.14%
2024-12-04 16:32:08,954 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: 7.9100 reward: -7.9100 ref_reward: 0.3861 improvement: -2148.91%
2024-12-04 16:32:09,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: 7.2647 reward: -7.2647 ref_reward: 0.3861 improvement: -1981.75%
2024-12-04 16:32:09,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: 6.6003 reward: -6.6003 ref_reward: 0.3861 improvement: -1809.66%
2024-12-04 16:32:09,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: 5.9274 reward: -5.9274 ref_reward: 0.3861 improvement: -1635.34%
2024-12-04 16:32:10,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: 5.2561 reward: -5.2561 ref_reward: 0.3861 improvement: -1461.46%
2024-12-04 16:32:10,369 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: 4.5992 reward: -4.5992 ref_reward: 0.3861 improvement: -1291.30%
2024-12-04 16:32:10,652 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: 3.9674 reward: -3.9674 ref_reward: 0.3861 improvement: -1127.66%
2024-12-04 16:32:10,933 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: 3.3737 reward: -3.3737 ref_reward: 0.3861 improvement: -973.86%
2024-12-04 16:32:11,215 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: 2.8280 reward: -2.8280 ref_reward: 0.3861 improvement: -832.53%
2024-12-04 16:32:11,498 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: 2.3375 reward: -2.3375 ref_reward: 0.3861 improvement: -705.48%
2024-12-04 16:32:11,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: 1.9058 reward: -1.9058 ref_reward: 0.3861 improvement: -593.64%
2024-12-04 16:32:12,062 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: 1.5330 reward: -1.5330 ref_reward: 0.3861 improvement: -497.10%
2024-12-04 16:32:12,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: 1.2167 reward: -1.2167 ref_reward: 0.3861 improvement: -415.15%
2024-12-04 16:32:12,626 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: 0.9520 reward: -0.9520 ref_reward: 0.3861 improvement: -346.58%
2024-12-04 16:32:12,909 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: 0.7330 reward: -0.7330 ref_reward: 0.3861 improvement: -289.86%
2024-12-04 16:32:13,193 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: 0.5533 reward: -0.5533 ref_reward: 0.3861 improvement: -243.31%
2024-12-04 16:32:13,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: 0.4066 reward: -0.4066 ref_reward: 0.3861 improvement: -205.32%
2024-12-04 16:32:13,760 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: 0.2872 reward: -0.2872 ref_reward: 0.3861 improvement: -174.39%
2024-12-04 16:32:14,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: 0.1900 reward: -0.1900 ref_reward: 0.3861 improvement: -149.23%
2024-12-04 16:32:14,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: 0.1109 reward: -0.1109 ref_reward: 0.3861 improvement: -128.72%
2024-12-04 16:32:14,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: 0.0461 reward: -0.0461 ref_reward: 0.3861 improvement: -111.95%
2024-12-04 16:32:14,896 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.0070 reward: 0.0070 ref_reward: 0.3861 improvement: -98.18%
2024-12-04 16:32:15,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.0509 reward: 0.0509 ref_reward: 0.3861 improvement: -86.82%
2024-12-04 16:32:15,465 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.0873 reward: 0.0873 ref_reward: 0.3861 improvement: -77.39%
2024-12-04 16:32:15,749 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.1178 reward: 0.1178 ref_reward: 0.3861 improvement: -69.50%
2024-12-04 16:32:16,033 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.1434 reward: 0.1434 ref_reward: 0.3861 improvement: -62.86%
2024-12-04 16:32:16,316 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.1651 reward: 0.1651 ref_reward: 0.3861 improvement: -57.24%
2024-12-04 16:32:16,601 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.1836 reward: 0.1836 ref_reward: 0.3861 improvement: -52.44%
2024-12-04 16:32:16,884 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.1995 reward: 0.1995 ref_reward: 0.3861 improvement: -48.32%
2024-12-04 16:32:17,168 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.2133 reward: 0.2133 ref_reward: 0.3861 improvement: -44.75%
2024-12-04 16:32:17,451 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.2253 reward: 0.2253 ref_reward: 0.3861 improvement: -41.64%
2024-12-04 16:32:17,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.2358 reward: 0.2358 ref_reward: 0.3861 improvement: -38.91%
2024-12-04 16:32:18,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.2451 reward: 0.2451 ref_reward: 0.3861 improvement: -36.51%
2024-12-04 16:32:18,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.2534 reward: 0.2534 ref_reward: 0.3861 improvement: -34.37%
2024-12-04 16:32:18,586 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.2607 reward: 0.2607 ref_reward: 0.3861 improvement: -32.46%
2024-12-04 16:32:18,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.2674 reward: 0.2674 ref_reward: 0.3861 improvement: -30.75%
2024-12-04 16:32:19,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.2733 reward: 0.2733 ref_reward: 0.3861 improvement: -29.20%
2024-12-04 16:32:19,436 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.2788 reward: 0.2788 ref_reward: 0.3861 improvement: -27.79%
2024-12-04 16:32:19,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.2837 reward: 0.2837 ref_reward: 0.3861 improvement: -26.51%
2024-12-04 16:32:20,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: 0.0917 reward: -0.0917 ref_reward: 0.1811 improvement: -150.64%
2024-12-04 16:32:20,681 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: 0.0511 reward: -0.0511 ref_reward: 0.1811 improvement: -128.23%
2024-12-04 16:32:20,963 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: 0.0147 reward: -0.0147 ref_reward: 0.1811 improvement: -108.11%
2024-12-04 16:32:21,246 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: -0.0180 reward: 0.0180 ref_reward: 0.1811 improvement: -90.06%
2024-12-04 16:32:21,529 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: -0.0466 reward: 0.0466 ref_reward: 0.1811 improvement: -74.28%
2024-12-04 16:32:21,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: -0.0722 reward: 0.0722 ref_reward: 0.1811 improvement: -60.16%
2024-12-04 16:32:22,096 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: -0.0939 reward: 0.0939 ref_reward: 0.1811 improvement: -48.19%
2024-12-04 16:32:22,380 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: -0.1118 reward: 0.1118 ref_reward: 0.1811 improvement: -38.29%
2024-12-04 16:32:22,664 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: -0.1276 reward: 0.1276 ref_reward: 0.1811 improvement: -29.54%
2024-12-04 16:32:22,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: -0.1417 reward: 0.1417 ref_reward: 0.1811 improvement: -21.79%
2024-12-04 16:32:23,231 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: -0.1534 reward: 0.1534 ref_reward: 0.1811 improvement: -15.30%
2024-12-04 16:32:23,517 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: -0.1634 reward: 0.1634 ref_reward: 0.1811 improvement: -9.79%
2024-12-04 16:32:23,800 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: -0.1718 reward: 0.1718 ref_reward: 0.1811 improvement: -5.14%
2024-12-04 16:32:24,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: -0.1786 reward: 0.1786 ref_reward: 0.1811 improvement: -1.38%
2024-12-04 16:32:24,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: -0.1839 reward: 0.1839 ref_reward: 0.1811 improvement: 1.52%
2024-12-04 16:32:24,651 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: -0.1877 reward: 0.1877 ref_reward: 0.1811 improvement: 3.60%
2024-12-04 16:32:24,937 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: -0.1901 reward: 0.1901 ref_reward: 0.1811 improvement: 4.96%
2024-12-04 16:32:25,221 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: -0.1915 reward: 0.1915 ref_reward: 0.1811 improvement: 5.71%
2024-12-04 16:32:25,505 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.03%
2024-12-04 16:32:25,789 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.07%
2024-12-04 16:32:26,072 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: -0.1920 reward: 0.1920 ref_reward: 0.1811 improvement: 5.99%
2024-12-04 16:32:26,356 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: -0.1918 reward: 0.1918 ref_reward: 0.1811 improvement: 5.87%
2024-12-04 16:32:26,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: -0.1916 reward: 0.1916 ref_reward: 0.1811 improvement: 5.76%
2024-12-04 16:32:26,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: -0.1914 reward: 0.1914 ref_reward: 0.1811 improvement: 5.67%
2024-12-04 16:32:27,211 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: -0.1913 reward: 0.1913 ref_reward: 0.1811 improvement: 5.62%
2024-12-04 16:32:27,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: -0.1912 reward: 0.1912 ref_reward: 0.1811 improvement: 5.58%
2024-12-04 16:32:27,784 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: -0.1912 reward: 0.1912 ref_reward: 0.1811 improvement: 5.58%
2024-12-04 16:32:28,069 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: -0.1913 reward: 0.1913 ref_reward: 0.1811 improvement: 5.61%
2024-12-04 16:32:28,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: -0.1914 reward: 0.1914 ref_reward: 0.1811 improvement: 5.67%
2024-12-04 16:32:28,637 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: -0.1916 reward: 0.1916 ref_reward: 0.1811 improvement: 5.77%
2024-12-04 16:32:28,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: -0.1918 reward: 0.1918 ref_reward: 0.1811 improvement: 5.87%
2024-12-04 16:32:29,207 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: -0.1920 reward: 0.1920 ref_reward: 0.1811 improvement: 5.97%
2024-12-04 16:32:29,492 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.06%
2024-12-04 16:32:29,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.12%
2024-12-04 16:32:30,057 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.17%
2024-12-04 16:32:30,340 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.19%
2024-12-04 16:32:30,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:30,908 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:31,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:31,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:31,755 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:32,038 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:32:32,321 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:32:32,606 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:32:32,889 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:32:33,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:32:33,454 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:33,738 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:34,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:32:34,304 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:32:35,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 0.6893 grad norm: 0.5533 policy: 0.4025 0.2957
2024-12-04 16:32:36,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 0.6418 grad norm: 0.3678 policy: 0.4233 0.3369
2024-12-04 16:32:36,932 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 0.6142 grad norm: 0.2262 policy: 0.4413 0.3694
2024-12-04 16:32:37,640 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 0.6015 grad norm: 0.0979 policy: 0.4588 0.3906
2024-12-04 16:32:38,346 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 0.5989 grad norm: 0.0165 policy: 0.4746 0.4009
2024-12-04 16:32:39,051 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 0.6003 grad norm: 0.0790 policy: 0.4865 0.4019
2024-12-04 16:32:39,758 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 0.6010 grad norm: 0.0938 policy: 0.4900 0.3999
2024-12-04 16:32:40,464 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 0.6000 grad norm: 0.0700 policy: 0.4861 0.3978
2024-12-04 16:32:41,171 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 0.5991 grad norm: 0.0314 policy: 0.4791 0.3959
2024-12-04 16:32:41,877 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 0.5988 grad norm: 0.0026 policy: 0.4731 0.3942
2024-12-04 16:32:42,585 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 0.5989 grad norm: 0.0216 policy: 0.4704 0.3928
2024-12-04 16:32:43,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 0.5990 grad norm: 0.0254 policy: 0.4709 0.3921
2024-12-04 16:32:44,001 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 0.5989 grad norm: 0.0184 policy: 0.4731 0.3922
2024-12-04 16:32:44,715 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0069 policy: 0.4751 0.3932
2024-12-04 16:32:45,422 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0035 policy: 0.4760 0.3944
2024-12-04 16:32:46,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0082 policy: 0.4760 0.3952
2024-12-04 16:32:46,834 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0074 policy: 0.4756 0.3952
2024-12-04 16:32:47,542 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0035 policy: 0.4752 0.3946
2024-12-04 16:32:48,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0006 policy: 0.4749 0.3940
2024-12-04 16:32:48,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0028 policy: 0.4748 0.3938
2024-12-04 16:32:50,217 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 1.0056 grad norm: 0.8977 policy: 0.3867 0.3528
2024-12-04 16:32:51,247 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 0.9128 grad norm: 0.8236 policy: 0.4262 0.3696
2024-12-04 16:32:52,055 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 0.8366 grad norm: 0.7952 policy: 0.4655 0.3755
2024-12-04 16:32:52,772 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 0.7680 grad norm: 0.7468 policy: 0.5160 0.3601
2024-12-04 16:32:53,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 0.7075 grad norm: 0.6714 policy: 0.5757 0.3293
2024-12-04 16:32:54,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 0.6573 grad norm: 0.5446 policy: 0.6400 0.2890
2024-12-04 16:32:54,912 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 0.6221 grad norm: 0.3716 policy: 0.7020 0.2458
2024-12-04 16:32:55,928 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 0.6037 grad norm: 0.1786 policy: 0.7548 0.2064
2024-12-04 16:32:57,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 0.5989 grad norm: 0.0267 policy: 0.7914 0.1784
2024-12-04 16:32:58,489 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 0.6005 grad norm: 0.1119 policy: 0.8094 0.1650
2024-12-04 16:32:59,594 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 0.6021 grad norm: 0.1543 policy: 0.8124 0.1637
2024-12-04 16:33:00,337 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 0.6017 grad norm: 0.1438 policy: 0.8068 0.1688
2024-12-04 16:33:01,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 0.6003 grad norm: 0.1025 policy: 0.7984 0.1753
2024-12-04 16:33:01,811 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 0.5992 grad norm: 0.0512 policy: 0.7906 0.1807
2024-12-04 16:33:02,556 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0084 policy: 0.7843 0.1848
2024-12-04 16:33:03,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 0.5989 grad norm: 0.0221 policy: 0.7797 0.1881
2024-12-04 16:33:04,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 0.5990 grad norm: 0.0325 policy: 0.7773 0.1902
2024-12-04 16:33:04,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 0.5989 grad norm: 0.0305 policy: 0.7775 0.1903
2024-12-04 16:33:05,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 0.5989 grad norm: 0.0207 policy: 0.7795 0.1888
2024-12-04 16:33:06,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0083 policy: 0.7822 0.1867
2024-12-04 16:33:07,263 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 9.1575 grad norm: 1.0567 policy: 0.3278 0.2949
2024-12-04 16:33:08,010 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 9.0551 grad norm: 1.0406 policy: 0.3859 0.2751
2024-12-04 16:33:08,749 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 8.9590 grad norm: 1.1041 policy: 0.4455 0.2513
2024-12-04 16:33:09,487 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 8.8625 grad norm: 1.2080 policy: 0.5085 0.2219
2024-12-04 16:33:10,231 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 8.7611 grad norm: 1.3741 policy: 0.5764 0.1899
2024-12-04 16:33:10,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 8.6439 grad norm: 1.5712 policy: 0.6516 0.1554
2024-12-04 16:33:11,712 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 8.5074 grad norm: 1.7868 policy: 0.7303 0.1192
2024-12-04 16:33:12,456 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 8.3487 grad norm: 2.0226 policy: 0.8065 0.0847
2024-12-04 16:33:13,195 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 8.1642 grad norm: 2.2765 policy: 0.8730 0.0550
2024-12-04 16:33:13,940 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 7.9512 grad norm: 2.5485 policy: 0.9244 0.0324
2024-12-04 16:33:14,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 7.7073 grad norm: 2.8384 policy: 0.9593 0.0172
2024-12-04 16:33:15,417 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 7.4300 grad norm: 3.1459 policy: 0.9803 0.0083
2024-12-04 16:33:16,163 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 7.1168 grad norm: 3.4706 policy: 0.9914 0.0036
2024-12-04 16:33:16,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 6.7655 grad norm: 3.8123 policy: 0.9966 0.0014
2024-12-04 16:33:17,645 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 6.3737 grad norm: 4.1706 policy: 0.9988 0.0005
2024-12-04 16:33:18,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 5.9391 grad norm: 4.5452 policy: 0.9996 0.0002
2024-12-04 16:33:19,138 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 5.4594 grad norm: 4.9359 policy: 0.9999 0.0000
2024-12-04 16:33:19,895 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 4.9324 grad norm: 5.3425 policy: 1.0000 0.0000
2024-12-04 16:33:20,687 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 4.3557 grad norm: 5.7648 policy: 1.0000 0.0000
2024-12-04 16:33:22,033 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 3.7272 grad norm: 6.2014 policy: 1.0000 0.0000
2024-12-04 16:33:23,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 0.6575 grad norm: 0.3383 policy: 0.3131 0.3098
2024-12-04 16:33:24,140 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 0.6339 grad norm: 0.2649 policy: 0.2953 0.3555
2024-12-04 16:33:24,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 0.6176 grad norm: 0.2003 policy: 0.2751 0.4005
2024-12-04 16:33:25,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 0.6076 grad norm: 0.1421 policy: 0.2521 0.4416
2024-12-04 16:33:26,273 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 0.6022 grad norm: 0.0891 policy: 0.2287 0.4733
2024-12-04 16:33:26,984 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 0.5998 grad norm: 0.0471 policy: 0.2081 0.4915
2024-12-04 16:33:27,694 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 0.5991 grad norm: 0.0284 policy: 0.1920 0.4980
2024-12-04 16:33:28,409 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 0.5992 grad norm: 0.0375 policy: 0.1827 0.4956
2024-12-04 16:33:29,123 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 0.5993 grad norm: 0.0416 policy: 0.1808 0.4908
2024-12-04 16:33:29,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 0.5992 grad norm: 0.0330 policy: 0.1838 0.4860
2024-12-04 16:33:30,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 0.5989 grad norm: 0.0186 policy: 0.1890 0.4831
2024-12-04 16:33:31,340 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0058 policy: 0.1940 0.4824
2024-12-04 16:33:32,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0064 policy: 0.1972 0.4828
2024-12-04 16:33:33,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0099 policy: 0.1983 0.4835
2024-12-04 16:33:34,386 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0090 policy: 0.1977 0.4840
2024-12-04 16:33:35,097 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0054 policy: 0.1962 0.4841
2024-12-04 16:33:35,805 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0012 policy: 0.1947 0.4840
2024-12-04 16:33:36,509 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0019 policy: 0.1937 0.4841
2024-12-04 16:33:37,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0030 policy: 0.1934 0.4843
2024-12-04 16:33:37,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0025 policy: 0.1937 0.4845
2024-12-04 16:33:39,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 0.0233 grad norm: 0.5358 
2024-12-04 16:33:40,186 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.0088 grad norm: 0.2707 
2024-12-04 16:33:40,993 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.0003 grad norm: 0.0539 
2024-12-04 16:33:42,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0017 grad norm: 0.1341 
2024-12-04 16:33:43,337 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0021 grad norm: 0.1292 
2024-12-04 16:33:44,051 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0002 grad norm: 0.0392 
2024-12-04 16:33:44,770 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0001 grad norm: 0.0362 
2024-12-04 16:33:45,487 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0005 grad norm: 0.0617 
2024-12-04 16:33:46,201 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0002 grad norm: 0.0412 
2024-12-04 16:33:46,916 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0089 
2024-12-04 16:33:47,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0166 
2024-12-04 16:33:48,348 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0001 grad norm: 0.0224 
2024-12-04 16:33:49,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0177 
2024-12-04 16:33:49,773 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0016 
2024-12-04 16:33:50,492 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0017 
2024-12-04 16:33:51,208 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0009 
2024-12-04 16:33:51,928 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0004 
2024-12-04 16:33:52,648 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0006 
2024-12-04 16:33:53,452 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0002 
2024-12-04 16:33:54,243 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0001 
2024-12-04 16:33:55,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 1.4108 grad norm: 4.0139 
2024-12-04 16:33:56,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.0177 grad norm: 0.5911 
2024-12-04 16:33:56,850 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.0341 grad norm: 0.5500 
2024-12-04 16:33:57,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0120 grad norm: 0.3581 
2024-12-04 16:33:58,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0076 grad norm: 0.3584 
2024-12-04 16:33:59,467 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0017 grad norm: 0.1644 
2024-12-04 16:34:00,333 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0038 grad norm: 0.2232 
2024-12-04 16:34:01,109 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0154 
2024-12-04 16:34:01,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0099 
2024-12-04 16:34:02,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0118 
2024-12-04 16:34:03,463 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0072 
2024-12-04 16:34:04,247 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0061 
2024-12-04 16:34:05,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0024 
2024-12-04 16:34:05,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0050 
2024-12-04 16:34:06,544 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0023 
2024-12-04 16:34:07,250 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0027 
2024-12-04 16:34:07,960 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0009 
2024-12-04 16:34:08,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0011 
2024-12-04 16:34:09,375 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0010 
2024-12-04 16:34:10,081 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0003 
2024-12-04 16:34:11,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 676.0389 grad norm: 55.6053 
2024-12-04 16:34:11,734 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.2365 grad norm: 2.5883 
2024-12-04 16:34:12,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.2186 grad norm: 2.5360 
2024-12-04 16:34:13,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0217 grad norm: 0.6926 
2024-12-04 16:34:13,862 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0260 grad norm: 0.7536 
2024-12-04 16:34:14,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0274 grad norm: 0.7287 
2024-12-04 16:34:15,276 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0013 grad norm: 0.1465 
2024-12-04 16:34:15,982 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0027 grad norm: 0.2060 
2024-12-04 16:34:16,689 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0023 grad norm: 0.1813 
2024-12-04 16:34:17,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0010 grad norm: 0.1201 
2024-12-04 16:34:18,100 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0409 
2024-12-04 16:34:18,806 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0001 grad norm: 0.0406 
2024-12-04 16:34:19,513 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0002 grad norm: 0.0562 
2024-12-04 16:34:20,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0105 
2024-12-04 16:34:20,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0133 
2024-12-04 16:34:21,632 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0071 
2024-12-04 16:34:22,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0016 
2024-12-04 16:34:23,044 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0052 
2024-12-04 16:34:23,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0043 
2024-12-04 16:34:24,457 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0014 
2024-12-04 16:34:25,386 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 0.7671 grad norm: 2.6855 
2024-12-04 16:34:26,095 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.0180 grad norm: 0.5258 
2024-12-04 16:34:26,814 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.0105 grad norm: 0.2918 
2024-12-04 16:34:27,521 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0009 grad norm: 0.0933 
2024-12-04 16:34:28,221 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0012 grad norm: 0.1265 
2024-12-04 16:34:28,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0007 grad norm: 0.0938 
2024-12-04 16:34:29,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0002 grad norm: 0.0477 
2024-12-04 16:34:30,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0037 
2024-12-04 16:34:31,061 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0001 grad norm: 0.0330 
2024-12-04 16:34:31,768 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0001 grad norm: 0.0340 
2024-12-04 16:34:32,485 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0265 
2024-12-04 16:34:33,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0082 
2024-12-04 16:34:33,899 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0101 
2024-12-04 16:34:34,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0012 
2024-12-04 16:34:35,317 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0061 
2024-12-04 16:34:36,026 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0047 
2024-12-04 16:34:36,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0002 
2024-12-04 16:34:37,669 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0028 
2024-12-04 16:34:38,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0019 
2024-12-04 16:34:39,565 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0014 
2024-12-04 16:34:41,118 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-04 16:34:41,213 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-04 16:34:41,307 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-04 16:34:41,400 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-04 16:34:41,488 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-04 16:34:41,623 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-04 16:34:41,738 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-04 16:34:41,855 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-04 16:34:41,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-04 16:34:42,092 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-04 16:34:42,212 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-04 16:34:42,335 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-04 16:34:42,458 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-04 16:34:42,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-04 16:34:42,704 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-04 16:34:42,837 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-04 16:34:42,971 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-04 16:34:43,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-04 16:34:43,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-04 16:34:43,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-04 16:34:43,531 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-04 16:34:43,654 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-04 16:34:43,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-04 16:34:43,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-04 16:34:44,024 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-04 16:34:44,148 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-04 16:34:44,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-04 16:34:44,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-04 16:34:44,501 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-04 16:34:44,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-04 16:34:44,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-04 16:34:44,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-04 16:34:44,887 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-04 16:34:44,982 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-04 16:34:45,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-04 16:34:45,166 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-04 16:34:45,258 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-04 16:34:45,358 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-04 16:34:45,450 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-04 16:34:45,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-04 16:34:45,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-04 16:34:45,727 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-04 16:34:45,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-04 16:34:45,937 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-04 16:34:46,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-04 16:34:46,126 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-04 16:34:46,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-04 16:34:46,324 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-04 16:34:46,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-04 16:34:46,507 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-04 16:34:46,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-04 16:34:46,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-04 16:34:46,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-04 16:34:46,879 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-04 16:34:46,987 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-04 16:34:47,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-04 16:34:47,182 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-04 16:34:47,277 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-04 16:34:47,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-04 16:34:47,465 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-04 16:34:47,557 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-04 16:34:47,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-04 16:34:47,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-04 16:34:47,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-04 16:34:47,926 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-04 16:34:48,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-04 16:34:48,117 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-04 16:34:48,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-04 16:34:48,316 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-04 16:34:48,411 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-04 16:34:48,503 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-04 16:34:48,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-04 16:34:48,697 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-04 16:34:48,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-04 16:34:48,932 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-04 16:34:49,064 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-04 16:34:49,207 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-04 16:34:49,340 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-04 16:34:49,462 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-04 16:34:49,585 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-04 16:34:49,708 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-04 16:34:49,831 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-04 16:34:49,947 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-04 16:34:50,053 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-04 16:34:50,152 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-04 16:34:50,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-04 16:34:50,331 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-04 16:34:50,427 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-04 16:34:50,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-04 16:34:50,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-04 16:34:50,724 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-04 16:34:50,821 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-04 16:34:50,918 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-04 16:34:51,013 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-04 16:34:51,113 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-04 16:34:51,210 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-04 16:34:51,310 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-04 16:34:51,406 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-04 16:34:51,499 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-04 16:34:51,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-04 16:34:51,929 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-04 16:34:52,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-04 16:34:52,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-04 16:34:52,212 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-04 16:34:52,308 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-04 16:34:52,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-04 16:34:52,505 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-04 16:34:52,600 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-04 16:34:52,694 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-04 16:34:52,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-04 16:34:52,879 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-04 16:34:52,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-04 16:34:53,067 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-04 16:34:53,161 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-04 16:34:53,254 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-04 16:34:53,351 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-04 16:34:53,451 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-04 16:34:53,552 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-04 16:34:53,654 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-04 16:34:53,770 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-04 16:34:53,873 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-04 16:34:53,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-04 16:34:54,105 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-04 16:34:54,231 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-04 16:34:54,352 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-04 16:34:54,488 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-04 16:34:54,616 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-04 16:34:54,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-04 16:34:54,851 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-04 16:34:54,958 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-04 16:34:55,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-04 16:34:55,165 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-04 16:34:55,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-04 16:34:55,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-04 16:34:55,463 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-04 16:34:55,559 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-04 16:34:55,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-04 16:34:55,752 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-04 16:34:55,848 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-04 16:34:55,940 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-04 16:34:56,032 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-04 16:34:56,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-04 16:34:56,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-04 16:34:56,310 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-04 16:34:56,400 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-04 16:34:56,499 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-04 16:34:56,594 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-04 16:34:56,693 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-04 16:34:56,788 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-04 16:34:56,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-04 16:34:56,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-04 16:34:57,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-04 16:34:57,174 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-04 16:34:57,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-04 16:34:57,362 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-04 16:34:57,455 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-04 16:34:57,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-04 16:34:57,640 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-04 16:34:57,729 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-04 16:34:57,828 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-04 16:34:57,933 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-04 16:34:58,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-04 16:34:58,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-04 16:34:58,230 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-04 16:34:58,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-04 16:34:58,428 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-04 16:34:58,524 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-04 16:34:58,616 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-04 16:34:58,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-04 16:34:58,807 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-04 16:34:58,900 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-04 16:34:58,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-04 16:34:59,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-04 16:34:59,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-04 16:34:59,287 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-04 16:34:59,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-04 16:34:59,482 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-04 16:34:59,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-04 16:34:59,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-04 16:34:59,794 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-04 16:34:59,887 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-04 16:34:59,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-04 16:35:00,076 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-04 16:35:00,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-04 16:35:00,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-04 16:35:00,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-04 16:35:00,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-04 16:35:00,573 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-04 16:35:00,672 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-04 16:35:00,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-04 16:35:00,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-04 16:35:00,973 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-04 16:35:01,081 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-04 16:35:01,173 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-04 16:35:01,272 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-04 16:35:01,364 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-04 16:35:01,461 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-04 16:35:01,555 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-04 16:35:01,647 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-04 16:35:01,743 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-04 16:35:02,091 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,589 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,873 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:02,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,348 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,449 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,546 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,739 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:03,935 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,028 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,120 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,213 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,406 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,503 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,694 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,886 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:04,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,088 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,181 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,274 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,462 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,554 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,771 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,869 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:05,974 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,077 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,282 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,380 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,745 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:06,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,114 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,235 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,352 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,455 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,552 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,743 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,840 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:07,932 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,018 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,106 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,191 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,369 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,458 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,638 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,800 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:08,946 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:09,091 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:09,236 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:09,385 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:09,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:09,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:09,778 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:09,913 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:10,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:10,158 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:10,313 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:10,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:10,629 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:10,786 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:10,939 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,175 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,270 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,566 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,675 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 89: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 90: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:11,910 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 91: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,028 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 92: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,146 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 93: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,262 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 94: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,380 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 95: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 96: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,590 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 97: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 98: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:12,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 99: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:35:13,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 0: ref_distribution = tensor([0.1000, 0.3000, 0.6000], device='cuda:0'), new_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0')
2024-12-04 16:35:13,270 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 1: ref_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0'), new_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0')
2024-12-04 16:35:13,389 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 2: ref_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0'), new_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0')
2024-12-04 16:35:13,506 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 3: ref_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0'), new_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0')
2024-12-04 16:35:13,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 4: ref_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0'), new_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0')
2024-12-04 16:35:13,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 5: ref_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0'), new_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0')
2024-12-04 16:35:13,860 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 6: ref_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0'), new_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0')
2024-12-04 16:35:13,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 7: ref_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0'), new_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0')
2024-12-04 16:35:14,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 8: ref_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0'), new_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0')
2024-12-04 16:35:14,160 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 9: ref_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0'), new_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0')
2024-12-04 16:35:14,255 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 10: ref_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0'), new_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0')
2024-12-04 16:35:14,346 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 11: ref_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0'), new_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0')
2024-12-04 16:35:14,449 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 12: ref_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0'), new_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0')
2024-12-04 16:35:14,567 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 13: ref_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0'), new_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0')
2024-12-04 16:35:14,683 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 14: ref_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0'), new_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0')
2024-12-04 16:35:14,801 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 15: ref_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0'), new_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0')
2024-12-04 16:35:14,918 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 16: ref_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0'), new_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0')
2024-12-04 16:35:15,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 17: ref_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0'), new_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0')
2024-12-04 16:35:15,159 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 18: ref_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0'), new_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0')
2024-12-04 16:35:15,267 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 19: ref_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0'), new_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0')
2024-12-04 16:35:15,359 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 20: ref_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0'), new_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0')
2024-12-04 16:35:15,459 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 21: ref_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0'), new_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0')
2024-12-04 16:35:15,559 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 22: ref_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0'), new_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0')
2024-12-04 16:35:15,651 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 23: ref_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0'), new_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0')
2024-12-04 16:35:15,758 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 24: ref_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0'), new_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0')
2024-12-04 16:35:15,875 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 25: ref_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0'), new_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0')
2024-12-04 16:35:15,992 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 26: ref_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0'), new_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0')
2024-12-04 16:35:16,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 27: ref_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0'), new_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0')
2024-12-04 16:35:16,227 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 28: ref_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0'), new_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0')
2024-12-04 16:35:16,351 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 29: ref_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0'), new_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0')
2024-12-04 16:35:16,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 30: ref_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0'), new_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0')
2024-12-04 16:35:16,578 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 31: ref_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0'), new_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0')
2024-12-04 16:35:16,670 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 32: ref_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0'), new_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0')
2024-12-04 16:35:16,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 33: ref_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0'), new_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0')
2024-12-04 16:35:16,892 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 34: ref_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0'), new_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0')
2024-12-04 16:35:17,015 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 35: ref_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0'), new_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0')
2024-12-04 16:35:17,154 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 36: ref_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0'), new_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0')
2024-12-04 16:35:17,301 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 37: ref_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0'), new_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0')
2024-12-04 16:35:17,440 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 38: ref_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0'), new_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0')
2024-12-04 16:35:17,588 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 39: ref_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0'), new_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0')
2024-12-04 16:35:17,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 40: ref_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0'), new_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0')
2024-12-04 16:35:17,894 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 41: ref_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0'), new_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0')
2024-12-04 16:35:18,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 42: ref_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0'), new_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0')
2024-12-04 16:35:18,202 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 43: ref_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0'), new_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0')
2024-12-04 16:35:18,350 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 44: ref_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0'), new_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0')
2024-12-04 16:35:18,506 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 45: ref_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0'), new_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0')
2024-12-04 16:35:18,637 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 46: ref_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0'), new_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0')
2024-12-04 16:35:18,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 47: ref_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0'), new_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0')
2024-12-04 16:35:18,853 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 48: ref_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0'), new_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0')
2024-12-04 16:35:18,958 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 49: ref_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0'), new_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0')
2024-12-04 16:35:19,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 50: ref_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0'), new_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0')
2024-12-04 16:35:19,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 51: ref_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0'), new_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0')
2024-12-04 16:35:19,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 52: ref_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0'), new_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0')
2024-12-04 16:35:19,350 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 53: ref_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0'), new_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0')
2024-12-04 16:35:19,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 54: ref_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0'), new_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0')
2024-12-04 16:35:19,543 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 55: ref_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0'), new_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0')
2024-12-04 16:35:19,646 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 56: ref_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0'), new_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0')
2024-12-04 16:35:19,738 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 57: ref_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0'), new_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0')
2024-12-04 16:35:19,835 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 58: ref_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0'), new_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0')
2024-12-04 16:35:19,930 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 59: ref_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0'), new_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0')
2024-12-04 16:35:20,034 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 60: ref_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0'), new_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0')
2024-12-04 16:35:20,131 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 61: ref_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0'), new_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0')
2024-12-04 16:35:20,231 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 62: ref_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0'), new_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0')
2024-12-04 16:35:20,330 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 63: ref_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0'), new_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0')
2024-12-04 16:35:20,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 64: ref_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0'), new_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0')
2024-12-04 16:35:20,524 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 65: ref_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0'), new_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0')
2024-12-04 16:35:20,623 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 66: ref_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0'), new_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0')
2024-12-04 16:35:20,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 67: ref_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0'), new_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0')
2024-12-04 16:35:20,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 68: ref_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0'), new_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0')
2024-12-04 16:35:20,913 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 69: ref_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0'), new_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0')
2024-12-04 16:35:21,006 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 70: ref_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0'), new_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0')
2024-12-04 16:35:21,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 71: ref_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0'), new_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0')
2024-12-04 16:35:21,191 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 72: ref_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0'), new_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0')
2024-12-04 16:35:21,279 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 73: ref_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0'), new_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0')
2024-12-04 16:35:21,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 74: ref_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0'), new_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0')
2024-12-04 16:35:21,469 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 75: ref_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0'), new_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0')
2024-12-04 16:35:21,572 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 76: ref_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0'), new_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0')
2024-12-04 16:35:21,668 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 77: ref_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0'), new_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0')
2024-12-04 16:35:21,767 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 78: ref_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0'), new_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0')
2024-12-04 16:35:21,871 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 79: ref_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0'), new_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0')
2024-12-04 16:35:21,966 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 80: ref_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0'), new_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0')
2024-12-04 16:35:22,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 81: ref_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0'), new_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0')
2024-12-04 16:35:22,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 82: ref_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0'), new_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0')
2024-12-04 16:35:22,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 83: ref_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0'), new_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0')
2024-12-04 16:35:22,409 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 84: ref_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0'), new_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0')
2024-12-04 16:35:22,543 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 85: ref_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0'), new_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0')
2024-12-04 16:35:22,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 86: ref_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0'), new_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0')
2024-12-04 16:35:22,772 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 87: ref_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0'), new_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0')
2024-12-04 16:35:22,874 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 88: ref_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0'), new_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0')
2024-12-04 16:35:22,974 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 89: ref_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0'), new_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0')
2024-12-04 16:35:23,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 90: ref_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0'), new_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0')
2024-12-04 16:35:23,174 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 91: ref_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0'), new_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0')
2024-12-04 16:35:23,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 92: ref_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0'), new_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0')
2024-12-04 16:35:23,380 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 93: ref_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0'), new_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0')
2024-12-04 16:35:23,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 94: ref_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0'), new_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0')
2024-12-04 16:35:23,585 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 95: ref_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0'), new_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0')
2024-12-04 16:35:23,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 96: ref_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0'), new_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0')
2024-12-04 16:35:23,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 97: ref_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0'), new_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0')
2024-12-04 16:35:23,873 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 98: ref_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0'), new_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0')
2024-12-04 16:35:23,965 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 99: ref_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0'), new_distribution = tensor([0.1051, 0.3544, 0.5405], device='cuda:0')
