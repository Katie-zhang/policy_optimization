2024-12-04 16:49:07,660 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-04 16:49:07,668 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-04 16:49:07,674 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-04 16:49:07,681 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-04 16:49:07,687 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-04 16:49:07,693 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-04 16:49:07,699 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-04 16:49:07,706 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-04 16:49:07,712 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-04 16:49:07,718 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-04 16:49:08,152 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: -0.2711 reward: 0.2711 ref_reward: 0.2734 improvement: -0.82%
2024-12-04 16:49:08,691 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: -0.2769 reward: 0.2769 ref_reward: 0.2734 improvement: 1.28%
2024-12-04 16:49:09,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: -0.2802 reward: 0.2802 ref_reward: 0.2734 improvement: 2.48%
2024-12-04 16:49:09,509 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: -0.2821 reward: 0.2821 ref_reward: 0.2734 improvement: 3.20%
2024-12-04 16:49:09,801 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.57%
2024-12-04 16:49:10,097 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:10,385 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 16:49:10,679 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: -0.2829 reward: 0.2829 ref_reward: 0.2734 improvement: 3.46%
2024-12-04 16:49:10,967 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: -0.2826 reward: 0.2826 ref_reward: 0.2734 improvement: 3.38%
2024-12-04 16:49:11,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: -0.2826 reward: 0.2826 ref_reward: 0.2734 improvement: 3.38%
2024-12-04 16:49:11,551 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: -0.2828 reward: 0.2828 ref_reward: 0.2734 improvement: 3.44%
2024-12-04 16:49:11,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: -0.2830 reward: 0.2830 ref_reward: 0.2734 improvement: 3.53%
2024-12-04 16:49:12,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.60%
2024-12-04 16:49:12,433 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 16:49:12,726 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:13,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:13,323 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.64%
2024-12-04 16:49:13,623 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.63%
2024-12-04 16:49:13,910 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.62%
2024-12-04 16:49:14,206 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.63%
2024-12-04 16:49:14,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.64%
2024-12-04 16:49:14,788 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 16:49:15,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:15,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:15,663 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:15,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:16,241 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:16,527 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:16,822 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:17,109 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:17,402 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:17,689 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:17,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:18,271 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:18,566 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:18,853 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:19,149 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 16:49:19,618 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:20,072 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:20,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:20,960 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:21,389 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:21,815 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:22,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:22,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:22,846 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:23,137 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:23,430 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:23,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:24,014 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 16:49:24,564 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: 0.0771 reward: -0.0771 ref_reward: 0.3541 improvement: -121.78%
2024-12-04 16:49:24,855 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: 0.0448 reward: -0.0448 ref_reward: 0.3541 improvement: -112.66%
2024-12-04 16:49:25,144 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: 0.0143 reward: -0.0143 ref_reward: 0.3541 improvement: -104.03%
2024-12-04 16:49:25,444 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: -0.0151 reward: 0.0151 ref_reward: 0.3541 improvement: -95.72%
2024-12-04 16:49:25,745 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: -0.0438 reward: 0.0438 ref_reward: 0.3541 improvement: -87.62%
2024-12-04 16:49:26,059 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: -0.0720 reward: 0.0720 ref_reward: 0.3541 improvement: -79.66%
2024-12-04 16:49:26,354 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: -0.0953 reward: 0.0953 ref_reward: 0.3541 improvement: -73.07%
2024-12-04 16:49:26,658 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: -0.1183 reward: 0.1183 ref_reward: 0.3541 improvement: -66.58%
2024-12-04 16:49:26,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: -0.1414 reward: 0.1414 ref_reward: 0.3541 improvement: -60.06%
2024-12-04 16:49:27,249 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: -0.1641 reward: 0.1641 ref_reward: 0.3541 improvement: -53.67%
2024-12-04 16:49:27,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: -0.1865 reward: 0.1865 ref_reward: 0.3541 improvement: -47.34%
2024-12-04 16:49:27,832 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: -0.2086 reward: 0.2086 ref_reward: 0.3541 improvement: -41.10%
2024-12-04 16:49:28,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: -0.2299 reward: 0.2299 ref_reward: 0.3541 improvement: -35.06%
2024-12-04 16:49:28,422 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: -0.2502 reward: 0.2502 ref_reward: 0.3541 improvement: -29.34%
2024-12-04 16:49:28,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: -0.2690 reward: 0.2690 ref_reward: 0.3541 improvement: -24.04%
2024-12-04 16:49:29,019 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: -0.2859 reward: 0.2859 ref_reward: 0.3541 improvement: -19.27%
2024-12-04 16:49:29,316 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: -0.3006 reward: 0.3006 ref_reward: 0.3541 improvement: -15.11%
2024-12-04 16:49:29,611 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: -0.3130 reward: 0.3130 ref_reward: 0.3541 improvement: -11.61%
2024-12-04 16:49:29,910 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: -0.3230 reward: 0.3230 ref_reward: 0.3541 improvement: -8.79%
2024-12-04 16:49:30,205 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: -0.3309 reward: 0.3309 ref_reward: 0.3541 improvement: -6.54%
2024-12-04 16:49:30,503 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: -0.3371 reward: 0.3371 ref_reward: 0.3541 improvement: -4.78%
2024-12-04 16:49:30,795 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: -0.3419 reward: 0.3419 ref_reward: 0.3541 improvement: -3.44%
2024-12-04 16:49:31,099 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: -0.3455 reward: 0.3455 ref_reward: 0.3541 improvement: -2.41%
2024-12-04 16:49:31,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: -0.3483 reward: 0.3483 ref_reward: 0.3541 improvement: -1.64%
2024-12-04 16:49:31,695 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: -0.3504 reward: 0.3504 ref_reward: 0.3541 improvement: -1.03%
2024-12-04 16:49:31,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: -0.3521 reward: 0.3521 ref_reward: 0.3541 improvement: -0.55%
2024-12-04 16:49:32,291 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: -0.3535 reward: 0.3535 ref_reward: 0.3541 improvement: -0.15%
2024-12-04 16:49:32,581 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: -0.3547 reward: 0.3547 ref_reward: 0.3541 improvement: 0.17%
2024-12-04 16:49:32,874 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: -0.3556 reward: 0.3556 ref_reward: 0.3541 improvement: 0.43%
2024-12-04 16:49:33,167 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: -0.3563 reward: 0.3563 ref_reward: 0.3541 improvement: 0.63%
2024-12-04 16:49:33,460 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: -0.3568 reward: 0.3568 ref_reward: 0.3541 improvement: 0.76%
2024-12-04 16:49:33,754 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 16:49:34,048 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 16:49:34,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 16:49:34,640 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 16:49:34,943 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.82%
2024-12-04 16:49:35,252 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.80%
2024-12-04 16:49:35,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.79%
2024-12-04 16:49:35,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.80%
2024-12-04 16:49:36,141 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.81%
2024-12-04 16:49:36,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.82%
2024-12-04 16:49:36,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 16:49:37,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 16:49:37,341 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 16:49:37,638 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 16:49:37,935 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 16:49:38,230 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 16:49:38,523 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 16:49:38,820 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 16:49:39,118 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 16:49:39,651 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: 14.2482 reward: -14.2482 ref_reward: 0.3861 improvement: -3790.67%
2024-12-04 16:49:39,946 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: 13.7537 reward: -13.7537 ref_reward: 0.3861 improvement: -3662.57%
2024-12-04 16:49:40,244 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: 13.2390 reward: -13.2390 ref_reward: 0.3861 improvement: -3529.25%
2024-12-04 16:49:40,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: 12.7036 reward: -12.7036 ref_reward: 0.3861 improvement: -3390.57%
2024-12-04 16:49:40,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: 12.1488 reward: -12.1488 ref_reward: 0.3861 improvement: -3246.87%
2024-12-04 16:49:41,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: 11.5797 reward: -11.5797 ref_reward: 0.3861 improvement: -3099.43%
2024-12-04 16:49:41,427 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: 10.9832 reward: -10.9832 ref_reward: 0.3861 improvement: -2944.93%
2024-12-04 16:49:41,723 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: 10.3735 reward: -10.3735 ref_reward: 0.3861 improvement: -2787.00%
2024-12-04 16:49:42,019 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: 9.7479 reward: -9.7479 ref_reward: 0.3861 improvement: -2624.97%
2024-12-04 16:49:42,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: 9.0877 reward: -9.0877 ref_reward: 0.3861 improvement: -2453.94%
2024-12-04 16:49:42,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: 8.3923 reward: -8.3923 ref_reward: 0.3861 improvement: -2273.83%
2024-12-04 16:49:42,909 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: 7.6681 reward: -7.6681 ref_reward: 0.3861 improvement: -2086.23%
2024-12-04 16:49:43,205 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: 6.9296 reward: -6.9296 ref_reward: 0.3861 improvement: -1894.96%
2024-12-04 16:49:43,501 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: 6.1954 reward: -6.1954 ref_reward: 0.3861 improvement: -1704.77%
2024-12-04 16:49:43,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: 5.4706 reward: -5.4706 ref_reward: 0.3861 improvement: -1517.02%
2024-12-04 16:49:44,095 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: 4.7639 reward: -4.7639 ref_reward: 0.3861 improvement: -1333.96%
2024-12-04 16:49:44,393 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: 4.0858 reward: -4.0858 ref_reward: 0.3861 improvement: -1158.34%
2024-12-04 16:49:44,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: 3.4505 reward: -3.4505 ref_reward: 0.3861 improvement: -993.77%
2024-12-04 16:49:44,989 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: 2.8695 reward: -2.8695 ref_reward: 0.3861 improvement: -843.27%
2024-12-04 16:49:45,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: 2.3507 reward: -2.3507 ref_reward: 0.3861 improvement: -708.90%
2024-12-04 16:49:45,588 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: 1.8980 reward: -1.8980 ref_reward: 0.3861 improvement: -591.64%
2024-12-04 16:49:45,884 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: 1.5111 reward: -1.5111 ref_reward: 0.3861 improvement: -491.42%
2024-12-04 16:49:46,181 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: 1.1863 reward: -1.1863 ref_reward: 0.3861 improvement: -407.29%
2024-12-04 16:49:46,487 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: 0.9177 reward: -0.9177 ref_reward: 0.3861 improvement: -337.70%
2024-12-04 16:49:46,784 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: 0.6980 reward: -0.6980 ref_reward: 0.3861 improvement: -280.80%
2024-12-04 16:49:47,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: 0.5197 reward: -0.5197 ref_reward: 0.3861 improvement: -234.62%
2024-12-04 16:49:47,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: 0.3757 reward: -0.3757 ref_reward: 0.3861 improvement: -197.32%
2024-12-04 16:49:47,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: 0.2596 reward: -0.2596 ref_reward: 0.3861 improvement: -167.25%
2024-12-04 16:49:47,969 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: 0.1660 reward: -0.1660 ref_reward: 0.3861 improvement: -142.99%
2024-12-04 16:49:48,262 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: 0.0902 reward: -0.0902 ref_reward: 0.3861 improvement: -123.37%
2024-12-04 16:49:48,554 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: 0.0287 reward: -0.0287 ref_reward: 0.3861 improvement: -107.43%
2024-12-04 16:49:48,852 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: -0.0216 reward: 0.0216 ref_reward: 0.3861 improvement: -94.40%
2024-12-04 16:49:49,147 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.0629 reward: 0.0629 ref_reward: 0.3861 improvement: -83.70%
2024-12-04 16:49:49,442 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.0971 reward: 0.0971 ref_reward: 0.3861 improvement: -74.85%
2024-12-04 16:49:49,739 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.1256 reward: 0.1256 ref_reward: 0.3861 improvement: -67.47%
2024-12-04 16:49:50,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.1495 reward: 0.1495 ref_reward: 0.3861 improvement: -61.27%
2024-12-04 16:49:50,331 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.1698 reward: 0.1698 ref_reward: 0.3861 improvement: -56.02%
2024-12-04 16:49:50,628 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.1871 reward: 0.1871 ref_reward: 0.3861 improvement: -51.55%
2024-12-04 16:49:50,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.2019 reward: 0.2019 ref_reward: 0.3861 improvement: -47.70%
2024-12-04 16:49:51,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.2147 reward: 0.2147 ref_reward: 0.3861 improvement: -44.38%
2024-12-04 16:49:51,517 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.2259 reward: 0.2259 ref_reward: 0.3861 improvement: -41.48%
2024-12-04 16:49:51,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.2358 reward: 0.2358 ref_reward: 0.3861 improvement: -38.93%
2024-12-04 16:49:52,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.2444 reward: 0.2444 ref_reward: 0.3861 improvement: -36.68%
2024-12-04 16:49:52,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.2522 reward: 0.2522 ref_reward: 0.3861 improvement: -34.68%
2024-12-04 16:49:52,701 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.2591 reward: 0.2591 ref_reward: 0.3861 improvement: -32.89%
2024-12-04 16:49:52,999 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.2653 reward: 0.2653 ref_reward: 0.3861 improvement: -31.28%
2024-12-04 16:49:53,295 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.2710 reward: 0.2710 ref_reward: 0.3861 improvement: -29.82%
2024-12-04 16:49:53,596 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.2761 reward: 0.2761 ref_reward: 0.3861 improvement: -28.49%
2024-12-04 16:49:53,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.2808 reward: 0.2808 ref_reward: 0.3861 improvement: -27.27%
2024-12-04 16:49:54,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.2851 reward: 0.2851 ref_reward: 0.3861 improvement: -26.16%
2024-12-04 16:49:54,935 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 0 loss: -0.0043 reward: 0.0043 ref_reward: 0.1811 improvement: -97.60%
2024-12-04 16:49:55,232 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 2 loss: -0.0431 reward: 0.0431 ref_reward: 0.1811 improvement: -76.21%
2024-12-04 16:49:55,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 4 loss: -0.0757 reward: 0.0757 ref_reward: 0.1811 improvement: -58.19%
2024-12-04 16:49:55,822 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 6 loss: -0.1033 reward: 0.1033 ref_reward: 0.1811 improvement: -42.95%
2024-12-04 16:49:56,117 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 8 loss: -0.1262 reward: 0.1262 ref_reward: 0.1811 improvement: -30.32%
2024-12-04 16:49:56,412 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 10 loss: -0.1442 reward: 0.1442 ref_reward: 0.1811 improvement: -20.38%
2024-12-04 16:49:56,708 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 12 loss: -0.1581 reward: 0.1581 ref_reward: 0.1811 improvement: -12.71%
2024-12-04 16:49:57,004 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 14 loss: -0.1688 reward: 0.1688 ref_reward: 0.1811 improvement: -6.82%
2024-12-04 16:49:57,296 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 16 loss: -0.1768 reward: 0.1768 ref_reward: 0.1811 improvement: -2.41%
2024-12-04 16:49:57,593 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 18 loss: -0.1825 reward: 0.1825 ref_reward: 0.1811 improvement: 0.73%
2024-12-04 16:49:57,888 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 20 loss: -0.1863 reward: 0.1863 ref_reward: 0.1811 improvement: 2.88%
2024-12-04 16:49:58,180 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 22 loss: -0.1889 reward: 0.1889 ref_reward: 0.1811 improvement: 4.30%
2024-12-04 16:49:58,474 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 24 loss: -0.1906 reward: 0.1906 ref_reward: 0.1811 improvement: 5.21%
2024-12-04 16:49:58,768 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 26 loss: -0.1915 reward: 0.1915 ref_reward: 0.1811 improvement: 5.73%
2024-12-04 16:49:59,063 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 28 loss: -0.1920 reward: 0.1920 ref_reward: 0.1811 improvement: 5.98%
2024-12-04 16:49:59,358 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 30 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.07%
2024-12-04 16:49:59,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 32 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.04%
2024-12-04 16:49:59,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 34 loss: -0.1919 reward: 0.1919 ref_reward: 0.1811 improvement: 5.94%
2024-12-04 16:50:00,244 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 36 loss: -0.1917 reward: 0.1917 ref_reward: 0.1811 improvement: 5.83%
2024-12-04 16:50:00,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 38 loss: -0.1915 reward: 0.1915 ref_reward: 0.1811 improvement: 5.71%
2024-12-04 16:50:00,835 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 40 loss: -0.1913 reward: 0.1913 ref_reward: 0.1811 improvement: 5.64%
2024-12-04 16:50:01,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 42 loss: -0.1913 reward: 0.1913 ref_reward: 0.1811 improvement: 5.61%
2024-12-04 16:50:01,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 44 loss: -0.1913 reward: 0.1913 ref_reward: 0.1811 improvement: 5.64%
2024-12-04 16:50:01,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 46 loss: -0.1915 reward: 0.1915 ref_reward: 0.1811 improvement: 5.71%
2024-12-04 16:50:02,016 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 48 loss: -0.1916 reward: 0.1916 ref_reward: 0.1811 improvement: 5.80%
2024-12-04 16:50:02,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 50 loss: -0.1918 reward: 0.1918 ref_reward: 0.1811 improvement: 5.90%
2024-12-04 16:50:02,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 52 loss: -0.1920 reward: 0.1920 ref_reward: 0.1811 improvement: 5.99%
2024-12-04 16:50:02,899 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 54 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.07%
2024-12-04 16:50:03,195 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 56 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.12%
2024-12-04 16:50:03,491 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 58 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.17%
2024-12-04 16:50:03,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 60 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.19%
2024-12-04 16:50:04,082 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 62 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:04,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 64 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:04,678 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 66 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:50:05,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 68 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:05,384 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 70 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:05,952 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 72 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:06,518 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 74 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:50:07,114 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 76 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:50:07,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 78 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:50:08,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 80 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 16:50:08,398 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 82 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:08,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 84 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:09,079 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 86 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 16:50:09,424 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 88 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:50:09,763 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 90 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:50:10,105 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 92 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:50:10,448 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 94 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:50:10,799 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 96 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:50:11,129 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:214] - INFO: [Policy] Epoch 98 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 16:50:12,527 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 0.6672 grad norm: 0.4209 policy: 0.3436 0.3753
2024-12-04 16:50:13,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 0.6312 grad norm: 0.2798 policy: 0.3654 0.4132
2024-12-04 16:50:14,279 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 0.6130 grad norm: 0.1891 policy: 0.3845 0.4380
2024-12-04 16:50:15,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 0.6039 grad norm: 0.1247 policy: 0.4175 0.4322
2024-12-04 16:50:16,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 0.5994 grad norm: 0.0484 policy: 0.4595 0.4072
2024-12-04 16:50:16,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 0.5990 grad norm: 0.0265 policy: 0.4924 0.3842
2024-12-04 16:50:17,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 0.5999 grad norm: 0.0624 policy: 0.5064 0.3745
2024-12-04 16:50:18,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 0.5998 grad norm: 0.0607 policy: 0.5018 0.3791
2024-12-04 16:50:19,431 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 0.5993 grad norm: 0.0391 policy: 0.4886 0.3891
2024-12-04 16:50:20,270 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 0.5989 grad norm: 0.0176 policy: 0.4769 0.3958
2024-12-04 16:50:21,129 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 0.5988 grad norm: 0.0071 policy: 0.4708 0.3971
2024-12-04 16:50:22,007 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 0.5989 grad norm: 0.0145 policy: 0.4690 0.3960
2024-12-04 16:50:22,879 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 0.5989 grad norm: 0.0161 policy: 0.4694 0.3956
2024-12-04 16:50:23,753 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0107 policy: 0.4709 0.3960
2024-12-04 16:50:24,611 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0043 policy: 0.4733 0.3956
2024-12-04 16:50:25,462 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0029 policy: 0.4759 0.3942
2024-12-04 16:50:26,313 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0048 policy: 0.4774 0.3930
2024-12-04 16:50:27,144 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0046 policy: 0.4771 0.3929
2024-12-04 16:50:28,007 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0023 policy: 0.4758 0.3938
2024-12-04 16:50:28,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0007 policy: 0.4746 0.3946
2024-12-04 16:50:30,141 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 1.0660 grad norm: 0.8587 policy: 0.3396 0.3704
2024-12-04 16:50:31,300 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 0.9878 grad norm: 0.8298 policy: 0.3741 0.3831
2024-12-04 16:50:32,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 0.9156 grad norm: 0.8473 policy: 0.4111 0.3883
2024-12-04 16:50:33,167 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 0.8484 grad norm: 0.8508 policy: 0.4554 0.3789
2024-12-04 16:50:34,030 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 0.7837 grad norm: 0.8428 policy: 0.5110 0.3542
2024-12-04 16:50:34,889 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 0.7224 grad norm: 0.7794 policy: 0.5748 0.3188
2024-12-04 16:50:35,734 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 0.6694 grad norm: 0.6558 policy: 0.6431 0.2759
2024-12-04 16:50:36,591 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 0.6302 grad norm: 0.4735 policy: 0.7064 0.2339
2024-12-04 16:50:37,431 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 0.6078 grad norm: 0.2660 policy: 0.7531 0.2035
2024-12-04 16:50:38,265 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 0.5994 grad norm: 0.0730 policy: 0.7804 0.1869
2024-12-04 16:50:39,109 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 0.5994 grad norm: 0.0722 policy: 0.7949 0.1785
2024-12-04 16:50:39,935 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 0.6013 grad norm: 0.1466 policy: 0.8032 0.1725
2024-12-04 16:50:40,765 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 0.6018 grad norm: 0.1607 policy: 0.8066 0.1693
2024-12-04 16:50:41,581 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 0.6009 grad norm: 0.1346 policy: 0.8034 0.1711
2024-12-04 16:50:42,420 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 0.5997 grad norm: 0.0864 policy: 0.7953 0.1772
2024-12-04 16:50:43,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 0.5990 grad norm: 0.0366 policy: 0.7870 0.1835
2024-12-04 16:50:44,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0052 policy: 0.7817 0.1872
2024-12-04 16:50:44,937 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 0.5989 grad norm: 0.0245 policy: 0.7794 0.1885
2024-12-04 16:50:45,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 0.5989 grad norm: 0.0321 policy: 0.7788 0.1888
2024-12-04 16:50:46,547 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 0.5989 grad norm: 0.0283 policy: 0.7792 0.1887
2024-12-04 16:50:47,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 9.0911 grad norm: 1.0768 policy: 0.3482 0.3402
2024-12-04 16:50:48,712 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 8.9925 grad norm: 1.0579 policy: 0.4000 0.3320
2024-12-04 16:50:50,070 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 8.9009 grad norm: 1.1030 policy: 0.4523 0.3159
2024-12-04 16:50:50,975 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 8.8122 grad norm: 1.2116 policy: 0.5078 0.2901
2024-12-04 16:50:51,821 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 8.7108 grad norm: 1.3594 policy: 0.5732 0.2558
2024-12-04 16:50:52,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 8.5920 grad norm: 1.5253 policy: 0.6468 0.2150
2024-12-04 16:50:53,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 8.4534 grad norm: 1.7082 policy: 0.7249 0.1701
2024-12-04 16:50:54,389 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 8.2923 grad norm: 1.9076 policy: 0.8010 0.1249
2024-12-04 16:50:55,252 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 8.1060 grad norm: 2.1233 policy: 0.8680 0.0841
2024-12-04 16:50:56,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 7.8919 grad norm: 2.3548 policy: 0.9204 0.0514
2024-12-04 16:50:56,946 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 7.6476 grad norm: 2.6023 policy: 0.9566 0.0284
2024-12-04 16:50:57,789 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 7.3701 grad norm: 2.8656 policy: 0.9787 0.0142
2024-12-04 16:50:58,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 7.0570 grad norm: 3.1447 policy: 0.9906 0.0064
2024-12-04 16:50:59,509 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 6.7057 grad norm: 3.4400 policy: 0.9962 0.0026
2024-12-04 16:51:00,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 6.3136 grad norm: 3.7513 policy: 0.9987 0.0009
2024-12-04 16:51:01,240 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 5.8782 grad norm: 4.0790 policy: 0.9996 0.0003
2024-12-04 16:51:02,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 5.3969 grad norm: 4.4232 policy: 0.9999 0.0001
2024-12-04 16:51:02,922 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 4.8672 grad norm: 4.7839 policy: 1.0000 0.0000
2024-12-04 16:51:03,759 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 4.2865 grad norm: 5.1611 policy: 1.0000 0.0000
2024-12-04 16:51:04,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 3.6524 grad norm: 5.5537 policy: 1.0000 0.0000
2024-12-04 16:51:05,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 0 loss: 0.6264 grad norm: 0.2162 policy: 0.2892 0.3776
2024-12-04 16:51:06,464 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 5 loss: 0.6107 grad norm: 0.1476 policy: 0.2587 0.4266
2024-12-04 16:51:07,326 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 10 loss: 0.6021 grad norm: 0.0824 policy: 0.2260 0.4684
2024-12-04 16:51:08,176 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 15 loss: 0.5991 grad norm: 0.0232 policy: 0.1959 0.4957
2024-12-04 16:51:09,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 20 loss: 0.5996 grad norm: 0.0524 policy: 0.1773 0.5026
2024-12-04 16:51:09,895 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 25 loss: 0.6000 grad norm: 0.0605 policy: 0.1753 0.4956
2024-12-04 16:51:10,872 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 30 loss: 0.5992 grad norm: 0.0330 policy: 0.1847 0.4872
2024-12-04 16:51:12,275 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 35 loss: 0.5988 grad norm: 0.0033 policy: 0.1959 0.4821
2024-12-04 16:51:13,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 40 loss: 0.5989 grad norm: 0.0174 policy: 0.2022 0.4803
2024-12-04 16:51:14,114 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 45 loss: 0.5990 grad norm: 0.0196 policy: 0.2020 0.4805
2024-12-04 16:51:14,927 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 50 loss: 0.5989 grad norm: 0.0111 policy: 0.1978 0.4820
2024-12-04 16:51:15,770 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0017 policy: 0.1933 0.4840
2024-12-04 16:51:16,614 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0077 policy: 0.1914 0.4859
2024-12-04 16:51:17,464 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0073 policy: 0.1922 0.4863
2024-12-04 16:51:18,322 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0026 policy: 0.1942 0.4851
2024-12-04 16:51:19,174 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0023 policy: 0.1956 0.4836
2024-12-04 16:51:20,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0036 policy: 0.1956 0.4831
2024-12-04 16:51:20,899 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0019 policy: 0.1949 0.4837
2024-12-04 16:51:21,732 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0006 policy: 0.1942 0.4846
2024-12-04 16:51:22,593 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:343] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0017 policy: 0.1940 0.4848
2024-12-04 16:51:24,386 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 0.0064 grad norm: 0.2670 
2024-12-04 16:51:25,242 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.0166 grad norm: 0.3567 
2024-12-04 16:51:26,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.0015 grad norm: 0.1135 
2024-12-04 16:51:26,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0001 grad norm: 0.0232 
2024-12-04 16:51:27,796 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0024 grad norm: 0.1206 
2024-12-04 16:51:28,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0007 grad norm: 0.0717 
2024-12-04 16:51:29,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0000 grad norm: 0.0158 
2024-12-04 16:51:30,393 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0005 grad norm: 0.0573 
2024-12-04 16:51:31,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0001 grad norm: 0.0251 
2024-12-04 16:51:32,109 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0126 
2024-12-04 16:51:32,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0265 
2024-12-04 16:51:33,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0175 
2024-12-04 16:51:34,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0021 
2024-12-04 16:51:35,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0110 
2024-12-04 16:51:36,318 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0080 
2024-12-04 16:51:37,151 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0026 
2024-12-04 16:51:37,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0018 
2024-12-04 16:51:38,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0034 
2024-12-04 16:51:39,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0032 
2024-12-04 16:51:40,414 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0010 
2024-12-04 16:51:41,473 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 1.4305 grad norm: 3.4353 
2024-12-04 16:51:42,302 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.0347 grad norm: 0.7002 
2024-12-04 16:51:43,148 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.0027 grad norm: 0.1392 
2024-12-04 16:51:44,004 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0083 grad norm: 0.2136 
2024-12-04 16:51:44,869 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0021 grad norm: 0.1145 
2024-12-04 16:51:45,730 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0003 grad norm: 0.0490 
2024-12-04 16:51:46,584 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0004 grad norm: 0.0547 
2024-12-04 16:51:47,427 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0001 grad norm: 0.0237 
2024-12-04 16:51:48,250 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0122 
2024-12-04 16:51:49,073 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0122 
2024-12-04 16:51:49,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0103 
2024-12-04 16:51:50,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0044 
2024-12-04 16:51:51,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0060 
2024-12-04 16:51:52,549 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0049 
2024-12-04 16:51:53,378 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0027 
2024-12-04 16:51:54,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0009 
2024-12-04 16:51:55,076 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0019 
2024-12-04 16:51:55,922 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0014 
2024-12-04 16:51:56,776 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0002 
2024-12-04 16:51:57,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0008 
2024-12-04 16:51:58,712 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 672.8380 grad norm: 60.0237 
2024-12-04 16:51:59,563 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.2019 grad norm: 2.3227 
2024-12-04 16:52:00,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.2565 grad norm: 2.8186 
2024-12-04 16:52:01,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0857 grad norm: 1.4131 
2024-12-04 16:52:02,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0086 grad norm: 0.3880 
2024-12-04 16:52:02,951 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0050 grad norm: 0.2536 
2024-12-04 16:52:03,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0144 grad norm: 0.4346 
2024-12-04 16:52:04,619 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0003 grad norm: 0.0664 
2024-12-04 16:52:05,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0007 grad norm: 0.0960 
2024-12-04 16:52:06,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0005 grad norm: 0.0811 
2024-12-04 16:52:07,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0002 grad norm: 0.0446 
2024-12-04 16:52:07,942 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0103 
2024-12-04 16:52:08,767 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0195 
2024-12-04 16:52:09,623 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0091 
2024-12-04 16:52:10,490 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0131 
2024-12-04 16:52:11,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0106 
2024-12-04 16:52:12,065 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0049 
2024-12-04 16:52:12,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0010 
2024-12-04 16:52:13,513 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0036 
2024-12-04 16:52:14,244 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0012 
2024-12-04 16:52:15,195 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 0 loss: 1.2875 grad norm: 3.6599 
2024-12-04 16:52:15,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 5 loss: 0.0384 grad norm: 0.6972 
2024-12-04 16:52:16,654 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 10 loss: 0.0006 grad norm: 0.0634 
2024-12-04 16:52:17,383 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 15 loss: 0.0085 grad norm: 0.1919 
2024-12-04 16:52:18,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 20 loss: 0.0061 grad norm: 0.1626 
2024-12-04 16:52:18,848 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 25 loss: 0.0008 grad norm: 0.0659 
2024-12-04 16:52:19,574 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 30 loss: 0.0006 grad norm: 0.0602 
2024-12-04 16:52:20,304 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 35 loss: 0.0004 grad norm: 0.0500 
2024-12-04 16:52:21,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 40 loss: 0.0002 grad norm: 0.0386 
2024-12-04 16:52:21,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0084 
2024-12-04 16:52:23,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0263 
2024-12-04 16:52:24,640 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0107 
2024-12-04 16:52:25,470 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0110 
2024-12-04 16:52:26,209 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0058 
2024-12-04 16:52:26,946 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0032 
2024-12-04 16:52:27,684 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0023 
2024-12-04 16:52:28,418 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0027 
2024-12-04 16:52:29,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0007 
2024-12-04 16:52:29,889 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0018 
2024-12-04 16:52:30,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:467] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0005 
2024-12-04 16:52:32,059 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-04 16:52:32,149 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-04 16:52:32,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-04 16:52:32,328 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-04 16:52:32,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-04 16:52:32,504 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-04 16:52:32,594 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-04 16:52:32,686 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-04 16:52:32,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-04 16:52:32,865 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-04 16:52:32,954 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-04 16:52:33,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-04 16:52:33,134 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-04 16:52:33,223 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-04 16:52:33,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-04 16:52:33,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-04 16:52:33,490 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-04 16:52:33,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-04 16:52:33,668 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-04 16:52:33,757 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-04 16:52:33,847 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-04 16:52:33,936 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-04 16:52:34,026 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-04 16:52:34,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-04 16:52:34,205 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-04 16:52:34,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-04 16:52:34,383 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-04 16:52:34,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-04 16:52:34,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-04 16:52:34,651 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-04 16:52:34,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-04 16:52:34,830 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-04 16:52:34,919 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-04 16:52:35,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-04 16:52:35,099 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-04 16:52:35,188 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-04 16:52:35,277 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-04 16:52:35,366 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-04 16:52:35,455 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-04 16:52:35,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-04 16:52:35,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-04 16:52:35,723 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-04 16:52:35,815 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-04 16:52:35,905 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-04 16:52:35,994 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-04 16:52:36,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-04 16:52:36,173 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-04 16:52:36,263 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-04 16:52:36,354 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-04 16:52:36,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-04 16:52:36,532 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-04 16:52:36,620 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-04 16:52:36,710 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-04 16:52:36,799 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-04 16:52:36,888 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-04 16:52:36,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-04 16:52:37,067 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-04 16:52:37,157 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-04 16:52:37,246 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-04 16:52:37,334 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-04 16:52:37,421 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-04 16:52:37,510 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-04 16:52:37,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-04 16:52:37,688 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-04 16:52:37,776 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-04 16:52:37,865 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-04 16:52:37,957 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-04 16:52:38,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-04 16:52:38,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-04 16:52:38,222 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-04 16:52:38,311 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-04 16:52:38,400 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-04 16:52:38,491 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-04 16:52:38,577 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-04 16:52:38,666 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-04 16:52:38,755 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-04 16:52:38,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-04 16:52:38,933 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-04 16:52:39,022 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-04 16:52:39,111 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-04 16:52:39,200 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-04 16:52:39,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-04 16:52:39,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-04 16:52:39,466 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-04 16:52:39,555 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-04 16:52:39,644 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-04 16:52:39,734 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-04 16:52:39,823 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-04 16:52:39,913 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-04 16:52:40,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-04 16:52:40,093 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-04 16:52:40,182 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-04 16:52:40,271 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-04 16:52:40,360 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-04 16:52:40,448 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-04 16:52:40,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-04 16:52:40,625 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-04 16:52:40,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-04 16:52:40,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-04 16:52:40,892 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-04 16:52:41,204 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-04 16:52:41,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-04 16:52:41,382 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-04 16:52:41,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-04 16:52:41,560 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-04 16:52:41,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-04 16:52:41,738 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-04 16:52:41,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-04 16:52:41,916 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-04 16:52:42,005 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-04 16:52:42,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-04 16:52:42,183 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-04 16:52:42,272 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-04 16:52:42,361 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-04 16:52:42,451 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-04 16:52:42,543 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-04 16:52:42,632 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-04 16:52:42,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-04 16:52:42,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-04 16:52:42,900 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-04 16:52:42,989 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-04 16:52:43,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-04 16:52:43,173 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-04 16:52:43,267 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-04 16:52:43,361 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-04 16:52:43,449 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-04 16:52:43,541 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-04 16:52:43,632 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-04 16:52:43,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-04 16:52:43,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-04 16:52:43,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-04 16:52:43,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-04 16:52:44,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-04 16:52:44,169 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-04 16:52:44,258 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-04 16:52:44,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-04 16:52:44,438 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-04 16:52:44,528 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-04 16:52:44,617 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-04 16:52:44,705 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-04 16:52:44,794 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-04 16:52:44,882 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-04 16:52:44,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-04 16:52:45,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-04 16:52:45,151 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-04 16:52:45,237 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-04 16:52:45,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-04 16:52:45,414 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-04 16:52:45,504 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-04 16:52:45,596 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-04 16:52:45,685 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-04 16:52:45,777 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-04 16:52:45,869 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-04 16:52:45,963 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-04 16:52:46,056 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-04 16:52:46,151 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-04 16:52:46,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-04 16:52:46,329 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-04 16:52:46,418 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-04 16:52:46,507 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-04 16:52:46,597 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-04 16:52:46,686 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-04 16:52:46,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-04 16:52:46,869 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-04 16:52:46,958 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-04 16:52:47,047 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-04 16:52:47,137 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-04 16:52:47,226 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-04 16:52:47,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-04 16:52:47,405 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-04 16:52:47,498 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-04 16:52:47,588 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-04 16:52:47,678 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-04 16:52:47,767 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-04 16:52:47,856 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-04 16:52:47,945 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-04 16:52:48,034 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-04 16:52:48,121 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-04 16:52:48,210 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-04 16:52:48,301 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-04 16:52:48,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-04 16:52:48,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-04 16:52:48,566 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-04 16:52:48,655 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-04 16:52:48,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-04 16:52:48,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-04 16:52:48,922 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-04 16:52:49,011 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-04 16:52:49,100 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-04 16:52:49,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-04 16:52:49,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-04 16:52:49,366 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-04 16:52:49,455 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-04 16:52:49,544 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-04 16:52:49,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-04 16:52:49,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-04 16:52:49,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-04 16:52:49,899 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-04 16:52:49,989 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-04 16:52:50,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-04 16:52:50,385 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:50,474 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:50,563 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:50,652 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:50,741 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:50,830 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:50,919 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,187 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,276 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,365 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,454 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,543 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,632 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,811 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,900 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:51,989 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,167 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,433 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,524 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,615 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,703 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,792 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,882 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:52,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,149 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,414 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,591 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,680 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,769 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,858 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:53,947 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,036 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,309 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,398 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,487 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,576 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,672 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:54,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:55,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:55,180 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:55,357 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:55,538 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:55,715 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:55,894 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:56,076 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:56,257 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:56,428 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:56,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:56,723 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:56,852 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:56,973 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,493 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,594 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,791 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,894 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:57,993 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,297 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,398 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,698 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,801 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:58,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:59,005 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:59,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:59,209 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:59,308 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:59,411 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:59,510 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:52:59,613 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:549] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
