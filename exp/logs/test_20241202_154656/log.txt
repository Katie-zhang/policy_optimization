2024-12-02 15:46:58,186 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-02 15:46:58,196 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-02 15:46:58,202 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-02 15:46:58,209 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-02 15:46:58,216 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-02 15:46:58,223 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-02 15:46:58,229 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-02 15:46:58,236 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-02 15:46:58,243 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-02 15:46:58,250 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-02 15:46:58,618 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 0 loss: -0.2660 reward: 0.2660 ref_reward: 0.2734 improvement: -2.69%
2024-12-02 15:46:59,042 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 2 loss: -0.2721 reward: 0.2721 ref_reward: 0.2734 improvement: -0.49%
2024-12-02 15:46:59,458 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 4 loss: -0.2777 reward: 0.2777 ref_reward: 0.2734 improvement: 1.58%
2024-12-02 15:46:59,875 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 6 loss: -0.2829 reward: 0.2829 ref_reward: 0.2734 improvement: 3.46%
2024-12-02 15:47:00,290 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 8 loss: -0.2875 reward: 0.2875 ref_reward: 0.2734 improvement: 5.17%
2024-12-02 15:47:00,861 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 10 loss: -0.2916 reward: 0.2916 ref_reward: 0.2734 improvement: 6.66%
2024-12-02 15:47:01,424 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 12 loss: -0.2956 reward: 0.2956 ref_reward: 0.2734 improvement: 8.11%
2024-12-02 15:47:01,982 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 14 loss: -0.2994 reward: 0.2994 ref_reward: 0.2734 improvement: 9.52%
2024-12-02 15:47:02,440 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 16 loss: -0.3033 reward: 0.3033 ref_reward: 0.2734 improvement: 10.93%
2024-12-02 15:47:02,796 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 18 loss: -0.3071 reward: 0.3071 ref_reward: 0.2734 improvement: 12.34%
2024-12-02 15:47:03,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 20 loss: -0.3110 reward: 0.3110 ref_reward: 0.2734 improvement: 13.74%
2024-12-02 15:47:03,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 22 loss: -0.3148 reward: 0.3148 ref_reward: 0.2734 improvement: 15.13%
2024-12-02 15:47:03,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 24 loss: -0.3185 reward: 0.3185 ref_reward: 0.2734 improvement: 16.49%
2024-12-02 15:47:03,988 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 26 loss: -0.3221 reward: 0.3221 ref_reward: 0.2734 improvement: 17.81%
2024-12-02 15:47:04,291 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 28 loss: -0.3256 reward: 0.3256 ref_reward: 0.2734 improvement: 19.08%
2024-12-02 15:47:04,589 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 30 loss: -0.3289 reward: 0.3289 ref_reward: 0.2734 improvement: 20.29%
2024-12-02 15:47:04,884 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 32 loss: -0.3320 reward: 0.3320 ref_reward: 0.2734 improvement: 21.43%
2024-12-02 15:47:05,180 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 34 loss: -0.3349 reward: 0.3349 ref_reward: 0.2734 improvement: 22.49%
2024-12-02 15:47:05,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 36 loss: -0.3376 reward: 0.3376 ref_reward: 0.2734 improvement: 23.48%
2024-12-02 15:47:05,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 38 loss: -0.3402 reward: 0.3402 ref_reward: 0.2734 improvement: 24.42%
2024-12-02 15:47:06,070 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 40 loss: -0.3426 reward: 0.3426 ref_reward: 0.2734 improvement: 25.30%
2024-12-02 15:47:06,365 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 42 loss: -0.3449 reward: 0.3449 ref_reward: 0.2734 improvement: 26.15%
2024-12-02 15:47:06,665 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 44 loss: -0.3472 reward: 0.3472 ref_reward: 0.2734 improvement: 26.98%
2024-12-02 15:47:06,965 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 46 loss: -0.3494 reward: 0.3494 ref_reward: 0.2734 improvement: 27.78%
2024-12-02 15:47:07,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 48 loss: -0.3515 reward: 0.3515 ref_reward: 0.2734 improvement: 28.58%
2024-12-02 15:47:07,557 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 50 loss: -0.3537 reward: 0.3537 ref_reward: 0.2734 improvement: 29.37%
2024-12-02 15:47:07,853 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 52 loss: -0.3559 reward: 0.3559 ref_reward: 0.2734 improvement: 30.17%
2024-12-02 15:47:08,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 54 loss: -0.3580 reward: 0.3580 ref_reward: 0.2734 improvement: 30.96%
2024-12-02 15:47:08,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 56 loss: -0.3602 reward: 0.3602 ref_reward: 0.2734 improvement: 31.75%
2024-12-02 15:47:08,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 58 loss: -0.3623 reward: 0.3623 ref_reward: 0.2734 improvement: 32.52%
2024-12-02 15:47:09,037 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 60 loss: -0.3644 reward: 0.3644 ref_reward: 0.2734 improvement: 33.29%
2024-12-02 15:47:09,334 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 62 loss: -0.3665 reward: 0.3665 ref_reward: 0.2734 improvement: 34.06%
2024-12-02 15:47:09,632 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 64 loss: -0.3685 reward: 0.3685 ref_reward: 0.2734 improvement: 34.79%
2024-12-02 15:47:09,929 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 66 loss: -0.3704 reward: 0.3704 ref_reward: 0.2734 improvement: 35.47%
2024-12-02 15:47:10,225 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 68 loss: -0.3721 reward: 0.3721 ref_reward: 0.2734 improvement: 36.11%
2024-12-02 15:47:10,521 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 70 loss: -0.3737 reward: 0.3737 ref_reward: 0.2734 improvement: 36.69%
2024-12-02 15:47:10,817 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 72 loss: -0.3752 reward: 0.3752 ref_reward: 0.2734 improvement: 37.22%
2024-12-02 15:47:11,113 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 74 loss: -0.3764 reward: 0.3764 ref_reward: 0.2734 improvement: 37.69%
2024-12-02 15:47:11,409 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 76 loss: -0.3776 reward: 0.3776 ref_reward: 0.2734 improvement: 38.11%
2024-12-02 15:47:11,707 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 78 loss: -0.3786 reward: 0.3786 ref_reward: 0.2734 improvement: 38.48%
2024-12-02 15:47:12,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 80 loss: -0.3795 reward: 0.3795 ref_reward: 0.2734 improvement: 38.80%
2024-12-02 15:47:12,299 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 82 loss: -0.3802 reward: 0.3802 ref_reward: 0.2734 improvement: 39.07%
2024-12-02 15:47:12,596 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 84 loss: -0.3809 reward: 0.3809 ref_reward: 0.2734 improvement: 39.31%
2024-12-02 15:47:12,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 86 loss: -0.3814 reward: 0.3814 ref_reward: 0.2734 improvement: 39.52%
2024-12-02 15:47:13,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 88 loss: -0.3819 reward: 0.3819 ref_reward: 0.2734 improvement: 39.70%
2024-12-02 15:47:13,485 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 90 loss: -0.3824 reward: 0.3824 ref_reward: 0.2734 improvement: 39.85%
2024-12-02 15:47:13,781 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 92 loss: -0.3827 reward: 0.3827 ref_reward: 0.2734 improvement: 39.98%
2024-12-02 15:47:14,082 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 94 loss: -0.3830 reward: 0.3830 ref_reward: 0.2734 improvement: 40.10%
2024-12-02 15:47:14,380 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 96 loss: -0.3833 reward: 0.3833 ref_reward: 0.2734 improvement: 40.19%
2024-12-02 15:47:14,675 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 98 loss: -0.3835 reward: 0.3835 ref_reward: 0.2734 improvement: 40.28%
2024-12-02 15:47:15,547 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 0.6721 grad norm: 0.3572 policy: 0.3656 0.3414
2024-12-02 15:47:16,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 0.6410 grad norm: 0.2508 policy: 0.3814 0.3720
2024-12-02 15:47:17,030 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.6217 grad norm: 0.1861 policy: 0.3938 0.3969
2024-12-02 15:47:17,772 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.6095 grad norm: 0.1324 policy: 0.4091 0.4125
2024-12-02 15:47:18,515 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.6027 grad norm: 0.0851 policy: 0.4294 0.4167
2024-12-02 15:47:19,255 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.5995 grad norm: 0.0410 policy: 0.4551 0.4094
2024-12-02 15:47:19,999 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.5990 grad norm: 0.0201 policy: 0.4802 0.3960
2024-12-02 15:47:20,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.5995 grad norm: 0.0406 policy: 0.4963 0.3850
2024-12-02 15:47:21,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.5997 grad norm: 0.0460 policy: 0.4984 0.3825
2024-12-02 15:47:22,227 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.5993 grad norm: 0.0331 policy: 0.4903 0.3871
2024-12-02 15:47:22,969 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.5989 grad norm: 0.0144 policy: 0.4794 0.3934
2024-12-02 15:47:23,711 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0051 policy: 0.4716 0.3971
2024-12-02 15:47:24,457 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.5989 grad norm: 0.0101 policy: 0.4689 0.3974
2024-12-02 15:47:25,201 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.5989 grad norm: 0.0114 policy: 0.4699 0.3957
2024-12-02 15:47:25,940 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0092 policy: 0.4723 0.3942
2024-12-02 15:47:26,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0047 policy: 0.4743 0.3938
2024-12-02 15:47:27,417 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0005 policy: 0.4755 0.3941
2024-12-02 15:47:28,152 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0032 policy: 0.4760 0.3943
2024-12-02 15:47:28,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0035 policy: 0.4761 0.3942
2024-12-02 15:47:29,634 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0022 policy: 0.4759 0.3939
2024-12-02 15:47:30,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 1.1600 grad norm: 0.9854 policy: 0.3507 0.2728
2024-12-02 15:47:31,349 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 1.0746 grad norm: 0.9267 policy: 0.3979 0.2776
2024-12-02 15:47:32,093 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.9970 grad norm: 0.9026 policy: 0.4413 0.2825
2024-12-02 15:47:32,842 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.9240 grad norm: 0.8778 policy: 0.4789 0.2901
2024-12-02 15:47:33,594 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.8560 grad norm: 0.8635 policy: 0.5135 0.2973
2024-12-02 15:47:34,341 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.7907 grad norm: 0.8267 policy: 0.5486 0.3014
2024-12-02 15:47:35,085 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.7315 grad norm: 0.7525 policy: 0.5838 0.3008
2024-12-02 15:47:35,830 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.6810 grad norm: 0.6427 policy: 0.6213 0.2921
2024-12-02 15:47:36,572 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.6425 grad norm: 0.5013 policy: 0.6629 0.2727
2024-12-02 15:47:37,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.6169 grad norm: 0.3407 policy: 0.7074 0.2441
2024-12-02 15:47:38,059 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.6032 grad norm: 0.1764 policy: 0.7493 0.2128
2024-12-02 15:47:38,804 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.5989 grad norm: 0.0308 policy: 0.7821 0.1868
2024-12-02 15:47:39,544 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.5995 grad norm: 0.0717 policy: 0.8017 0.1710
2024-12-02 15:47:40,287 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.6007 grad norm: 0.1189 policy: 0.8084 0.1659
2024-12-02 15:47:41,032 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.6007 grad norm: 0.1198 policy: 0.8057 0.1686
2024-12-02 15:47:41,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.6001 grad norm: 0.0953 policy: 0.7985 0.1750
2024-12-02 15:47:42,519 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5994 grad norm: 0.0631 policy: 0.7909 0.1811
2024-12-02 15:47:43,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5990 grad norm: 0.0322 policy: 0.7855 0.1850
2024-12-02 15:47:44,004 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0077 policy: 0.7824 0.1868
2024-12-02 15:47:44,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0133 policy: 0.7811 0.1873
2024-12-02 15:47:45,856 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 9.1424 grad norm: 1.0745 policy: 0.3354 0.2933
2024-12-02 15:47:46,601 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 9.0402 grad norm: 1.0707 policy: 0.3925 0.2795
2024-12-02 15:47:47,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 8.9396 grad norm: 1.1287 policy: 0.4532 0.2604
2024-12-02 15:47:48,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 8.8383 grad norm: 1.2052 policy: 0.5168 0.2348
2024-12-02 15:47:48,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 8.7289 grad norm: 1.3449 policy: 0.5879 0.2025
2024-12-02 15:47:49,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 8.6025 grad norm: 1.5119 policy: 0.6661 0.1656
2024-12-02 15:47:50,308 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 8.4572 grad norm: 1.6959 policy: 0.7461 0.1265
2024-12-02 15:47:51,051 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 8.2890 grad norm: 1.9090 policy: 0.8219 0.0889
2024-12-02 15:47:51,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 8.0928 grad norm: 2.1420 policy: 0.8866 0.0567
2024-12-02 15:47:52,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 7.8651 grad norm: 2.3953 policy: 0.9351 0.0324
2024-12-02 15:47:53,279 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 7.6024 grad norm: 2.6686 policy: 0.9669 0.0166
2024-12-02 15:47:54,022 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 7.3011 grad norm: 2.9621 policy: 0.9849 0.0075
2024-12-02 15:47:54,764 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 6.9576 grad norm: 3.2760 policy: 0.9939 0.0030
2024-12-02 15:47:55,506 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 6.5685 grad norm: 3.6106 policy: 0.9979 0.0011
2024-12-02 15:47:56,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 6.1301 grad norm: 3.9662 policy: 0.9993 0.0003
2024-12-02 15:47:56,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 5.6391 grad norm: 4.3428 policy: 0.9998 0.0001
2024-12-02 15:47:57,734 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 5.0919 grad norm: 4.7407 policy: 1.0000 0.0000
2024-12-02 15:47:58,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 4.4851 grad norm: 5.1599 policy: 1.0000 0.0000
2024-12-02 15:47:59,222 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 3.8149 grad norm: 5.6001 policy: 1.0000 0.0000
2024-12-02 15:47:59,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 3.0780 grad norm: 6.0549 policy: 1.0000 0.0000
2024-12-02 15:48:00,928 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 0.6949 grad norm: 0.4393 policy: 0.4446 0.3129
2024-12-02 15:48:01,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 0.6528 grad norm: 0.3191 policy: 0.3701 0.3724
2024-12-02 15:48:02,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.6255 grad norm: 0.2310 policy: 0.3080 0.4254
2024-12-02 15:48:03,158 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.6095 grad norm: 0.1474 policy: 0.2584 0.4668
2024-12-02 15:48:03,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.6020 grad norm: 0.0775 policy: 0.2212 0.4943
2024-12-02 15:48:04,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.5996 grad norm: 0.0427 policy: 0.1965 0.5042
2024-12-02 15:48:05,384 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.5992 grad norm: 0.0410 policy: 0.1834 0.4988
2024-12-02 15:48:06,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.5994 grad norm: 0.0425 policy: 0.1795 0.4867
2024-12-02 15:48:06,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.5995 grad norm: 0.0398 policy: 0.1815 0.4775
2024-12-02 15:48:07,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.5993 grad norm: 0.0312 policy: 0.1861 0.4756
2024-12-02 15:48:08,355 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.5990 grad norm: 0.0173 policy: 0.1907 0.4791
2024-12-02 15:48:09,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0031 policy: 0.1942 0.4841
2024-12-02 15:48:09,838 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0064 policy: 0.1964 0.4871
2024-12-02 15:48:10,576 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.5989 grad norm: 0.0095 policy: 0.1974 0.4872
2024-12-02 15:48:11,317 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0083 policy: 0.1975 0.4854
2024-12-02 15:48:12,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0060 policy: 0.1966 0.4837
2024-12-02 15:48:12,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0036 policy: 0.1954 0.4831
2024-12-02 15:48:13,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0015 policy: 0.1942 0.4837
2024-12-02 15:48:14,282 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0024 policy: 0.1935 0.4844
2024-12-02 15:48:15,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0028 policy: 0.1935 0.4847
2024-12-02 15:48:16,621 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 0.0506 grad norm: 0.7167 
2024-12-02 15:48:17,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0013 grad norm: 0.0974 
2024-12-02 15:48:18,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0030 grad norm: 0.1323 
2024-12-02 15:48:18,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0001 grad norm: 0.0254 
2024-12-02 15:48:19,620 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0008 grad norm: 0.0656 
2024-12-02 15:48:20,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0000 grad norm: 0.0093 
2024-12-02 15:48:21,113 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0003 grad norm: 0.0364 
2024-12-02 15:48:21,860 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0093 
2024-12-02 15:48:22,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0060 
2024-12-02 15:48:23,354 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0057 
2024-12-02 15:48:24,101 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0038 
2024-12-02 15:48:24,847 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0027 
2024-12-02 15:48:25,591 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0026 
2024-12-02 15:48:26,340 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0015 
2024-12-02 15:48:27,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:48:27,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0009 
2024-12-02 15:48:28,590 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0003 
2024-12-02 15:48:29,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:48:30,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0000 
2024-12-02 15:48:30,834 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0004 
2024-12-02 15:48:31,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 1.6218 grad norm: 3.7993 
2024-12-02 15:48:32,554 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0304 grad norm: 0.6328 
2024-12-02 15:48:33,304 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0054 grad norm: 0.1828 
2024-12-02 15:48:34,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0094 grad norm: 0.2225 
2024-12-02 15:48:34,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0018 grad norm: 0.1002 
2024-12-02 15:48:35,552 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0007 grad norm: 0.0698 
2024-12-02 15:48:36,302 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0017 grad norm: 0.1105 
2024-12-02 15:48:37,055 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0002 grad norm: 0.0367 
2024-12-02 15:48:37,807 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0001 grad norm: 0.0253 
2024-12-02 15:48:38,560 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0127 
2024-12-02 15:48:39,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0201 
2024-12-02 15:48:40,064 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0021 
2024-12-02 15:48:40,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0125 
2024-12-02 15:48:41,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0021 
2024-12-02 15:48:42,321 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0011 
2024-12-02 15:48:43,072 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0010 
2024-12-02 15:48:43,822 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0009 
2024-12-02 15:48:44,573 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:48:45,323 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0005 
2024-12-02 15:48:46,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0006 
2024-12-02 15:48:47,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 669.6451 grad norm: 60.3926 
2024-12-02 15:48:47,784 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.6153 grad norm: 6.1350 
2024-12-02 15:48:48,533 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.2249 grad norm: 3.4697 
2024-12-02 15:48:49,282 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0217 grad norm: 0.9170 
2024-12-02 15:48:50,029 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0826 grad norm: 1.6992 
2024-12-02 15:48:50,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0116 grad norm: 0.6816 
2024-12-02 15:48:51,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0139 grad norm: 0.7591 
2024-12-02 15:48:52,274 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0066 grad norm: 0.5091 
2024-12-02 15:48:53,029 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0002 grad norm: 0.0794 
2024-12-02 15:48:53,781 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0022 grad norm: 0.2964 
2024-12-02 15:48:54,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0004 grad norm: 0.1136 
2024-12-02 15:48:55,286 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0005 grad norm: 0.1400 
2024-12-02 15:48:56,036 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0003 grad norm: 0.1128 
2024-12-02 15:48:56,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0001 grad norm: 0.0457 
2024-12-02 15:48:57,546 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0117 
2024-12-02 15:48:58,298 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0241 
2024-12-02 15:48:59,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0149 
2024-12-02 15:48:59,800 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0113 
2024-12-02 15:49:00,552 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0088 
2024-12-02 15:49:01,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0085 
2024-12-02 15:49:02,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 1.1722 grad norm: 3.1270 
2024-12-02 15:49:03,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0205 grad norm: 0.5914 
2024-12-02 15:49:03,773 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0138 grad norm: 0.3351 
2024-12-02 15:49:04,525 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0074 grad norm: 0.2494 
2024-12-02 15:49:05,279 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0016 grad norm: 0.1378 
2024-12-02 15:49:06,027 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0026 grad norm: 0.1781 
2024-12-02 15:49:06,778 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0006 grad norm: 0.0774 
2024-12-02 15:49:07,530 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0005 grad norm: 0.0619 
2024-12-02 15:49:08,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0001 grad norm: 0.0351 
2024-12-02 15:49:09,030 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0001 grad norm: 0.0309 
2024-12-02 15:49:09,778 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0251 
2024-12-02 15:49:10,530 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0168 
2024-12-02 15:49:11,280 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0154 
2024-12-02 15:49:12,026 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0128 
2024-12-02 15:49:12,776 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0024 
2024-12-02 15:49:13,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0077 
2024-12-02 15:49:14,274 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0010 
2024-12-02 15:49:15,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0048 
2024-12-02 15:49:15,771 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0011 
2024-12-02 15:49:16,523 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0023 
2024-12-02 15:49:17,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-02 15:49:18,046 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-02 15:49:18,109 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-02 15:49:18,174 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-02 15:49:18,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-02 15:49:18,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-02 15:49:18,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-02 15:49:18,432 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-02 15:49:18,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-02 15:49:18,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-02 15:49:18,625 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-02 15:49:18,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-02 15:49:18,754 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-02 15:49:18,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-02 15:49:18,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-02 15:49:18,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-02 15:49:19,013 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-02 15:49:19,077 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-02 15:49:19,142 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-02 15:49:19,206 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-02 15:49:19,271 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-02 15:49:19,335 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-02 15:49:19,400 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-02 15:49:19,464 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-02 15:49:19,529 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-02 15:49:19,593 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-02 15:49:19,658 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-02 15:49:19,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-02 15:49:19,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-02 15:49:19,851 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-02 15:49:19,916 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-02 15:49:19,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-02 15:49:20,048 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-02 15:49:20,112 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-02 15:49:20,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-02 15:49:20,242 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-02 15:49:20,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-02 15:49:20,371 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-02 15:49:20,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-02 15:49:20,500 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-02 15:49:20,565 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-02 15:49:20,629 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-02 15:49:20,694 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-02 15:49:20,758 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-02 15:49:20,823 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-02 15:49:20,887 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-02 15:49:20,951 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-02 15:49:21,016 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-02 15:49:21,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-02 15:49:21,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-02 15:49:21,209 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-02 15:49:21,276 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-02 15:49:21,341 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-02 15:49:21,405 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-02 15:49:21,470 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-02 15:49:21,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-02 15:49:21,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-02 15:49:21,663 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-02 15:49:21,728 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-02 15:49:21,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-02 15:49:21,856 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-02 15:49:21,919 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-02 15:49:21,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-02 15:49:22,047 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-02 15:49:22,112 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-02 15:49:22,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-02 15:49:22,241 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-02 15:49:22,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-02 15:49:22,370 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-02 15:49:22,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-02 15:49:22,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-02 15:49:22,566 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-02 15:49:22,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-02 15:49:22,695 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-02 15:49:22,759 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-02 15:49:22,824 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-02 15:49:22,888 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-02 15:49:22,953 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-02 15:49:23,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-02 15:49:23,082 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-02 15:49:23,147 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-02 15:49:23,211 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-02 15:49:23,276 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-02 15:49:23,340 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-02 15:49:23,404 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-02 15:49:23,469 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-02 15:49:23,533 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-02 15:49:23,598 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-02 15:49:23,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-02 15:49:23,727 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-02 15:49:23,791 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-02 15:49:23,856 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-02 15:49:23,921 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-02 15:49:23,985 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-02 15:49:24,050 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-02 15:49:24,115 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-02 15:49:24,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-02 15:49:24,244 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-02 15:49:24,308 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-02 15:49:24,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-02 15:49:24,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-02 15:49:24,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-02 15:49:24,789 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-02 15:49:24,853 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-02 15:49:24,918 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-02 15:49:24,982 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-02 15:49:25,047 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-02 15:49:25,111 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-02 15:49:25,176 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-02 15:49:25,240 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-02 15:49:25,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-02 15:49:25,370 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-02 15:49:25,432 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-02 15:49:25,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-02 15:49:25,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-02 15:49:25,625 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-02 15:49:25,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-02 15:49:25,754 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-02 15:49:25,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-02 15:49:25,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-02 15:49:25,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-02 15:49:26,012 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-02 15:49:26,077 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-02 15:49:26,142 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-02 15:49:26,205 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-02 15:49:26,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-02 15:49:26,333 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-02 15:49:26,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-02 15:49:26,462 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-02 15:49:26,527 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-02 15:49:26,591 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-02 15:49:26,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-02 15:49:26,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-02 15:49:26,786 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-02 15:49:26,851 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-02 15:49:26,916 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-02 15:49:26,980 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-02 15:49:27,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-02 15:49:27,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-02 15:49:27,174 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-02 15:49:27,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-02 15:49:27,304 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-02 15:49:27,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-02 15:49:27,433 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-02 15:49:27,498 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-02 15:49:27,562 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-02 15:49:27,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-02 15:49:27,691 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-02 15:49:27,756 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-02 15:49:27,820 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-02 15:49:27,885 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-02 15:49:27,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-02 15:49:28,013 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-02 15:49:28,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-02 15:49:28,143 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-02 15:49:28,207 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-02 15:49:28,273 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-02 15:49:28,335 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-02 15:49:28,398 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-02 15:49:28,463 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-02 15:49:28,527 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-02 15:49:28,591 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-02 15:49:28,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-02 15:49:28,720 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-02 15:49:28,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-02 15:49:28,850 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-02 15:49:28,914 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-02 15:49:28,979 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-02 15:49:29,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-02 15:49:29,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-02 15:49:29,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-02 15:49:29,237 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-02 15:49:29,302 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-02 15:49:29,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-02 15:49:29,432 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-02 15:49:29,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-02 15:49:29,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-02 15:49:29,625 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-02 15:49:29,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-02 15:49:29,754 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-02 15:49:29,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-02 15:49:29,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-02 15:49:29,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-02 15:49:30,012 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-02 15:49:30,077 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-02 15:49:30,141 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-02 15:49:30,206 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-02 15:49:30,270 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-02 15:49:30,335 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-02 15:49:30,399 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-02 15:49:30,464 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-02 15:49:30,528 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-02 15:49:30,592 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-02 15:49:30,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-02 15:49:30,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-02 15:49:30,786 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-02 15:49:30,851 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-02 15:49:30,915 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-02 15:49:30,980 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-02 15:49:31,044 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-02 15:49:31,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,456 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,584 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,713 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,777 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,842 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:31,971 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,100 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,162 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,226 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,290 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,355 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,419 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,612 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,741 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,805 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,934 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:32,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,063 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,191 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,258 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,323 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,451 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,645 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,709 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,838 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,903 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:33,967 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,095 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,160 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,417 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,482 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,546 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,674 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,804 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,930 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:34,994 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,122 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,186 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,507 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,572 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,636 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,764 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,829 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:35,957 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,085 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,214 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,471 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,535 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,663 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,727 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,792 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,856 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,920 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:36,985 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,114 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,178 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,242 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,371 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,499 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,564 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,628 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-02 15:49:37,992 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.1000, 0.3000, 0.6000], device='cuda:0'), new_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0')
2024-12-02 15:49:38,056 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0'), new_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0')
2024-12-02 15:49:38,121 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0'), new_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0')
2024-12-02 15:49:38,185 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0'), new_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0')
2024-12-02 15:49:38,250 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0'), new_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0')
2024-12-02 15:49:38,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0'), new_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0')
2024-12-02 15:49:38,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0'), new_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0')
2024-12-02 15:49:38,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0'), new_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0')
2024-12-02 15:49:38,508 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0'), new_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0')
2024-12-02 15:49:38,572 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0'), new_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0')
2024-12-02 15:49:38,637 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0'), new_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0')
2024-12-02 15:49:38,701 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0'), new_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0')
2024-12-02 15:49:38,766 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0'), new_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0')
2024-12-02 15:49:38,830 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0'), new_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0')
2024-12-02 15:49:38,895 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0'), new_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0')
2024-12-02 15:49:38,959 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0'), new_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0')
2024-12-02 15:49:39,024 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0'), new_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0')
2024-12-02 15:49:39,088 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0'), new_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0')
2024-12-02 15:49:39,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0'), new_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0')
2024-12-02 15:49:39,217 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0'), new_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0')
2024-12-02 15:49:39,282 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0'), new_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0')
2024-12-02 15:49:39,344 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0'), new_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0')
2024-12-02 15:49:39,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0'), new_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0')
2024-12-02 15:49:39,471 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0'), new_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0')
2024-12-02 15:49:39,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0'), new_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0')
2024-12-02 15:49:39,600 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0'), new_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0')
2024-12-02 15:49:39,665 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0'), new_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0')
2024-12-02 15:49:39,729 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0'), new_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0')
2024-12-02 15:49:39,794 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0'), new_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0')
2024-12-02 15:49:39,858 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0'), new_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0')
2024-12-02 15:49:39,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0'), new_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0')
2024-12-02 15:49:39,987 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0'), new_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0')
2024-12-02 15:49:40,052 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0'), new_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0')
2024-12-02 15:49:40,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0'), new_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0')
2024-12-02 15:49:40,181 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0'), new_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0')
2024-12-02 15:49:40,246 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0'), new_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0')
2024-12-02 15:49:40,310 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0'), new_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0')
2024-12-02 15:49:40,375 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0'), new_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0')
2024-12-02 15:49:40,439 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0'), new_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0')
2024-12-02 15:49:40,504 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0'), new_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0')
2024-12-02 15:49:40,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0'), new_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0')
2024-12-02 15:49:40,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0'), new_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0')
2024-12-02 15:49:40,697 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0'), new_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0')
2024-12-02 15:49:40,762 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0'), new_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0')
2024-12-02 15:49:40,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0'), new_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0')
2024-12-02 15:49:40,891 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0'), new_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0')
2024-12-02 15:49:40,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0'), new_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0')
2024-12-02 15:49:41,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0'), new_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0')
2024-12-02 15:49:41,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0'), new_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0')
2024-12-02 15:49:41,149 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0'), new_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0')
2024-12-02 15:49:41,214 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0'), new_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0')
2024-12-02 15:49:41,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0'), new_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0')
2024-12-02 15:49:41,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0'), new_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0')
2024-12-02 15:49:41,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0'), new_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0')
2024-12-02 15:49:41,471 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0'), new_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0')
2024-12-02 15:49:41,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0'), new_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0')
2024-12-02 15:49:41,600 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0'), new_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0')
2024-12-02 15:49:41,665 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0'), new_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0')
2024-12-02 15:49:41,729 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0'), new_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0')
2024-12-02 15:49:41,794 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0'), new_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0')
2024-12-02 15:49:41,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0'), new_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0')
2024-12-02 15:49:41,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0'), new_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0')
2024-12-02 15:49:41,988 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0'), new_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0')
2024-12-02 15:49:42,052 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0'), new_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0')
2024-12-02 15:49:42,117 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0'), new_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0')
2024-12-02 15:49:42,181 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0'), new_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0')
2024-12-02 15:49:42,246 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0'), new_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0')
2024-12-02 15:49:42,310 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0'), new_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0')
2024-12-02 15:49:42,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0'), new_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0')
2024-12-02 15:49:42,436 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0'), new_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0')
2024-12-02 15:49:42,500 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0'), new_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0')
2024-12-02 15:49:42,569 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0'), new_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0')
2024-12-02 15:49:42,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0'), new_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0')
2024-12-02 15:49:42,698 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0'), new_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0')
2024-12-02 15:49:42,762 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0'), new_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0')
2024-12-02 15:49:42,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0'), new_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0')
2024-12-02 15:49:42,891 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0'), new_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0')
2024-12-02 15:49:42,956 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0'), new_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0')
2024-12-02 15:49:43,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0'), new_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0')
2024-12-02 15:49:43,085 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0'), new_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0')
2024-12-02 15:49:43,149 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0'), new_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0')
2024-12-02 15:49:43,214 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0'), new_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0')
2024-12-02 15:49:43,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0'), new_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0')
2024-12-02 15:49:43,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0'), new_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0')
2024-12-02 15:49:43,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0'), new_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0')
2024-12-02 15:49:43,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0'), new_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0')
2024-12-02 15:49:43,537 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0'), new_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0')
2024-12-02 15:49:43,601 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0'), new_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0')
2024-12-02 15:49:43,666 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0'), new_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0')
2024-12-02 15:49:43,730 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0'), new_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0')
2024-12-02 15:49:43,795 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0'), new_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0')
2024-12-02 15:49:43,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0'), new_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0')
2024-12-02 15:49:43,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0'), new_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0')
2024-12-02 15:49:43,988 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0'), new_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0')
2024-12-02 15:49:44,053 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0'), new_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0')
2024-12-02 15:49:44,117 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0'), new_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0')
2024-12-02 15:49:44,181 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0'), new_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0')
2024-12-02 15:49:44,246 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0'), new_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0')
2024-12-02 15:49:44,310 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0'), new_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0')
2024-12-02 15:49:44,375 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0'), new_distribution = tensor([0.1051, 0.3544, 0.5405], device='cuda:0')
