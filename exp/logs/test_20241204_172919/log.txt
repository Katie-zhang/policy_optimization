2024-12-04 17:29:21,253 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-04 17:29:21,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-04 17:29:21,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-04 17:29:21,274 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-04 17:29:21,280 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-04 17:29:21,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-04 17:29:21,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-04 17:29:21,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-04 17:29:21,308 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-04 17:29:21,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-04 17:29:21,771 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: -0.2341 reward: 0.2341 ref_reward: 0.2734 improvement: -14.38%
2024-12-04 17:29:22,366 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: -0.2496 reward: 0.2496 ref_reward: 0.2734 improvement: -8.72%
2024-12-04 17:29:22,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: -0.2610 reward: 0.2610 ref_reward: 0.2734 improvement: -4.52%
2024-12-04 17:29:23,255 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: -0.2694 reward: 0.2694 ref_reward: 0.2734 improvement: -1.45%
2024-12-04 17:29:23,573 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: -0.2753 reward: 0.2753 ref_reward: 0.2734 improvement: 0.69%
2024-12-04 17:29:23,892 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: -0.2792 reward: 0.2792 ref_reward: 0.2734 improvement: 2.11%
2024-12-04 17:29:24,204 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: -0.2815 reward: 0.2815 ref_reward: 0.2734 improvement: 2.94%
2024-12-04 17:29:24,518 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: -0.2826 reward: 0.2826 ref_reward: 0.2734 improvement: 3.37%
2024-12-04 17:29:24,837 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 17:29:25,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:25,476 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.64%
2024-12-04 17:29:25,805 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.57%
2024-12-04 17:29:26,140 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: -0.2829 reward: 0.2829 ref_reward: 0.2734 improvement: 3.49%
2024-12-04 17:29:26,473 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: -0.2828 reward: 0.2828 ref_reward: 0.2734 improvement: 3.43%
2024-12-04 17:29:26,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: -0.2827 reward: 0.2827 ref_reward: 0.2734 improvement: 3.40%
2024-12-04 17:29:27,113 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: -0.2827 reward: 0.2827 ref_reward: 0.2734 improvement: 3.41%
2024-12-04 17:29:27,410 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: -0.2828 reward: 0.2828 ref_reward: 0.2734 improvement: 3.45%
2024-12-04 17:29:27,711 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: -0.2830 reward: 0.2830 ref_reward: 0.2734 improvement: 3.50%
2024-12-04 17:29:28,028 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: -0.2831 reward: 0.2831 ref_reward: 0.2734 improvement: 3.55%
2024-12-04 17:29:28,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.60%
2024-12-04 17:29:28,674 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.63%
2024-12-04 17:29:29,005 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:29:29,334 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:29,644 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:29,951 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:30,267 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:30,576 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:29:30,872 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:29:31,183 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:29:31,500 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:29:31,800 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:29:32,114 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:32,420 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:32,743 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:33,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:33,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:33,720 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:34,053 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:34,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:34,704 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:35,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:35,354 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:35,674 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:36,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:29:36,298 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:36,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:36,860 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:37,143 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:37,427 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:37,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:29:38,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: 0.1268 reward: -0.1268 ref_reward: 0.3541 improvement: -135.80%
2024-12-04 17:29:38,542 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: 0.0827 reward: -0.0827 ref_reward: 0.3541 improvement: -123.35%
2024-12-04 17:29:38,826 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: 0.0411 reward: -0.0411 ref_reward: 0.3541 improvement: -111.61%
2024-12-04 17:29:39,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: 0.0018 reward: -0.0018 ref_reward: 0.3541 improvement: -100.50%
2024-12-04 17:29:39,391 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: -0.0340 reward: 0.0340 ref_reward: 0.3541 improvement: -90.40%
2024-12-04 17:29:39,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: -0.0686 reward: 0.0686 ref_reward: 0.3541 improvement: -80.61%
2024-12-04 17:29:39,961 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: -0.1021 reward: 0.1021 ref_reward: 0.3541 improvement: -71.17%
2024-12-04 17:29:40,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: -0.1350 reward: 0.1350 ref_reward: 0.3541 improvement: -61.88%
2024-12-04 17:29:40,557 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: -0.1663 reward: 0.1663 ref_reward: 0.3541 improvement: -53.03%
2024-12-04 17:29:40,841 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: -0.1958 reward: 0.1958 ref_reward: 0.3541 improvement: -44.71%
2024-12-04 17:29:41,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: -0.2230 reward: 0.2230 ref_reward: 0.3541 improvement: -37.01%
2024-12-04 17:29:41,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: -0.2485 reward: 0.2485 ref_reward: 0.3541 improvement: -29.83%
2024-12-04 17:29:41,689 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: -0.2716 reward: 0.2716 ref_reward: 0.3541 improvement: -23.30%
2024-12-04 17:29:41,973 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: -0.2918 reward: 0.2918 ref_reward: 0.3541 improvement: -17.59%
2024-12-04 17:29:42,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: -0.3088 reward: 0.3088 ref_reward: 0.3541 improvement: -12.78%
2024-12-04 17:29:42,544 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: -0.3223 reward: 0.3223 ref_reward: 0.3541 improvement: -8.96%
2024-12-04 17:29:42,830 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: -0.3325 reward: 0.3325 ref_reward: 0.3541 improvement: -6.10%
2024-12-04 17:29:43,115 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: -0.3396 reward: 0.3396 ref_reward: 0.3541 improvement: -4.08%
2024-12-04 17:29:43,399 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: -0.3443 reward: 0.3443 ref_reward: 0.3541 improvement: -2.77%
2024-12-04 17:29:43,683 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: -0.3470 reward: 0.3470 ref_reward: 0.3541 improvement: -1.99%
2024-12-04 17:29:43,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: -0.3485 reward: 0.3485 ref_reward: 0.3541 improvement: -1.56%
2024-12-04 17:29:44,253 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: -0.3494 reward: 0.3494 ref_reward: 0.3541 improvement: -1.33%
2024-12-04 17:29:44,538 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: -0.3499 reward: 0.3499 ref_reward: 0.3541 improvement: -1.18%
2024-12-04 17:29:44,828 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: -0.3505 reward: 0.3505 ref_reward: 0.3541 improvement: -1.02%
2024-12-04 17:29:45,113 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: -0.3512 reward: 0.3512 ref_reward: 0.3541 improvement: -0.81%
2024-12-04 17:29:45,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: -0.3521 reward: 0.3521 ref_reward: 0.3541 improvement: -0.56%
2024-12-04 17:29:45,685 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: -0.3531 reward: 0.3531 ref_reward: 0.3541 improvement: -0.27%
2024-12-04 17:29:45,969 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: -0.3542 reward: 0.3542 ref_reward: 0.3541 improvement: 0.02%
2024-12-04 17:29:46,252 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: -0.3551 reward: 0.3551 ref_reward: 0.3541 improvement: 0.30%
2024-12-04 17:29:46,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: -0.3559 reward: 0.3559 ref_reward: 0.3541 improvement: 0.53%
2024-12-04 17:29:46,820 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: -0.3565 reward: 0.3565 ref_reward: 0.3541 improvement: 0.70%
2024-12-04 17:29:47,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.81%
2024-12-04 17:29:47,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:47,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 17:29:47,961 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 17:29:48,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 17:29:48,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.80%
2024-12-04 17:29:48,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.79%
2024-12-04 17:29:49,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.79%
2024-12-04 17:29:49,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.80%
2024-12-04 17:29:49,672 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.81%
2024-12-04 17:29:49,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 17:29:50,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 17:29:50,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:50,806 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:51,090 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:51,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:51,658 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:51,942 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:52,225 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:29:52,733 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: 13.6645 reward: -13.6645 ref_reward: 0.3861 improvement: -3639.45%
2024-12-04 17:29:53,015 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: 13.2004 reward: -13.2004 ref_reward: 0.3861 improvement: -3519.26%
2024-12-04 17:29:53,298 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: 12.7277 reward: -12.7277 ref_reward: 0.3861 improvement: -3396.80%
2024-12-04 17:29:53,581 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: 12.2490 reward: -12.2490 ref_reward: 0.3861 improvement: -3272.81%
2024-12-04 17:29:53,865 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: 11.7699 reward: -11.7699 ref_reward: 0.3861 improvement: -3148.72%
2024-12-04 17:29:54,146 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: 11.2694 reward: -11.2694 ref_reward: 0.3861 improvement: -3019.08%
2024-12-04 17:29:54,427 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: 10.7453 reward: -10.7453 ref_reward: 0.3861 improvement: -2883.32%
2024-12-04 17:29:54,709 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: 10.1963 reward: -10.1963 ref_reward: 0.3861 improvement: -2741.11%
2024-12-04 17:29:54,992 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: 9.6509 reward: -9.6509 ref_reward: 0.3861 improvement: -2599.83%
2024-12-04 17:29:55,277 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: 9.0863 reward: -9.0863 ref_reward: 0.3861 improvement: -2453.58%
2024-12-04 17:29:55,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: 8.5033 reward: -8.5033 ref_reward: 0.3861 improvement: -2302.57%
2024-12-04 17:29:55,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: 7.9101 reward: -7.9101 ref_reward: 0.3861 improvement: -2148.92%
2024-12-04 17:29:56,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: 7.2980 reward: -7.2980 ref_reward: 0.3861 improvement: -1990.38%
2024-12-04 17:29:56,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: 6.6728 reward: -6.6728 ref_reward: 0.3861 improvement: -1828.44%
2024-12-04 17:29:56,689 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: 6.0416 reward: -6.0416 ref_reward: 0.3861 improvement: -1664.92%
2024-12-04 17:29:56,971 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: 5.4133 reward: -5.4133 ref_reward: 0.3861 improvement: -1502.19%
2024-12-04 17:29:57,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: 4.7976 reward: -4.7976 ref_reward: 0.3861 improvement: -1342.71%
2024-12-04 17:29:57,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: 4.2043 reward: -4.2043 ref_reward: 0.3861 improvement: -1189.02%
2024-12-04 17:29:57,822 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: 3.6425 reward: -3.6425 ref_reward: 0.3861 improvement: -1043.49%
2024-12-04 17:29:58,104 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: 3.1201 reward: -3.1201 ref_reward: 0.3861 improvement: -908.19%
2024-12-04 17:29:58,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: 2.6462 reward: -2.6462 ref_reward: 0.3861 improvement: -785.43%
2024-12-04 17:29:58,668 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: 2.2230 reward: -2.2230 ref_reward: 0.3861 improvement: -675.81%
2024-12-04 17:29:58,954 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: 1.8485 reward: -1.8485 ref_reward: 0.3861 improvement: -578.81%
2024-12-04 17:29:59,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: 1.5221 reward: -1.5221 ref_reward: 0.3861 improvement: -494.25%
2024-12-04 17:29:59,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: 1.2412 reward: -1.2412 ref_reward: 0.3861 improvement: -421.50%
2024-12-04 17:29:59,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: 1.0022 reward: -1.0022 ref_reward: 0.3861 improvement: -359.59%
2024-12-04 17:30:00,085 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: 0.8006 reward: -0.8006 ref_reward: 0.3861 improvement: -307.36%
2024-12-04 17:30:00,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: 0.6315 reward: -0.6315 ref_reward: 0.3861 improvement: -263.58%
2024-12-04 17:30:00,658 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: 0.4905 reward: -0.4905 ref_reward: 0.3861 improvement: -227.04%
2024-12-04 17:30:00,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: 0.3730 reward: -0.3730 ref_reward: 0.3861 improvement: -196.61%
2024-12-04 17:30:01,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: 0.2752 reward: -0.2752 ref_reward: 0.3861 improvement: -171.27%
2024-12-04 17:30:01,506 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: 0.1936 reward: -0.1936 ref_reward: 0.3861 improvement: -150.16%
2024-12-04 17:30:01,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: 0.1256 reward: -0.1256 ref_reward: 0.3861 improvement: -132.53%
2024-12-04 17:30:02,072 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: 0.0685 reward: -0.0685 ref_reward: 0.3861 improvement: -117.75%
2024-12-04 17:30:02,357 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: 0.0205 reward: -0.0205 ref_reward: 0.3861 improvement: -105.32%
2024-12-04 17:30:02,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.0200 reward: 0.0200 ref_reward: 0.3861 improvement: -94.81%
2024-12-04 17:30:02,931 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.0545 reward: 0.0545 ref_reward: 0.3861 improvement: -85.88%
2024-12-04 17:30:03,212 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.0840 reward: 0.0840 ref_reward: 0.3861 improvement: -78.24%
2024-12-04 17:30:03,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.1093 reward: 0.1093 ref_reward: 0.3861 improvement: -71.69%
2024-12-04 17:30:03,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.1312 reward: 0.1312 ref_reward: 0.3861 improvement: -66.02%
2024-12-04 17:30:04,062 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.1502 reward: 0.1502 ref_reward: 0.3861 improvement: -61.09%
2024-12-04 17:30:04,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.1669 reward: 0.1669 ref_reward: 0.3861 improvement: -56.78%
2024-12-04 17:30:04,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.1815 reward: 0.1815 ref_reward: 0.3861 improvement: -52.99%
2024-12-04 17:30:04,912 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.1944 reward: 0.1944 ref_reward: 0.3861 improvement: -49.64%
2024-12-04 17:30:05,195 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.2060 reward: 0.2060 ref_reward: 0.3861 improvement: -46.65%
2024-12-04 17:30:05,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.2162 reward: 0.2162 ref_reward: 0.3861 improvement: -43.99%
2024-12-04 17:30:05,760 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.2255 reward: 0.2255 ref_reward: 0.3861 improvement: -41.59%
2024-12-04 17:30:06,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.2338 reward: 0.2338 ref_reward: 0.3861 improvement: -39.43%
2024-12-04 17:30:06,323 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.2414 reward: 0.2414 ref_reward: 0.3861 improvement: -37.47%
2024-12-04 17:30:06,606 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.2483 reward: 0.2483 ref_reward: 0.3861 improvement: -35.68%
2024-12-04 17:30:07,254 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: 0.0431 reward: -0.0431 ref_reward: 0.1811 improvement: -123.78%
2024-12-04 17:30:07,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: 0.0035 reward: -0.0035 ref_reward: 0.1811 improvement: -101.92%
2024-12-04 17:30:07,825 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: -0.0301 reward: 0.0301 ref_reward: 0.1811 improvement: -83.37%
2024-12-04 17:30:08,109 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: -0.0574 reward: 0.0574 ref_reward: 0.1811 improvement: -68.31%
2024-12-04 17:30:08,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: -0.0813 reward: 0.0813 ref_reward: 0.1811 improvement: -55.10%
2024-12-04 17:30:08,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: -0.1022 reward: 0.1022 ref_reward: 0.1811 improvement: -43.55%
2024-12-04 17:30:08,959 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: -0.1195 reward: 0.1195 ref_reward: 0.1811 improvement: -34.00%
2024-12-04 17:30:09,242 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: -0.1343 reward: 0.1343 ref_reward: 0.1811 improvement: -25.87%
2024-12-04 17:30:09,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: -0.1468 reward: 0.1468 ref_reward: 0.1811 improvement: -18.97%
2024-12-04 17:30:09,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: -0.1572 reward: 0.1572 ref_reward: 0.1811 improvement: -13.24%
2024-12-04 17:30:10,093 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: -0.1658 reward: 0.1658 ref_reward: 0.1811 improvement: -8.45%
2024-12-04 17:30:10,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: -0.1730 reward: 0.1730 ref_reward: 0.1811 improvement: -4.50%
2024-12-04 17:30:10,661 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: -0.1787 reward: 0.1787 ref_reward: 0.1811 improvement: -1.33%
2024-12-04 17:30:10,944 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: -0.1832 reward: 0.1832 ref_reward: 0.1811 improvement: 1.16%
2024-12-04 17:30:11,228 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: -0.1867 reward: 0.1867 ref_reward: 0.1811 improvement: 3.09%
2024-12-04 17:30:11,511 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: -0.1893 reward: 0.1893 ref_reward: 0.1811 improvement: 4.49%
2024-12-04 17:30:11,794 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: -0.1909 reward: 0.1909 ref_reward: 0.1811 improvement: 5.40%
2024-12-04 17:30:12,077 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: -0.1919 reward: 0.1919 ref_reward: 0.1811 improvement: 5.94%
2024-12-04 17:30:12,361 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.17%
2024-12-04 17:30:12,645 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.19%
2024-12-04 17:30:12,934 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.09%
2024-12-04 17:30:13,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: -0.1919 reward: 0.1919 ref_reward: 0.1811 improvement: 5.96%
2024-12-04 17:30:13,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: -0.1917 reward: 0.1917 ref_reward: 0.1811 improvement: 5.84%
2024-12-04 17:30:13,786 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: -0.1916 reward: 0.1916 ref_reward: 0.1811 improvement: 5.78%
2024-12-04 17:30:14,070 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: -0.1916 reward: 0.1916 ref_reward: 0.1811 improvement: 5.76%
2024-12-04 17:30:14,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: -0.1916 reward: 0.1916 ref_reward: 0.1811 improvement: 5.78%
2024-12-04 17:30:14,635 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: -0.1917 reward: 0.1917 ref_reward: 0.1811 improvement: 5.81%
2024-12-04 17:30:14,920 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: -0.1917 reward: 0.1917 ref_reward: 0.1811 improvement: 5.85%
2024-12-04 17:30:15,203 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: -0.1918 reward: 0.1918 ref_reward: 0.1811 improvement: 5.90%
2024-12-04 17:30:15,488 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: -0.1919 reward: 0.1919 ref_reward: 0.1811 improvement: 5.94%
2024-12-04 17:30:15,771 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: -0.1920 reward: 0.1920 ref_reward: 0.1811 improvement: 5.99%
2024-12-04 17:30:16,056 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.04%
2024-12-04 17:30:16,339 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.09%
2024-12-04 17:30:16,622 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.13%
2024-12-04 17:30:16,906 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.17%
2024-12-04 17:30:17,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 17:30:17,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:17,763 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:30:18,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:30:18,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:18,619 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:18,905 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:19,188 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:19,472 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:19,755 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:20,038 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:20,321 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:20,604 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:20,886 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:30:21,169 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:30:22,351 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 0.6683 grad norm: 0.3798 policy: 0.3567 0.3566
2024-12-04 17:30:23,055 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 0.6352 grad norm: 0.2844 policy: 0.3943 0.3711
2024-12-04 17:30:23,762 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 0.6137 grad norm: 0.1939 policy: 0.4267 0.3831
2024-12-04 17:30:24,471 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 0.6018 grad norm: 0.0949 policy: 0.4569 0.3909
2024-12-04 17:30:25,182 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 0.5988 grad norm: 0.0077 policy: 0.4812 0.3946
2024-12-04 17:30:25,885 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 0.6005 grad norm: 0.0766 policy: 0.4930 0.3964
2024-12-04 17:30:26,592 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 0.6009 grad norm: 0.0864 policy: 0.4922 0.3971
2024-12-04 17:30:27,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 0.5997 grad norm: 0.0553 policy: 0.4843 0.3968
2024-12-04 17:30:28,025 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 0.5989 grad norm: 0.0145 policy: 0.4756 0.3954
2024-12-04 17:30:28,731 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 0.5989 grad norm: 0.0152 policy: 0.4704 0.3936
2024-12-04 17:30:29,439 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 0.5990 grad norm: 0.0265 policy: 0.4694 0.3924
2024-12-04 17:30:30,154 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 0.5990 grad norm: 0.0223 policy: 0.4711 0.3926
2024-12-04 17:30:30,862 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0098 policy: 0.4737 0.3936
2024-12-04 17:30:31,573 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0028 policy: 0.4758 0.3947
2024-12-04 17:30:32,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0092 policy: 0.4768 0.3950
2024-12-04 17:30:32,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0083 policy: 0.4765 0.3947
2024-12-04 17:30:33,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0033 policy: 0.4756 0.3943
2024-12-04 17:30:34,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0015 policy: 0.4746 0.3941
2024-12-04 17:30:35,117 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0035 policy: 0.4742 0.3941
2024-12-04 17:30:35,823 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0028 policy: 0.4745 0.3941
2024-12-04 17:30:36,755 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 1.1453 grad norm: 0.9900 policy: 0.3010 0.3671
2024-12-04 17:30:37,461 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 1.0488 grad norm: 0.9257 policy: 0.3373 0.3914
2024-12-04 17:30:38,167 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 0.9668 grad norm: 0.9167 policy: 0.3754 0.4009
2024-12-04 17:30:38,872 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 0.8893 grad norm: 0.9311 policy: 0.4266 0.3889
2024-12-04 17:30:39,577 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 0.8121 grad norm: 0.9178 policy: 0.4920 0.3591
2024-12-04 17:30:40,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 0.7391 grad norm: 0.8467 policy: 0.5678 0.3162
2024-12-04 17:30:40,987 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 0.6775 grad norm: 0.7063 policy: 0.6448 0.2690
2024-12-04 17:30:41,694 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 0.6328 grad norm: 0.5015 policy: 0.7085 0.2303
2024-12-04 17:30:42,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 0.6076 grad norm: 0.2671 policy: 0.7511 0.2059
2024-12-04 17:30:43,111 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 0.5992 grad norm: 0.0590 policy: 0.7778 0.1904
2024-12-04 17:30:43,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 0.5998 grad norm: 0.0896 policy: 0.7964 0.1776
2024-12-04 17:30:44,830 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 0.6018 grad norm: 0.1621 policy: 0.8078 0.1685
2024-12-04 17:30:46,114 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 0.6023 grad norm: 0.1738 policy: 0.8090 0.1672
2024-12-04 17:30:46,929 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 0.6011 grad norm: 0.1404 policy: 0.8022 0.1726
2024-12-04 17:30:47,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 0.5997 grad norm: 0.0879 policy: 0.7937 0.1790
2024-12-04 17:30:48,342 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 0.5990 grad norm: 0.0358 policy: 0.7872 0.1833
2024-12-04 17:30:49,036 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0056 policy: 0.7825 0.1862
2024-12-04 17:30:49,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 0.5989 grad norm: 0.0276 policy: 0.7790 0.1887
2024-12-04 17:30:50,438 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 0.5990 grad norm: 0.0348 policy: 0.7778 0.1897
2024-12-04 17:30:51,144 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 0.5989 grad norm: 0.0301 policy: 0.7790 0.1889
2024-12-04 17:30:52,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 9.1585 grad norm: 0.9524 policy: 0.3036 0.3787
2024-12-04 17:30:53,106 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 9.0781 grad norm: 0.8994 policy: 0.3460 0.3604
2024-12-04 17:30:53,922 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 9.0098 grad norm: 0.9185 policy: 0.3852 0.3420
2024-12-04 17:30:54,725 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 8.9460 grad norm: 0.9820 policy: 0.4231 0.3261
2024-12-04 17:30:55,616 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 8.8794 grad norm: 1.0981 policy: 0.4644 0.3085
2024-12-04 17:30:56,451 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 8.8017 grad norm: 1.2356 policy: 0.5141 0.2851
2024-12-04 17:30:57,264 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 8.7102 grad norm: 1.3883 policy: 0.5725 0.2557
2024-12-04 17:30:58,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 8.6018 grad norm: 1.5562 policy: 0.6394 0.2200
2024-12-04 17:30:58,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 8.4743 grad norm: 1.7390 policy: 0.7118 0.1793
2024-12-04 17:30:59,748 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 8.3254 grad norm: 1.9366 policy: 0.7845 0.1368
2024-12-04 17:31:00,569 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 8.1531 grad norm: 2.1488 policy: 0.8509 0.0965
2024-12-04 17:31:01,391 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 7.9552 grad norm: 2.3758 policy: 0.9055 0.0623
2024-12-04 17:31:02,229 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 7.7297 grad norm: 2.6175 policy: 0.9453 0.0367
2024-12-04 17:31:03,102 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 7.4745 grad norm: 2.8738 policy: 0.9712 0.0197
2024-12-04 17:31:03,938 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 7.1876 grad norm: 3.1449 policy: 0.9863 0.0096
2024-12-04 17:31:04,796 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 6.8669 grad norm: 3.4308 policy: 0.9940 0.0042
2024-12-04 17:31:05,637 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 6.5104 grad norm: 3.7315 policy: 0.9977 0.0017
2024-12-04 17:31:06,492 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 6.1160 grad norm: 4.0474 policy: 0.9992 0.0006
2024-12-04 17:31:07,313 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 5.6796 grad norm: 4.3785 policy: 0.9997 0.0002
2024-12-04 17:31:08,132 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 5.1983 grad norm: 4.7265 policy: 0.9999 0.0001
2024-12-04 17:31:09,160 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 0.6691 grad norm: 0.4437 policy: 0.3659 0.3156
2024-12-04 17:31:09,951 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 0.6337 grad norm: 0.2859 policy: 0.3096 0.3720
2024-12-04 17:31:10,645 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 0.6129 grad norm: 0.1840 policy: 0.2627 0.4198
2024-12-04 17:31:11,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 0.6020 grad norm: 0.0929 policy: 0.2221 0.4603
2024-12-04 17:31:12,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 0.5988 grad norm: 0.0073 policy: 0.1927 0.4887
2024-12-04 17:31:12,741 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 0.5997 grad norm: 0.0549 policy: 0.1769 0.5047
2024-12-04 17:31:13,444 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 0.6003 grad norm: 0.0733 policy: 0.1736 0.5063
2024-12-04 17:31:14,144 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 0.5997 grad norm: 0.0552 policy: 0.1795 0.4995
2024-12-04 17:31:14,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 0.5990 grad norm: 0.0243 policy: 0.1886 0.4903
2024-12-04 17:31:15,544 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 0.5988 grad norm: 0.0016 policy: 0.1964 0.4827
2024-12-04 17:31:16,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 0.5989 grad norm: 0.0157 policy: 0.2004 0.4785
2024-12-04 17:31:16,946 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 0.5989 grad norm: 0.0187 policy: 0.2007 0.4779
2024-12-04 17:31:17,647 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 0.5989 grad norm: 0.0138 policy: 0.1986 0.4799
2024-12-04 17:31:18,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0054 policy: 0.1957 0.4829
2024-12-04 17:31:19,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0021 policy: 0.1934 0.4854
2024-12-04 17:31:19,745 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0059 policy: 0.1925 0.4864
2024-12-04 17:31:20,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0055 policy: 0.1928 0.4860
2024-12-04 17:31:21,144 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0027 policy: 0.1938 0.4849
2024-12-04 17:31:21,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0002 policy: 0.1947 0.4840
2024-12-04 17:31:22,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0019 policy: 0.1952 0.4836
2024-12-04 17:31:24,068 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 0.0370 grad norm: 0.8322 
2024-12-04 17:31:24,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 0.0035 grad norm: 0.1837 
2024-12-04 17:31:25,484 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.0018 grad norm: 0.1152 
2024-12-04 17:31:26,191 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0028 grad norm: 0.1609 
2024-12-04 17:31:26,899 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0012 grad norm: 0.0901 
2024-12-04 17:31:27,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.0003 grad norm: 0.0445 
2024-12-04 17:31:28,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0007 grad norm: 0.0737 
2024-12-04 17:31:29,018 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0098 
2024-12-04 17:31:29,728 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0047 
2024-12-04 17:31:30,434 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0030 
2024-12-04 17:31:31,137 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0039 
2024-12-04 17:31:32,159 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0023 
2024-12-04 17:31:33,419 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0023 
2024-12-04 17:31:34,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0017 
2024-12-04 17:31:35,896 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0013 
2024-12-04 17:31:36,679 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0004 
2024-12-04 17:31:37,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0005 
2024-12-04 17:31:38,881 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0006 
2024-12-04 17:31:39,897 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0005 
2024-12-04 17:31:40,631 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0004 
2024-12-04 17:31:41,588 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 1.6022 grad norm: 3.6176 
2024-12-04 17:31:42,313 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 0.0359 grad norm: 0.7129 
2024-12-04 17:31:43,040 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.0034 grad norm: 0.1518 
2024-12-04 17:31:43,764 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0094 grad norm: 0.2194 
2024-12-04 17:31:44,494 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0037 grad norm: 0.1441 
2024-12-04 17:31:45,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.0001 grad norm: 0.0225 
2024-12-04 17:31:45,945 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0002 grad norm: 0.0413 
2024-12-04 17:31:46,671 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0141 
2024-12-04 17:31:47,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0153 
2024-12-04 17:31:48,119 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0160 
2024-12-04 17:31:48,846 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0113 
2024-12-04 17:31:49,573 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0132 
2024-12-04 17:31:50,300 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0120 
2024-12-04 17:31:51,022 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0035 
2024-12-04 17:31:51,745 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0021 
2024-12-04 17:31:52,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0019 
2024-12-04 17:31:53,192 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0021 
2024-12-04 17:31:53,914 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0025 
2024-12-04 17:31:54,635 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0019 
2024-12-04 17:31:55,356 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0006 
2024-12-04 17:31:56,323 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 677.1509 grad norm: 54.9174 
2024-12-04 17:31:57,052 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 0.2451 grad norm: 2.3932 
2024-12-04 17:31:57,781 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.2229 grad norm: 2.2980 
2024-12-04 17:31:58,514 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0480 grad norm: 0.9049 
2024-12-04 17:31:59,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0003 grad norm: 0.0669 
2024-12-04 17:31:59,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.0118 grad norm: 0.3922 
2024-12-04 17:32:00,684 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0184 grad norm: 0.4835 
2024-12-04 17:32:01,399 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0001 grad norm: 0.0388 
2024-12-04 17:32:02,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0002 grad norm: 0.0552 
2024-12-04 17:32:02,852 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0001 grad norm: 0.0418 
2024-12-04 17:32:03,571 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0054 
2024-12-04 17:32:04,287 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0131 
2024-12-04 17:32:05,006 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0136 
2024-12-04 17:32:05,734 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0046 
2024-12-04 17:32:06,457 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0062 
2024-12-04 17:32:07,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0040 
2024-12-04 17:32:07,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0008 
2024-12-04 17:32:08,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0018 
2024-12-04 17:32:09,349 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0022 
2024-12-04 17:32:10,070 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0003 
2024-12-04 17:32:11,015 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 1.2934 grad norm: 4.2877 
2024-12-04 17:32:11,733 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 0.0310 grad norm: 0.7569 
2024-12-04 17:32:12,457 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.0226 grad norm: 0.4910 
2024-12-04 17:32:13,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0059 grad norm: 0.2346 
2024-12-04 17:32:13,902 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0013 grad norm: 0.1264 
2024-12-04 17:32:14,623 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.0026 grad norm: 0.1779 
2024-12-04 17:32:15,351 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0002 grad norm: 0.0454 
2024-12-04 17:32:16,072 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0006 grad norm: 0.0716 
2024-12-04 17:32:16,792 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0002 grad norm: 0.0444 
2024-12-04 17:32:17,517 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0001 grad norm: 0.0323 
2024-12-04 17:32:18,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0334 
2024-12-04 17:32:18,961 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0155 
2024-12-04 17:32:19,680 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0213 
2024-12-04 17:32:20,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0125 
2024-12-04 17:32:21,121 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0030 
2024-12-04 17:32:21,842 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0075 
2024-12-04 17:32:22,824 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0014 
2024-12-04 17:32:24,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0047 
2024-12-04 17:32:25,185 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0004 
2024-12-04 17:32:25,906 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0025 
2024-12-04 17:32:27,720 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-04 17:32:27,887 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-04 17:32:28,052 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-04 17:32:28,212 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-04 17:32:28,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-04 17:32:28,541 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-04 17:32:28,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-04 17:32:28,832 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-04 17:32:28,958 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-04 17:32:29,073 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-04 17:32:29,181 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-04 17:32:29,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-04 17:32:29,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-04 17:32:29,462 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-04 17:32:29,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-04 17:32:29,638 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-04 17:32:29,726 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-04 17:32:29,814 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-04 17:32:29,902 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-04 17:32:29,990 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-04 17:32:30,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-04 17:32:30,164 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-04 17:32:30,252 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-04 17:32:30,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-04 17:32:30,426 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-04 17:32:30,514 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-04 17:32:30,602 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-04 17:32:30,689 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-04 17:32:30,777 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-04 17:32:30,864 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-04 17:32:30,951 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-04 17:32:31,039 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-04 17:32:31,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-04 17:32:31,215 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-04 17:32:31,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-04 17:32:31,390 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-04 17:32:31,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-04 17:32:31,565 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-04 17:32:31,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-04 17:32:31,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-04 17:32:31,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-04 17:32:31,915 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-04 17:32:32,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-04 17:32:32,090 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-04 17:32:32,178 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-04 17:32:32,265 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-04 17:32:32,352 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-04 17:32:32,440 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-04 17:32:32,529 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-04 17:32:32,617 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-04 17:32:32,705 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-04 17:32:32,792 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-04 17:32:32,880 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-04 17:32:32,967 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-04 17:32:33,055 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-04 17:32:33,143 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-04 17:32:33,230 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-04 17:32:33,318 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-04 17:32:33,405 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-04 17:32:33,492 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-04 17:32:33,584 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-04 17:32:33,672 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-04 17:32:33,760 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-04 17:32:33,847 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-04 17:32:33,935 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-04 17:32:34,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-04 17:32:34,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-04 17:32:34,198 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-04 17:32:34,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-04 17:32:34,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-04 17:32:34,461 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-04 17:32:34,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-04 17:32:34,636 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-04 17:32:34,723 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-04 17:32:34,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-04 17:32:34,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-04 17:32:35,148 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-04 17:32:35,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-04 17:32:35,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-04 17:32:35,647 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-04 17:32:35,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-04 17:32:35,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-04 17:32:36,143 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-04 17:32:36,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-04 17:32:36,475 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-04 17:32:36,629 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-04 17:32:36,768 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-04 17:32:36,895 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-04 17:32:37,015 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-04 17:32:37,123 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-04 17:32:37,221 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-04 17:32:37,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-04 17:32:37,402 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-04 17:32:37,490 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-04 17:32:37,578 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-04 17:32:37,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-04 17:32:37,756 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-04 17:32:37,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-04 17:32:37,932 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-04 17:32:38,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-04 17:32:38,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-04 17:32:38,427 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-04 17:32:38,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-04 17:32:38,603 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-04 17:32:38,691 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-04 17:32:38,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-04 17:32:38,866 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-04 17:32:38,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-04 17:32:39,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-04 17:32:39,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-04 17:32:39,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-04 17:32:39,307 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-04 17:32:39,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-04 17:32:39,482 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-04 17:32:39,570 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-04 17:32:39,660 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-04 17:32:39,749 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-04 17:32:39,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-04 17:32:39,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-04 17:32:40,011 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-04 17:32:40,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-04 17:32:40,185 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-04 17:32:40,272 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-04 17:32:40,360 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-04 17:32:40,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-04 17:32:40,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-04 17:32:40,621 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-04 17:32:40,709 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-04 17:32:40,797 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-04 17:32:40,885 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-04 17:32:40,973 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-04 17:32:41,062 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-04 17:32:41,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-04 17:32:41,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-04 17:32:41,325 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-04 17:32:41,413 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-04 17:32:41,501 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-04 17:32:41,589 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-04 17:32:41,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-04 17:32:41,765 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-04 17:32:41,853 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-04 17:32:41,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-04 17:32:42,029 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-04 17:32:42,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-04 17:32:42,204 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-04 17:32:42,291 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-04 17:32:42,380 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-04 17:32:42,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-04 17:32:42,557 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-04 17:32:42,644 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-04 17:32:42,732 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-04 17:32:42,820 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-04 17:32:42,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-04 17:32:42,995 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-04 17:32:43,083 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-04 17:32:43,171 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-04 17:32:43,259 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-04 17:32:43,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-04 17:32:43,434 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-04 17:32:43,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-04 17:32:43,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-04 17:32:43,701 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-04 17:32:43,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-04 17:32:43,878 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-04 17:32:43,965 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-04 17:32:44,053 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-04 17:32:44,140 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-04 17:32:44,228 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-04 17:32:44,316 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-04 17:32:44,404 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-04 17:32:44,492 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-04 17:32:44,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-04 17:32:44,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-04 17:32:44,755 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-04 17:32:44,842 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-04 17:32:44,930 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-04 17:32:45,018 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-04 17:32:45,107 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-04 17:32:45,196 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-04 17:32:45,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-04 17:32:45,371 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-04 17:32:45,459 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-04 17:32:45,546 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-04 17:32:45,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-04 17:32:45,720 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-04 17:32:45,807 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-04 17:32:45,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-04 17:32:45,996 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-04 17:32:46,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-04 17:32:46,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-04 17:32:46,263 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-04 17:32:46,351 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-04 17:32:46,438 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-04 17:32:46,525 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-04 17:32:46,613 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-04 17:32:46,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-04 17:32:46,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-04 17:32:46,874 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-04 17:32:46,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-04 17:32:47,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-04 17:32:47,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:47,457 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:47,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:47,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:47,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:47,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:47,898 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:47,986 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,162 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,250 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,426 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,603 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,691 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:48,956 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,044 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,132 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,308 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,396 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,484 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,572 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,660 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,748 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:49,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,012 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,100 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,188 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,320 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,530 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,629 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,731 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,832 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:50,934 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,038 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,126 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,326 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,512 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,613 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,718 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:51,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,011 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,113 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,321 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,430 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,518 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,618 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,727 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:52,920 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,118 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,324 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,432 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,650 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,761 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,874 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:53,979 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,093 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,196 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,430 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,552 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,763 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,881 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:54,993 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,104 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,212 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,321 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,438 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,622 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:55,920 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,025 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,122 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,319 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,505 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,694 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,896 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:56,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:57,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:32:57,412 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([0.1000, 0.3000, 0.6000], device='cuda:0'), new_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0')
2024-12-04 17:32:57,514 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0'), new_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0')
2024-12-04 17:32:57,616 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0'), new_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0')
2024-12-04 17:32:57,726 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0'), new_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0')
2024-12-04 17:32:57,830 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0'), new_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0')
2024-12-04 17:32:57,934 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0'), new_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0')
2024-12-04 17:32:58,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0'), new_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0')
2024-12-04 17:32:58,135 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0'), new_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0')
2024-12-04 17:32:58,232 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0'), new_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0')
2024-12-04 17:32:58,339 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0'), new_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0')
2024-12-04 17:32:58,454 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0'), new_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0')
2024-12-04 17:32:58,554 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0'), new_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0')
2024-12-04 17:32:58,660 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0'), new_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0')
2024-12-04 17:32:58,768 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0'), new_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0')
2024-12-04 17:32:58,874 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0'), new_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0')
2024-12-04 17:32:58,976 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0'), new_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0')
2024-12-04 17:32:59,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0'), new_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0')
2024-12-04 17:32:59,170 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0'), new_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0')
2024-12-04 17:32:59,276 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0'), new_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0')
2024-12-04 17:32:59,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0'), new_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0')
2024-12-04 17:32:59,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0'), new_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0')
2024-12-04 17:32:59,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0'), new_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0')
2024-12-04 17:32:59,691 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0'), new_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0')
2024-12-04 17:32:59,797 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0'), new_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0')
2024-12-04 17:32:59,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0'), new_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0')
2024-12-04 17:33:00,014 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0'), new_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0')
2024-12-04 17:33:00,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0'), new_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0')
2024-12-04 17:33:00,229 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0'), new_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0')
2024-12-04 17:33:00,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0'), new_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0')
2024-12-04 17:33:00,440 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0'), new_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0')
2024-12-04 17:33:00,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0'), new_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0')
2024-12-04 17:33:00,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0'), new_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0')
2024-12-04 17:33:00,771 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0'), new_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0')
2024-12-04 17:33:00,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0'), new_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0')
2024-12-04 17:33:01,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0'), new_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0')
2024-12-04 17:33:01,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0'), new_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0')
2024-12-04 17:33:01,184 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0'), new_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0')
2024-12-04 17:33:01,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0'), new_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0')
2024-12-04 17:33:01,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0'), new_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0')
2024-12-04 17:33:01,473 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0'), new_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0')
2024-12-04 17:33:01,566 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0'), new_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0')
2024-12-04 17:33:01,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0'), new_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0')
2024-12-04 17:33:01,739 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0'), new_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0')
2024-12-04 17:33:01,826 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0'), new_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0')
2024-12-04 17:33:01,915 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0'), new_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0')
2024-12-04 17:33:02,004 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0'), new_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0')
2024-12-04 17:33:02,090 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0'), new_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0')
2024-12-04 17:33:02,176 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0'), new_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0')
2024-12-04 17:33:02,263 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0'), new_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0')
2024-12-04 17:33:02,349 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0'), new_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0')
2024-12-04 17:33:02,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0'), new_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0')
2024-12-04 17:33:02,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0'), new_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0')
2024-12-04 17:33:02,608 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0'), new_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0')
2024-12-04 17:33:02,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0'), new_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0')
2024-12-04 17:33:02,783 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0'), new_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0')
2024-12-04 17:33:02,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0'), new_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0')
2024-12-04 17:33:02,957 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0'), new_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0')
2024-12-04 17:33:03,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0'), new_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0')
2024-12-04 17:33:03,135 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0'), new_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0')
2024-12-04 17:33:03,222 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0'), new_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0')
2024-12-04 17:33:03,309 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0'), new_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0')
2024-12-04 17:33:03,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0'), new_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0')
2024-12-04 17:33:03,485 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0'), new_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0')
2024-12-04 17:33:03,572 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0'), new_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0')
2024-12-04 17:33:03,659 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0'), new_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0')
2024-12-04 17:33:03,746 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0'), new_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0')
2024-12-04 17:33:03,834 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0'), new_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0')
2024-12-04 17:33:03,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0'), new_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0')
2024-12-04 17:33:04,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0'), new_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0')
2024-12-04 17:33:04,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0'), new_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0')
2024-12-04 17:33:04,233 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0'), new_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0')
2024-12-04 17:33:04,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0'), new_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0')
2024-12-04 17:33:04,439 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0'), new_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0')
2024-12-04 17:33:04,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0'), new_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0')
2024-12-04 17:33:04,628 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0'), new_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0')
2024-12-04 17:33:04,731 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0'), new_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0')
2024-12-04 17:33:04,825 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0'), new_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0')
2024-12-04 17:33:04,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0'), new_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0')
2024-12-04 17:33:05,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0'), new_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0')
2024-12-04 17:33:05,119 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0'), new_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0')
2024-12-04 17:33:05,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0'), new_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0')
2024-12-04 17:33:05,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0'), new_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0')
2024-12-04 17:33:05,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0'), new_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0')
2024-12-04 17:33:05,523 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0'), new_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0')
2024-12-04 17:33:05,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0'), new_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0')
2024-12-04 17:33:05,711 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0'), new_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0')
2024-12-04 17:33:05,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0'), new_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0')
2024-12-04 17:33:05,915 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0'), new_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0')
2024-12-04 17:33:06,018 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0'), new_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0')
2024-12-04 17:33:06,105 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0'), new_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0')
2024-12-04 17:33:06,207 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0'), new_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0')
2024-12-04 17:33:06,302 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0'), new_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0')
2024-12-04 17:33:06,403 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0'), new_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0')
2024-12-04 17:33:06,493 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0'), new_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0')
2024-12-04 17:33:06,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0'), new_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0')
2024-12-04 17:33:06,698 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0'), new_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0')
2024-12-04 17:33:06,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0'), new_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0')
2024-12-04 17:33:06,885 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0'), new_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0')
2024-12-04 17:33:06,986 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0'), new_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0')
2024-12-04 17:33:07,081 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0'), new_distribution = tensor([0.1051, 0.3544, 0.5405], device='cuda:0')
