2024-12-04 17:23:48,252 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-04 17:23:48,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-04 17:23:48,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-04 17:23:48,274 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-04 17:23:48,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-04 17:23:48,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-04 17:23:48,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-04 17:23:48,300 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-04 17:23:48,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-04 17:23:48,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-04 17:23:48,761 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: -0.2709 reward: 0.2709 ref_reward: 0.2734 improvement: -0.91%
2024-12-04 17:23:49,301 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: -0.2777 reward: 0.2777 ref_reward: 0.2734 improvement: 1.59%
2024-12-04 17:23:49,834 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: -0.2817 reward: 0.2817 ref_reward: 0.2734 improvement: 3.04%
2024-12-04 17:23:50,289 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.61%
2024-12-04 17:23:50,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.62%
2024-12-04 17:23:50,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: -0.2826 reward: 0.2826 ref_reward: 0.2734 improvement: 3.38%
2024-12-04 17:23:51,217 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: -0.2821 reward: 0.2821 ref_reward: 0.2734 improvement: 3.20%
2024-12-04 17:23:51,506 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: -0.2821 reward: 0.2821 ref_reward: 0.2734 improvement: 3.18%
2024-12-04 17:23:51,795 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: -0.2824 reward: 0.2824 ref_reward: 0.2734 improvement: 3.30%
2024-12-04 17:23:52,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: -0.2829 reward: 0.2829 ref_reward: 0.2734 improvement: 3.46%
2024-12-04 17:23:52,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 17:23:52,668 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:52,960 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:53,249 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.62%
2024-12-04 17:23:53,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 17:23:53,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.58%
2024-12-04 17:23:54,126 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: -0.2832 reward: 0.2832 ref_reward: 0.2734 improvement: 3.59%
2024-12-04 17:23:54,422 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: -0.2833 reward: 0.2833 ref_reward: 0.2734 improvement: 3.62%
2024-12-04 17:23:54,711 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:23:55,001 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:55,290 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:55,577 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:55,868 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:23:56,157 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:23:56,447 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.65%
2024-12-04 17:23:56,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:57,027 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:57,316 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:23:57,606 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:57,895 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:58,187 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:58,481 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:58,769 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:23:59,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:23:59,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:23:59,637 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:23:59,926 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:24:00,215 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:24:00,501 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.66%
2024-12-04 17:24:00,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:01,079 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:01,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:01,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:01,943 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:02,234 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:02,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:02,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:03,102 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:03,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:03,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.2834 reward: 0.2834 ref_reward: 0.2734 improvement: 3.67%
2024-12-04 17:24:04,234 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: -0.0712 reward: 0.0712 ref_reward: 0.3541 improvement: -79.88%
2024-12-04 17:24:04,525 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: -0.1069 reward: 0.1069 ref_reward: 0.3541 improvement: -69.80%
2024-12-04 17:24:04,817 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: -0.1391 reward: 0.1391 ref_reward: 0.3541 improvement: -60.71%
2024-12-04 17:24:05,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: -0.1694 reward: 0.1694 ref_reward: 0.3541 improvement: -52.15%
2024-12-04 17:24:05,400 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: -0.1981 reward: 0.1981 ref_reward: 0.3541 improvement: -44.07%
2024-12-04 17:24:05,689 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: -0.2243 reward: 0.2243 ref_reward: 0.3541 improvement: -36.64%
2024-12-04 17:24:05,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: -0.2483 reward: 0.2483 ref_reward: 0.3541 improvement: -29.87%
2024-12-04 17:24:06,269 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: -0.2701 reward: 0.2701 ref_reward: 0.3541 improvement: -23.71%
2024-12-04 17:24:06,556 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: -0.2894 reward: 0.2894 ref_reward: 0.3541 improvement: -18.27%
2024-12-04 17:24:06,845 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: -0.3060 reward: 0.3060 ref_reward: 0.3541 improvement: -13.59%
2024-12-04 17:24:07,135 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: -0.3196 reward: 0.3196 ref_reward: 0.3541 improvement: -9.74%
2024-12-04 17:24:07,423 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: -0.3304 reward: 0.3304 ref_reward: 0.3541 improvement: -6.69%
2024-12-04 17:24:07,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: -0.3385 reward: 0.3385 ref_reward: 0.3541 improvement: -4.40%
2024-12-04 17:24:08,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: -0.3442 reward: 0.3442 ref_reward: 0.3541 improvement: -2.79%
2024-12-04 17:24:08,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: -0.3479 reward: 0.3479 ref_reward: 0.3541 improvement: -1.75%
2024-12-04 17:24:08,581 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: -0.3502 reward: 0.3502 ref_reward: 0.3541 improvement: -1.10%
2024-12-04 17:24:08,869 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: -0.3516 reward: 0.3516 ref_reward: 0.3541 improvement: -0.71%
2024-12-04 17:24:09,157 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: -0.3525 reward: 0.3525 ref_reward: 0.3541 improvement: -0.44%
2024-12-04 17:24:09,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: -0.3533 reward: 0.3533 ref_reward: 0.3541 improvement: -0.22%
2024-12-04 17:24:09,735 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: -0.3540 reward: 0.3540 ref_reward: 0.3541 improvement: -0.01%
2024-12-04 17:24:10,024 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: -0.3548 reward: 0.3548 ref_reward: 0.3541 improvement: 0.21%
2024-12-04 17:24:10,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: -0.3555 reward: 0.3555 ref_reward: 0.3541 improvement: 0.39%
2024-12-04 17:24:10,604 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: -0.3560 reward: 0.3560 ref_reward: 0.3541 improvement: 0.56%
2024-12-04 17:24:10,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: -0.3565 reward: 0.3565 ref_reward: 0.3541 improvement: 0.69%
2024-12-04 17:24:11,184 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.79%
2024-12-04 17:24:11,474 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 17:24:11,763 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 17:24:12,052 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.82%
2024-12-04 17:24:12,339 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.79%
2024-12-04 17:24:12,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: -0.3568 reward: 0.3568 ref_reward: 0.3541 improvement: 0.77%
2024-12-04 17:24:12,915 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: -0.3567 reward: 0.3567 ref_reward: 0.3541 improvement: 0.75%
2024-12-04 17:24:13,204 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: -0.3567 reward: 0.3567 ref_reward: 0.3541 improvement: 0.75%
2024-12-04 17:24:13,493 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: -0.3568 reward: 0.3568 ref_reward: 0.3541 improvement: 0.77%
2024-12-04 17:24:13,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.79%
2024-12-04 17:24:14,067 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: -0.3569 reward: 0.3569 ref_reward: 0.3541 improvement: 0.81%
2024-12-04 17:24:14,356 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.83%
2024-12-04 17:24:14,645 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.3570 reward: 0.3570 ref_reward: 0.3541 improvement: 0.84%
2024-12-04 17:24:14,934 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 17:24:15,230 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 17:24:15,542 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 17:24:15,852 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.85%
2024-12-04 17:24:16,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:24:16,466 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:24:16,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.86%
2024-12-04 17:24:17,082 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.3571 reward: 0.3571 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 17:24:17,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.3572 reward: 0.3572 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 17:24:17,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.3572 reward: 0.3572 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 17:24:17,990 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.3572 reward: 0.3572 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 17:24:18,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.3572 reward: 0.3572 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 17:24:18,608 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.3572 reward: 0.3572 ref_reward: 0.3541 improvement: 0.87%
2024-12-04 17:24:19,157 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: 14.5695 reward: -14.5695 ref_reward: 0.3861 improvement: -3873.88%
2024-12-04 17:24:19,455 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: 13.8916 reward: -13.8916 ref_reward: 0.3861 improvement: -3698.29%
2024-12-04 17:24:19,751 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: 13.2222 reward: -13.2222 ref_reward: 0.3861 improvement: -3524.90%
2024-12-04 17:24:20,057 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: 12.5396 reward: -12.5396 ref_reward: 0.3861 improvement: -3348.10%
2024-12-04 17:24:20,363 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: 11.8726 reward: -11.8726 ref_reward: 0.3861 improvement: -3175.32%
2024-12-04 17:24:20,670 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: 11.1894 reward: -11.1894 ref_reward: 0.3861 improvement: -2998.35%
2024-12-04 17:24:20,956 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: 10.5035 reward: -10.5035 ref_reward: 0.3861 improvement: -2820.67%
2024-12-04 17:24:21,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: 9.8029 reward: -9.8029 ref_reward: 0.3861 improvement: -2639.20%
2024-12-04 17:24:21,531 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: 9.0868 reward: -9.0868 ref_reward: 0.3861 improvement: -2453.72%
2024-12-04 17:24:21,814 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: 8.3545 reward: -8.3545 ref_reward: 0.3861 improvement: -2264.03%
2024-12-04 17:24:22,105 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: 7.6300 reward: -7.6300 ref_reward: 0.3861 improvement: -2076.37%
2024-12-04 17:24:22,399 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: 6.9141 reward: -6.9141 ref_reward: 0.3861 improvement: -1890.92%
2024-12-04 17:24:22,684 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: 6.1969 reward: -6.1969 ref_reward: 0.3861 improvement: -1705.16%
2024-12-04 17:24:22,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: 5.4898 reward: -5.4898 ref_reward: 0.3861 improvement: -1522.00%
2024-12-04 17:24:23,272 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: 4.8047 reward: -4.8047 ref_reward: 0.3861 improvement: -1344.53%
2024-12-04 17:24:23,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: 4.1539 reward: -4.1539 ref_reward: 0.3861 improvement: -1175.98%
2024-12-04 17:24:23,884 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: 3.5511 reward: -3.5511 ref_reward: 0.3861 improvement: -1019.83%
2024-12-04 17:24:24,188 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: 3.0059 reward: -3.0059 ref_reward: 0.3861 improvement: -878.60%
2024-12-04 17:24:24,492 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: 2.5147 reward: -2.5147 ref_reward: 0.3861 improvement: -751.38%
2024-12-04 17:24:24,782 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: 2.0805 reward: -2.0805 ref_reward: 0.3861 improvement: -638.91%
2024-12-04 17:24:25,071 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: 1.7032 reward: -1.7032 ref_reward: 0.3861 improvement: -541.18%
2024-12-04 17:24:25,359 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: 1.3803 reward: -1.3803 ref_reward: 0.3861 improvement: -457.53%
2024-12-04 17:24:25,648 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: 1.1074 reward: -1.1074 ref_reward: 0.3861 improvement: -386.83%
2024-12-04 17:24:25,937 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: 0.8790 reward: -0.8790 ref_reward: 0.3861 improvement: -327.69%
2024-12-04 17:24:26,232 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: 0.6894 reward: -0.6894 ref_reward: 0.3861 improvement: -278.57%
2024-12-04 17:24:26,521 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: 0.5327 reward: -0.5327 ref_reward: 0.3861 improvement: -237.97%
2024-12-04 17:24:26,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: 0.4035 reward: -0.4035 ref_reward: 0.3861 improvement: -204.51%
2024-12-04 17:24:27,097 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: 0.2970 reward: -0.2970 ref_reward: 0.3861 improvement: -176.94%
2024-12-04 17:24:27,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: 0.2093 reward: -0.2093 ref_reward: 0.3861 improvement: -154.20%
2024-12-04 17:24:27,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: 0.1366 reward: -0.1366 ref_reward: 0.3861 improvement: -135.39%
2024-12-04 17:24:27,964 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: 0.0764 reward: -0.0764 ref_reward: 0.3861 improvement: -119.78%
2024-12-04 17:24:28,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: 0.0261 reward: -0.0261 ref_reward: 0.3861 improvement: -106.75%
2024-12-04 17:24:28,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: -0.0161 reward: 0.0161 ref_reward: 0.3861 improvement: -95.83%
2024-12-04 17:24:28,829 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: -0.0516 reward: 0.0516 ref_reward: 0.3861 improvement: -86.62%
2024-12-04 17:24:29,117 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: -0.0818 reward: 0.0818 ref_reward: 0.3861 improvement: -78.81%
2024-12-04 17:24:29,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.1076 reward: 0.1076 ref_reward: 0.3861 improvement: -72.13%
2024-12-04 17:24:29,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.1297 reward: 0.1297 ref_reward: 0.3861 improvement: -66.39%
2024-12-04 17:24:29,985 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.1489 reward: 0.1489 ref_reward: 0.3861 improvement: -61.43%
2024-12-04 17:24:30,274 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.1656 reward: 0.1656 ref_reward: 0.3861 improvement: -57.11%
2024-12-04 17:24:30,563 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.1802 reward: 0.1802 ref_reward: 0.3861 improvement: -53.33%
2024-12-04 17:24:30,852 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.1931 reward: 0.1931 ref_reward: 0.3861 improvement: -49.99%
2024-12-04 17:24:31,140 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.2045 reward: 0.2045 ref_reward: 0.3861 improvement: -47.03%
2024-12-04 17:24:31,429 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.2147 reward: 0.2147 ref_reward: 0.3861 improvement: -44.40%
2024-12-04 17:24:31,717 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.2238 reward: 0.2238 ref_reward: 0.3861 improvement: -42.03%
2024-12-04 17:24:32,005 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.2320 reward: 0.2320 ref_reward: 0.3861 improvement: -39.90%
2024-12-04 17:24:32,295 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.2395 reward: 0.2395 ref_reward: 0.3861 improvement: -37.97%
2024-12-04 17:24:32,587 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.2463 reward: 0.2463 ref_reward: 0.3861 improvement: -36.21%
2024-12-04 17:24:32,875 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.2525 reward: 0.2525 ref_reward: 0.3861 improvement: -34.60%
2024-12-04 17:24:33,164 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.2582 reward: 0.2582 ref_reward: 0.3861 improvement: -33.12%
2024-12-04 17:24:33,453 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.2634 reward: 0.2634 ref_reward: 0.3861 improvement: -31.76%
2024-12-04 17:24:34,115 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 0 loss: -0.0231 reward: 0.0231 ref_reward: 0.1811 improvement: -87.27%
2024-12-04 17:24:34,406 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 2 loss: -0.0607 reward: 0.0607 ref_reward: 0.1811 improvement: -66.46%
2024-12-04 17:24:34,697 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 4 loss: -0.0902 reward: 0.0902 ref_reward: 0.1811 improvement: -50.23%
2024-12-04 17:24:34,987 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 6 loss: -0.1142 reward: 0.1142 ref_reward: 0.1811 improvement: -36.95%
2024-12-04 17:24:35,277 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 8 loss: -0.1327 reward: 0.1327 ref_reward: 0.1811 improvement: -26.76%
2024-12-04 17:24:35,564 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 10 loss: -0.1478 reward: 0.1478 ref_reward: 0.1811 improvement: -18.42%
2024-12-04 17:24:35,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 12 loss: -0.1598 reward: 0.1598 ref_reward: 0.1811 improvement: -11.76%
2024-12-04 17:24:36,144 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 14 loss: -0.1693 reward: 0.1693 ref_reward: 0.1811 improvement: -6.52%
2024-12-04 17:24:36,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 16 loss: -0.1766 reward: 0.1766 ref_reward: 0.1811 improvement: -2.50%
2024-12-04 17:24:36,726 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 18 loss: -0.1819 reward: 0.1819 ref_reward: 0.1811 improvement: 0.44%
2024-12-04 17:24:37,016 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 20 loss: -0.1855 reward: 0.1855 ref_reward: 0.1811 improvement: 2.41%
2024-12-04 17:24:37,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 22 loss: -0.1880 reward: 0.1880 ref_reward: 0.1811 improvement: 3.79%
2024-12-04 17:24:37,596 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 24 loss: -0.1897 reward: 0.1897 ref_reward: 0.1811 improvement: 4.76%
2024-12-04 17:24:37,886 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 26 loss: -0.1909 reward: 0.1909 ref_reward: 0.1811 improvement: 5.40%
2024-12-04 17:24:38,175 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 28 loss: -0.1916 reward: 0.1916 ref_reward: 0.1811 improvement: 5.81%
2024-12-04 17:24:38,463 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 30 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.05%
2024-12-04 17:24:38,752 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 32 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.16%
2024-12-04 17:24:39,041 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 34 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.16%
2024-12-04 17:24:39,332 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 36 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.09%
2024-12-04 17:24:39,622 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 38 loss: -0.1920 reward: 0.1920 ref_reward: 0.1811 improvement: 5.99%
2024-12-04 17:24:39,919 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 40 loss: -0.1918 reward: 0.1918 ref_reward: 0.1811 improvement: 5.90%
2024-12-04 17:24:40,208 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 42 loss: -0.1917 reward: 0.1917 ref_reward: 0.1811 improvement: 5.84%
2024-12-04 17:24:40,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 44 loss: -0.1917 reward: 0.1917 ref_reward: 0.1811 improvement: 5.82%
2024-12-04 17:24:40,787 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 46 loss: -0.1917 reward: 0.1917 ref_reward: 0.1811 improvement: 5.84%
2024-12-04 17:24:41,076 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 48 loss: -0.1918 reward: 0.1918 ref_reward: 0.1811 improvement: 5.88%
2024-12-04 17:24:41,365 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 50 loss: -0.1919 reward: 0.1919 ref_reward: 0.1811 improvement: 5.94%
2024-12-04 17:24:41,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 52 loss: -0.1920 reward: 0.1920 ref_reward: 0.1811 improvement: 5.99%
2024-12-04 17:24:41,945 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 54 loss: -0.1921 reward: 0.1921 ref_reward: 0.1811 improvement: 6.04%
2024-12-04 17:24:42,235 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 56 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.09%
2024-12-04 17:24:42,524 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 58 loss: -0.1922 reward: 0.1922 ref_reward: 0.1811 improvement: 6.13%
2024-12-04 17:24:42,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 60 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.16%
2024-12-04 17:24:43,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 62 loss: -0.1923 reward: 0.1923 ref_reward: 0.1811 improvement: 6.19%
2024-12-04 17:24:43,393 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 64 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.20%
2024-12-04 17:24:43,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 66 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:43,971 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 68 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:44,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 70 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:44,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 72 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:44,839 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 74 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:45,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 76 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:45,420 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 78 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:45,709 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 80 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:45,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 82 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:46,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 84 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:46,577 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 86 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:46,866 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 88 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.21%
2024-12-04 17:24:47,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 90 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:47,444 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 92 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:47,734 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 94 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:48,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 96 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:48,313 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:219] - INFO: [Policy] Epoch 98 loss: -0.1924 reward: 0.1924 ref_reward: 0.1811 improvement: 6.22%
2024-12-04 17:24:49,532 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 0.7547 grad norm: 0.6695 policy: 0.3048 0.3033
2024-12-04 17:24:50,254 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 0.6920 grad norm: 0.5082 policy: 0.3266 0.3567
2024-12-04 17:24:50,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 0.6488 grad norm: 0.3791 policy: 0.3501 0.3968
2024-12-04 17:24:51,701 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 0.6233 grad norm: 0.2767 policy: 0.3774 0.4159
2024-12-04 17:24:52,426 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 0.6082 grad norm: 0.1809 policy: 0.4098 0.4190
2024-12-04 17:24:53,150 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 0.6008 grad norm: 0.0908 policy: 0.4442 0.4109
2024-12-04 17:24:53,879 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 0.5988 grad norm: 0.0129 policy: 0.4777 0.3951
2024-12-04 17:24:54,601 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 0.5996 grad norm: 0.0591 policy: 0.4999 0.3823
2024-12-04 17:24:55,322 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 0.6002 grad norm: 0.0800 policy: 0.5040 0.3809
2024-12-04 17:24:56,044 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 0.5998 grad norm: 0.0676 policy: 0.4960 0.3864
2024-12-04 17:24:56,765 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 0.5992 grad norm: 0.0408 policy: 0.4848 0.3922
2024-12-04 17:24:57,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0133 policy: 0.4763 0.3948
2024-12-04 17:24:58,206 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0081 policy: 0.4715 0.3952
2024-12-04 17:24:58,926 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 0.5989 grad norm: 0.0181 policy: 0.4695 0.3953
2024-12-04 17:24:59,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 0.5989 grad norm: 0.0181 policy: 0.4694 0.3959
2024-12-04 17:25:00,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0124 policy: 0.4709 0.3960
2024-12-04 17:25:01,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0050 policy: 0.4735 0.3951
2024-12-04 17:25:01,817 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0018 policy: 0.4759 0.3939
2024-12-04 17:25:02,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0051 policy: 0.4771 0.3933
2024-12-04 17:25:03,263 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0051 policy: 0.4768 0.3935
2024-12-04 17:25:04,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 0.9553 grad norm: 0.9238 policy: 0.4457 0.3105
2024-12-04 17:25:04,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 0.8704 grad norm: 0.8496 policy: 0.4933 0.3128
2024-12-04 17:25:05,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 0.7949 grad norm: 0.8299 policy: 0.5353 0.3155
2024-12-04 17:25:06,362 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 0.7268 grad norm: 0.7427 policy: 0.5781 0.3121
2024-12-04 17:25:07,085 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 0.6703 grad norm: 0.6155 policy: 0.6286 0.2926
2024-12-04 17:25:07,804 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 0.6290 grad norm: 0.4355 policy: 0.6892 0.2547
2024-12-04 17:25:08,523 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 0.6057 grad norm: 0.2198 policy: 0.7484 0.2110
2024-12-04 17:25:09,244 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 0.5990 grad norm: 0.0429 policy: 0.7916 0.1776
2024-12-04 17:25:09,965 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 0.6007 grad norm: 0.1240 policy: 0.8111 0.1635
2024-12-04 17:25:10,686 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 0.6025 grad norm: 0.1683 policy: 0.8116 0.1650
2024-12-04 17:25:11,411 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 0.6022 grad norm: 0.1601 policy: 0.8038 0.1724
2024-12-04 17:25:12,132 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 0.6006 grad norm: 0.1170 policy: 0.7962 0.1780
2024-12-04 17:25:12,853 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 0.5993 grad norm: 0.0574 policy: 0.7906 0.1808
2024-12-04 17:25:13,575 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0141 policy: 0.7854 0.1836
2024-12-04 17:25:14,296 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 0.5989 grad norm: 0.0278 policy: 0.7801 0.1875
2024-12-04 17:25:15,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 0.5990 grad norm: 0.0376 policy: 0.7767 0.1906
2024-12-04 17:25:15,735 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 0.5990 grad norm: 0.0350 policy: 0.7768 0.1908
2024-12-04 17:25:16,454 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 0.5989 grad norm: 0.0222 policy: 0.7798 0.1885
2024-12-04 17:25:17,169 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0075 policy: 0.7831 0.1858
2024-12-04 17:25:17,891 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0054 policy: 0.7848 0.1846
2024-12-04 17:25:18,831 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 9.0693 grad norm: 1.0172 policy: 0.3611 0.3238
2024-12-04 17:25:19,556 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 8.9988 grad norm: 0.9503 policy: 0.4028 0.3036
2024-12-04 17:25:20,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 8.9355 grad norm: 0.9744 policy: 0.4429 0.2831
2024-12-04 17:25:21,001 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 8.8749 grad norm: 1.0284 policy: 0.4823 0.2623
2024-12-04 17:25:21,725 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 8.8136 grad norm: 1.1229 policy: 0.5241 0.2382
2024-12-04 17:25:22,445 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 8.7467 grad norm: 1.2514 policy: 0.5697 0.2134
2024-12-04 17:25:23,167 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 8.6684 grad norm: 1.3982 policy: 0.6212 0.1866
2024-12-04 17:25:23,892 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 8.5769 grad norm: 1.5630 policy: 0.6778 0.1578
2024-12-04 17:25:24,614 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 8.4688 grad norm: 1.7443 policy: 0.7383 0.1276
2024-12-04 17:25:25,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 8.3418 grad norm: 1.9427 policy: 0.7992 0.0976
2024-12-04 17:25:26,061 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 8.1934 grad norm: 2.1583 policy: 0.8559 0.0698
2024-12-04 17:25:26,786 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 8.0211 grad norm: 2.3910 policy: 0.9042 0.0463
2024-12-04 17:25:27,508 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 7.8221 grad norm: 2.6411 policy: 0.9415 0.0283
2024-12-04 17:25:28,230 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 7.5941 grad norm: 2.9083 policy: 0.9674 0.0158
2024-12-04 17:25:28,953 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 7.3346 grad norm: 3.1927 policy: 0.9834 0.0080
2024-12-04 17:25:29,678 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 7.0415 grad norm: 3.4941 policy: 0.9924 0.0037
2024-12-04 17:25:30,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 6.7125 grad norm: 3.8121 policy: 0.9968 0.0016
2024-12-04 17:25:31,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 6.3457 grad norm: 4.1467 policy: 0.9988 0.0006
2024-12-04 17:25:31,845 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 5.9390 grad norm: 4.4975 policy: 0.9996 0.0002
2024-12-04 17:25:32,566 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 5.4899 grad norm: 4.8674 policy: 0.9999 0.0001
2024-12-04 17:25:33,498 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 0 loss: 0.6580 grad norm: 0.2947 policy: 0.3824 0.3582
2024-12-04 17:25:34,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 5 loss: 0.6343 grad norm: 0.2378 policy: 0.3301 0.3960
2024-12-04 17:25:34,943 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 10 loss: 0.6163 grad norm: 0.1830 policy: 0.2811 0.4299
2024-12-04 17:25:35,666 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 15 loss: 0.6042 grad norm: 0.1151 policy: 0.2359 0.4612
2024-12-04 17:25:36,378 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 20 loss: 0.5991 grad norm: 0.0270 policy: 0.1981 0.4870
2024-12-04 17:25:37,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 25 loss: 0.5996 grad norm: 0.0575 policy: 0.1749 0.5006
2024-12-04 17:25:37,816 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 30 loss: 0.6007 grad norm: 0.0848 policy: 0.1701 0.4995
2024-12-04 17:25:38,533 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 35 loss: 0.5998 grad norm: 0.0578 policy: 0.1786 0.4914
2024-12-04 17:25:39,249 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 40 loss: 0.5989 grad norm: 0.0166 policy: 0.1911 0.4834
2024-12-04 17:25:39,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 45 loss: 0.5989 grad norm: 0.0143 policy: 0.2004 0.4794
2024-12-04 17:25:40,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 50 loss: 0.5990 grad norm: 0.0244 policy: 0.2036 0.4796
2024-12-04 17:25:41,411 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 55 loss: 0.5990 grad norm: 0.0204 policy: 0.2014 0.4819
2024-12-04 17:25:42,131 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0089 policy: 0.1968 0.4841
2024-12-04 17:25:42,843 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0031 policy: 0.1929 0.4852
2024-12-04 17:25:43,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0087 policy: 0.1913 0.4853
2024-12-04 17:25:44,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0074 policy: 0.1921 0.4849
2024-12-04 17:25:45,006 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0025 policy: 0.1940 0.4845
2024-12-04 17:25:45,728 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0019 policy: 0.1954 0.4841
2024-12-04 17:25:46,452 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0034 policy: 0.1957 0.4838
2024-12-04 17:25:47,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:348] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0023 policy: 0.1952 0.4839
2024-12-04 17:25:48,716 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 0.0053 grad norm: 0.2498 
2024-12-04 17:25:49,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 0.0107 grad norm: 0.3118 
2024-12-04 17:25:50,175 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.0014 grad norm: 0.1198 
2024-12-04 17:25:50,903 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0000 grad norm: 0.0079 
2024-12-04 17:25:51,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0016 grad norm: 0.0963 
2024-12-04 17:25:52,358 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.0005 grad norm: 0.0577 
2024-12-04 17:25:53,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0000 grad norm: 0.0123 
2024-12-04 17:25:53,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0003 grad norm: 0.0426 
2024-12-04 17:25:54,541 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0142 
2024-12-04 17:25:55,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0144 
2024-12-04 17:25:55,995 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0200 
2024-12-04 17:25:56,730 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0080 
2024-12-04 17:25:57,459 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0045 
2024-12-04 17:25:58,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0091 
2024-12-04 17:25:58,917 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0061 
2024-12-04 17:25:59,645 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0009 
2024-12-04 17:26:00,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0026 
2024-12-04 17:26:01,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0033 
2024-12-04 17:26:01,832 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0023 
2024-12-04 17:26:02,559 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0001 
2024-12-04 17:26:03,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 2.1032 grad norm: 5.0533 
2024-12-04 17:26:04,229 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 0.0609 grad norm: 0.9240 
2024-12-04 17:26:04,952 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.0034 grad norm: 0.1855 
2024-12-04 17:26:05,681 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0121 grad norm: 0.3233 
2024-12-04 17:26:06,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0018 grad norm: 0.1165 
2024-12-04 17:26:07,140 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.0012 grad norm: 0.0974 
2024-12-04 17:26:07,868 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0011 grad norm: 0.0974 
2024-12-04 17:26:08,598 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0001 grad norm: 0.0283 
2024-12-04 17:26:09,329 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0047 
2024-12-04 17:26:10,060 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0151 
2024-12-04 17:26:10,789 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0096 
2024-12-04 17:26:11,519 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0040 
2024-12-04 17:26:12,250 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0079 
2024-12-04 17:26:12,980 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0024 
2024-12-04 17:26:13,708 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0012 
2024-12-04 17:26:14,437 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0006 
2024-12-04 17:26:15,159 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0012 
2024-12-04 17:26:15,885 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0006 
2024-12-04 17:26:16,615 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0002 
2024-12-04 17:26:17,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0006 
2024-12-04 17:26:18,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 670.2220 grad norm: 81.3779 
2024-12-04 17:26:19,025 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 1.3024 grad norm: 12.0562 
2024-12-04 17:26:19,755 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.3495 grad norm: 5.0014 
2024-12-04 17:26:20,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0074 grad norm: 0.4917 
2024-12-04 17:26:21,217 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0905 grad norm: 1.5403 
2024-12-04 17:26:21,946 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.1054 grad norm: 1.5539 
2024-12-04 17:26:22,678 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0603 grad norm: 1.1795 
2024-12-04 17:26:23,402 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0053 grad norm: 0.3748 
2024-12-04 17:26:24,128 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0108 grad norm: 0.5386 
2024-12-04 17:26:24,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0072 grad norm: 0.4292 
2024-12-04 17:26:25,583 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0020 grad norm: 0.2117 
2024-12-04 17:26:26,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0002 grad norm: 0.0561 
2024-12-04 17:26:27,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0005 grad norm: 0.1086 
2024-12-04 17:26:27,763 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0001 grad norm: 0.0410 
2024-12-04 17:26:28,491 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0002 grad norm: 0.0591 
2024-12-04 17:26:29,220 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0001 grad norm: 0.0488 
2024-12-04 17:26:29,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0247 
2024-12-04 17:26:30,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0046 
2024-12-04 17:26:31,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0150 
2024-12-04 17:26:32,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0047 
2024-12-04 17:26:33,070 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 0 loss: 1.1229 grad norm: 3.2925 
2024-12-04 17:26:33,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 5 loss: 0.0290 grad norm: 0.6574 
2024-12-04 17:26:34,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 10 loss: 0.0101 grad norm: 0.2541 
2024-12-04 17:26:35,253 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 15 loss: 0.0127 grad norm: 0.2680 
2024-12-04 17:26:35,979 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 20 loss: 0.0011 grad norm: 0.0919 
2024-12-04 17:26:36,705 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 25 loss: 0.0030 grad norm: 0.1731 
2024-12-04 17:26:37,432 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 30 loss: 0.0013 grad norm: 0.1107 
2024-12-04 17:26:38,159 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 35 loss: 0.0010 grad norm: 0.0924 
2024-12-04 17:26:38,879 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 40 loss: 0.0004 grad norm: 0.0579 
2024-12-04 17:26:39,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 45 loss: 0.0002 grad norm: 0.0455 
2024-12-04 17:26:40,535 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 50 loss: 0.0002 grad norm: 0.0414 
2024-12-04 17:26:41,866 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 55 loss: 0.0001 grad norm: 0.0240 
2024-12-04 17:26:42,769 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 60 loss: 0.0001 grad norm: 0.0260 
2024-12-04 17:26:43,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0186 
2024-12-04 17:26:44,221 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0035 
2024-12-04 17:26:44,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0113 
2024-12-04 17:26:45,688 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0010 
2024-12-04 17:26:46,430 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0069 
2024-12-04 17:26:47,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0008 
2024-12-04 17:26:47,912 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:472] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0036 
2024-12-04 17:26:49,354 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-04 17:26:49,446 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-04 17:26:49,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-04 17:26:49,625 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-04 17:26:49,715 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-04 17:26:49,804 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-04 17:26:49,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-04 17:26:49,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-04 17:26:50,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-04 17:26:50,164 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-04 17:26:50,253 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-04 17:26:50,342 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-04 17:26:50,431 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-04 17:26:50,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-04 17:26:50,610 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-04 17:26:50,701 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-04 17:26:50,792 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-04 17:26:50,881 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-04 17:26:50,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-04 17:26:51,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-04 17:26:51,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-04 17:26:51,233 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-04 17:26:51,323 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-04 17:26:51,410 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-04 17:26:51,499 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-04 17:26:51,588 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-04 17:26:51,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-04 17:26:51,767 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-04 17:26:51,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-04 17:26:51,943 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-04 17:26:52,033 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-04 17:26:52,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-04 17:26:52,213 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-04 17:26:52,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-04 17:26:52,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-04 17:26:52,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-04 17:26:52,573 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-04 17:26:52,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-04 17:26:52,752 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-04 17:26:52,841 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-04 17:26:52,931 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-04 17:26:53,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-04 17:26:53,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-04 17:26:53,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-04 17:26:53,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-04 17:26:53,378 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-04 17:26:53,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-04 17:26:53,558 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-04 17:26:53,648 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-04 17:26:53,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-04 17:26:53,828 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-04 17:26:53,916 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-04 17:26:54,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-04 17:26:54,090 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-04 17:26:54,178 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-04 17:26:54,266 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-04 17:26:54,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-04 17:26:54,441 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-04 17:26:54,530 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-04 17:26:54,619 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-04 17:26:54,709 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-04 17:26:54,799 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-04 17:26:54,888 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-04 17:26:54,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-04 17:26:55,067 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-04 17:26:55,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-04 17:26:55,247 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-04 17:26:55,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-04 17:26:55,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-04 17:26:55,515 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-04 17:26:55,604 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-04 17:26:55,693 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-04 17:26:55,782 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-04 17:26:55,872 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-04 17:26:55,961 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-04 17:26:56,051 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-04 17:26:56,142 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-04 17:26:56,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-04 17:26:56,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-04 17:26:56,521 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-04 17:26:56,648 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-04 17:26:56,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-04 17:26:56,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-04 17:26:57,028 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-04 17:26:57,154 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-04 17:26:57,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-04 17:26:57,404 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-04 17:26:57,529 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-04 17:26:57,655 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-04 17:26:57,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-04 17:26:57,906 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-04 17:26:58,032 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-04 17:26:58,157 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-04 17:26:58,279 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-04 17:26:58,389 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-04 17:26:58,493 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-04 17:26:58,590 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-04 17:26:58,681 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-04 17:26:58,770 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-04 17:26:58,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-04 17:26:59,191 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-04 17:26:59,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-04 17:26:59,370 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-04 17:26:59,460 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-04 17:26:59,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-04 17:26:59,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-04 17:26:59,730 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-04 17:26:59,841 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-04 17:27:00,008 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-04 17:27:00,173 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-04 17:27:00,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-04 17:27:00,509 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-04 17:27:00,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-04 17:27:00,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-04 17:27:01,010 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-04 17:27:01,178 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-04 17:27:01,344 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-04 17:27:01,515 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-04 17:27:01,681 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-04 17:27:01,848 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-04 17:27:02,006 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-04 17:27:02,157 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-04 17:27:02,292 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-04 17:27:02,415 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-04 17:27:02,528 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-04 17:27:02,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-04 17:27:02,728 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-04 17:27:02,822 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-04 17:27:02,912 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-04 17:27:03,001 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-04 17:27:03,090 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-04 17:27:03,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-04 17:27:03,269 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-04 17:27:03,358 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-04 17:27:03,448 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-04 17:27:03,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-04 17:27:03,628 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-04 17:27:03,717 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-04 17:27:03,807 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-04 17:27:03,896 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-04 17:27:03,985 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-04 17:27:04,075 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-04 17:27:04,165 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-04 17:27:04,254 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-04 17:27:04,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-04 17:27:04,433 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-04 17:27:04,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-04 17:27:04,611 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-04 17:27:04,701 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-04 17:27:04,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-04 17:27:04,881 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-04 17:27:04,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-04 17:27:05,065 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-04 17:27:05,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-04 17:27:05,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-04 17:27:05,337 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-04 17:27:05,426 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-04 17:27:05,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-04 17:27:05,606 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-04 17:27:05,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-04 17:27:05,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-04 17:27:05,885 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-04 17:27:05,980 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-04 17:27:06,101 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-04 17:27:06,229 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-04 17:27:06,358 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-04 17:27:06,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-04 17:27:06,616 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-04 17:27:06,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-04 17:27:06,875 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-04 17:27:07,004 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-04 17:27:07,132 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-04 17:27:07,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-04 17:27:07,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-04 17:27:07,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-04 17:27:07,646 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-04 17:27:07,774 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-04 17:27:07,905 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-04 17:27:08,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-04 17:27:08,157 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-04 17:27:08,275 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-04 17:27:08,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-04 17:27:08,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-04 17:27:08,576 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-04 17:27:08,686 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-04 17:27:08,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-04 17:27:08,876 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-04 17:27:08,966 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-04 17:27:09,057 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-04 17:27:09,152 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-04 17:27:09,250 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-04 17:27:09,351 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-04 17:27:09,450 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-04 17:27:09,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-04 17:27:09,647 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-04 17:27:09,747 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-04 17:27:09,847 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-04 17:27:09,946 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-04 17:27:10,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-04 17:27:10,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-04 17:27:10,507 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:10,605 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:10,703 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:10,802 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:10,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:10,999 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,300 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,398 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,602 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,799 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,897 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:11,995 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,191 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,290 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,487 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,586 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,686 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,884 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:12,983 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,378 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,478 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,576 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,674 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:13,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:14,092 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:14,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:14,419 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:14,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:14,746 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:14,909 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:15,072 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:15,236 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:15,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:15,549 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:15,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:15,823 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:15,944 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,055 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,159 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,255 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,346 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,697 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,862 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:16,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:17,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:17,249 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:17,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:17,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:17,628 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:17,753 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:17,878 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,128 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,264 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,519 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:18,909 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,040 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,171 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,301 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,432 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,563 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,694 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,825 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:19,957 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,091 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,222 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,343 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,452 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,553 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,651 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,849 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:20,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,046 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,243 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,340 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,440 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,637 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,835 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:21,933 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:22,032 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 17:27:22,400 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 0: ref_distribution = tensor([0.1000, 0.3000, 0.6000], device='cuda:0'), new_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0')
2024-12-04 17:27:22,500 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 1: ref_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0'), new_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0')
2024-12-04 17:27:22,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 2: ref_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0'), new_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0')
2024-12-04 17:27:22,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 3: ref_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0'), new_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0')
2024-12-04 17:27:22,799 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 4: ref_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0'), new_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0')
2024-12-04 17:27:22,898 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 5: ref_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0'), new_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0')
2024-12-04 17:27:22,997 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 6: ref_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0'), new_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0')
2024-12-04 17:27:23,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 7: ref_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0'), new_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0')
2024-12-04 17:27:23,193 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 8: ref_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0'), new_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0')
2024-12-04 17:27:23,292 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 9: ref_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0'), new_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0')
2024-12-04 17:27:23,390 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 10: ref_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0'), new_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0')
2024-12-04 17:27:23,489 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 11: ref_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0'), new_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0')
2024-12-04 17:27:23,588 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 12: ref_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0'), new_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0')
2024-12-04 17:27:23,687 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 13: ref_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0'), new_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0')
2024-12-04 17:27:23,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 14: ref_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0'), new_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0')
2024-12-04 17:27:23,882 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 15: ref_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0'), new_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0')
2024-12-04 17:27:23,980 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 16: ref_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0'), new_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0')
2024-12-04 17:27:24,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 17: ref_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0'), new_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0')
2024-12-04 17:27:24,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 18: ref_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0'), new_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0')
2024-12-04 17:27:24,302 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 19: ref_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0'), new_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0')
2024-12-04 17:27:24,470 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 20: ref_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0'), new_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0')
2024-12-04 17:27:24,647 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 21: ref_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0'), new_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0')
2024-12-04 17:27:24,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 22: ref_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0'), new_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0')
2024-12-04 17:27:24,977 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 23: ref_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0'), new_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0')
2024-12-04 17:27:25,141 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 24: ref_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0'), new_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0')
2024-12-04 17:27:25,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 25: ref_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0'), new_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0')
2024-12-04 17:27:25,466 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 26: ref_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0'), new_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0')
2024-12-04 17:27:25,629 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 27: ref_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0'), new_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0')
2024-12-04 17:27:25,794 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 28: ref_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0'), new_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0')
2024-12-04 17:27:25,959 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 29: ref_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0'), new_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0')
2024-12-04 17:27:26,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 30: ref_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0'), new_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0')
2024-12-04 17:27:26,217 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 31: ref_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0'), new_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0')
2024-12-04 17:27:26,344 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 32: ref_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0'), new_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0')
2024-12-04 17:27:26,471 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 33: ref_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0'), new_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0')
2024-12-04 17:27:26,596 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 34: ref_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0'), new_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0')
2024-12-04 17:27:26,720 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 35: ref_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0'), new_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0')
2024-12-04 17:27:26,846 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 36: ref_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0'), new_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0')
2024-12-04 17:27:26,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 37: ref_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0'), new_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0')
2024-12-04 17:27:27,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 38: ref_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0'), new_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0')
2024-12-04 17:27:27,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 39: ref_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0'), new_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0')
2024-12-04 17:27:27,348 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 40: ref_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0'), new_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0')
2024-12-04 17:27:27,474 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 41: ref_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0'), new_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0')
2024-12-04 17:27:27,600 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 42: ref_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0'), new_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0')
2024-12-04 17:27:27,727 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 43: ref_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0'), new_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0')
2024-12-04 17:27:27,853 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 44: ref_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0'), new_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0')
2024-12-04 17:27:27,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 45: ref_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0'), new_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0')
2024-12-04 17:27:28,099 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 46: ref_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0'), new_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0')
2024-12-04 17:27:28,211 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 47: ref_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0'), new_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0')
2024-12-04 17:27:28,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 48: ref_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0'), new_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0')
2024-12-04 17:27:28,412 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 49: ref_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0'), new_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0')
2024-12-04 17:27:28,505 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 50: ref_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0'), new_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0')
2024-12-04 17:27:28,597 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 51: ref_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0'), new_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0')
2024-12-04 17:27:28,686 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 52: ref_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0'), new_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0')
2024-12-04 17:27:28,776 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 53: ref_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0'), new_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0')
2024-12-04 17:27:28,865 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 54: ref_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0'), new_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0')
2024-12-04 17:27:28,952 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 55: ref_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0'), new_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0')
2024-12-04 17:27:29,040 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 56: ref_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0'), new_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0')
2024-12-04 17:27:29,128 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 57: ref_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0'), new_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0')
2024-12-04 17:27:29,216 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 58: ref_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0'), new_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0')
2024-12-04 17:27:29,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 59: ref_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0'), new_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0')
2024-12-04 17:27:29,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 60: ref_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0'), new_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0')
2024-12-04 17:27:29,481 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 61: ref_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0'), new_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0')
2024-12-04 17:27:29,570 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 62: ref_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0'), new_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0')
2024-12-04 17:27:29,659 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 63: ref_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0'), new_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0')
2024-12-04 17:27:29,746 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 64: ref_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0'), new_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0')
2024-12-04 17:27:29,834 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 65: ref_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0'), new_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0')
2024-12-04 17:27:29,921 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 66: ref_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0'), new_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0')
2024-12-04 17:27:30,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 67: ref_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0'), new_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0')
2024-12-04 17:27:30,097 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 68: ref_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0'), new_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0')
2024-12-04 17:27:30,184 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 69: ref_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0'), new_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0')
2024-12-04 17:27:30,273 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 70: ref_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0'), new_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0')
2024-12-04 17:27:30,363 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 71: ref_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0'), new_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0')
2024-12-04 17:27:30,452 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 72: ref_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0'), new_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0')
2024-12-04 17:27:30,541 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 73: ref_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0'), new_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0')
2024-12-04 17:27:30,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 74: ref_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0'), new_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0')
2024-12-04 17:27:30,718 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 75: ref_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0'), new_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0')
2024-12-04 17:27:30,806 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 76: ref_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0'), new_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0')
2024-12-04 17:27:30,894 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 77: ref_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0'), new_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0')
2024-12-04 17:27:30,981 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 78: ref_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0'), new_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0')
2024-12-04 17:27:31,069 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 79: ref_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0'), new_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0')
2024-12-04 17:27:31,158 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 80: ref_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0'), new_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0')
2024-12-04 17:27:31,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 81: ref_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0'), new_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0')
2024-12-04 17:27:31,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 82: ref_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0'), new_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0')
2024-12-04 17:27:31,423 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 83: ref_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0'), new_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0')
2024-12-04 17:27:31,511 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 84: ref_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0'), new_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0')
2024-12-04 17:27:31,603 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 85: ref_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0'), new_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0')
2024-12-04 17:27:31,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 86: ref_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0'), new_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0')
2024-12-04 17:27:31,778 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 87: ref_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0'), new_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0')
2024-12-04 17:27:31,868 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 88: ref_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0'), new_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0')
2024-12-04 17:27:31,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 89: ref_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0'), new_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0')
2024-12-04 17:27:32,044 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 90: ref_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0'), new_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0')
2024-12-04 17:27:32,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 91: ref_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0'), new_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0')
2024-12-04 17:27:32,227 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 92: ref_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0'), new_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0')
2024-12-04 17:27:32,318 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 93: ref_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0'), new_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0')
2024-12-04 17:27:32,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 94: ref_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0'), new_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0')
2024-12-04 17:27:32,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 95: ref_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0'), new_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0')
2024-12-04 17:27:32,587 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 96: ref_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0'), new_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0')
2024-12-04 17:27:32,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 97: ref_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0'), new_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0')
2024-12-04 17:27:32,769 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 98: ref_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0'), new_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0')
2024-12-04 17:27:32,858 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:554] - INFO: Iteration 99: ref_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0'), new_distribution = tensor([0.1051, 0.3544, 0.5405], device='cuda:0')
