2024-12-04 16:09:13,679 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-04 16:09:13,687 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-04 16:09:13,693 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-04 16:09:13,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-04 16:09:13,706 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-04 16:09:13,712 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-04 16:09:13,718 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-04 16:09:13,724 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-04 16:09:13,730 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-04 16:09:13,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-04 16:09:14,112 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 0 loss: -0.2725 reward: 0.2725 ref_reward: 0.2734 improvement: -0.34%
2024-12-04 16:09:14,533 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 2 loss: -0.2794 reward: 0.2794 ref_reward: 0.2734 improvement: 2.18%
2024-12-04 16:09:14,950 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 4 loss: -0.2858 reward: 0.2858 ref_reward: 0.2734 improvement: 4.53%
2024-12-04 16:09:15,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 6 loss: -0.2917 reward: 0.2917 ref_reward: 0.2734 improvement: 6.71%
2024-12-04 16:09:15,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 8 loss: -0.2972 reward: 0.2972 ref_reward: 0.2734 improvement: 8.71%
2024-12-04 16:09:16,181 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 10 loss: -0.3022 reward: 0.3022 ref_reward: 0.2734 improvement: 10.55%
2024-12-04 16:09:16,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 12 loss: -0.3069 reward: 0.3069 ref_reward: 0.2734 improvement: 12.24%
2024-12-04 16:09:16,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 14 loss: -0.3111 reward: 0.3111 ref_reward: 0.2734 improvement: 13.77%
2024-12-04 16:09:17,073 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 16 loss: -0.3148 reward: 0.3148 ref_reward: 0.2734 improvement: 15.14%
2024-12-04 16:09:17,364 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 18 loss: -0.3182 reward: 0.3182 ref_reward: 0.2734 improvement: 16.38%
2024-12-04 16:09:17,655 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 20 loss: -0.3212 reward: 0.3212 ref_reward: 0.2734 improvement: 17.48%
2024-12-04 16:09:17,954 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 22 loss: -0.3238 reward: 0.3238 ref_reward: 0.2734 improvement: 18.44%
2024-12-04 16:09:18,249 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 24 loss: -0.3260 reward: 0.3260 ref_reward: 0.2734 improvement: 19.25%
2024-12-04 16:09:18,539 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 26 loss: -0.3278 reward: 0.3278 ref_reward: 0.2734 improvement: 19.91%
2024-12-04 16:09:18,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 28 loss: -0.3293 reward: 0.3293 ref_reward: 0.2734 improvement: 20.44%
2024-12-04 16:09:19,138 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 30 loss: -0.3304 reward: 0.3304 ref_reward: 0.2734 improvement: 20.86%
2024-12-04 16:09:19,429 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 32 loss: -0.3313 reward: 0.3313 ref_reward: 0.2734 improvement: 21.17%
2024-12-04 16:09:19,732 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 34 loss: -0.3319 reward: 0.3319 ref_reward: 0.2734 improvement: 21.40%
2024-12-04 16:09:20,025 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 36 loss: -0.3324 reward: 0.3324 ref_reward: 0.2734 improvement: 21.57%
2024-12-04 16:09:20,317 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 38 loss: -0.3327 reward: 0.3327 ref_reward: 0.2734 improvement: 21.69%
2024-12-04 16:09:20,611 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 40 loss: -0.3329 reward: 0.3329 ref_reward: 0.2734 improvement: 21.78%
2024-12-04 16:09:20,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 42 loss: -0.3331 reward: 0.3331 ref_reward: 0.2734 improvement: 21.85%
2024-12-04 16:09:21,197 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 44 loss: -0.3333 reward: 0.3333 ref_reward: 0.2734 improvement: 21.90%
2024-12-04 16:09:21,491 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 46 loss: -0.3334 reward: 0.3334 ref_reward: 0.2734 improvement: 21.93%
2024-12-04 16:09:21,784 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 48 loss: -0.3334 reward: 0.3334 ref_reward: 0.2734 improvement: 21.96%
2024-12-04 16:09:22,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 50 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.97%
2024-12-04 16:09:22,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 52 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:22,663 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 54 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:22,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 56 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:23,249 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 58 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:23,543 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 60 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:23,837 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 62 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:24,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 64 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:24,423 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 66 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:24,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 68 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:25,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 70 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:26,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 72 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:26,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 74 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:26,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 76 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:27,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 78 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:27,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 80 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:28,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 82 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:28,603 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 84 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:29,023 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 86 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:29,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 88 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:29,677 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 90 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:29,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 92 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:30,268 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 94 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:30,564 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 96 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:30,860 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 98 loss: -0.3335 reward: 0.3335 ref_reward: 0.2734 improvement: 21.98%
2024-12-04 16:09:31,416 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 0 loss: -0.2397 reward: 0.2397 ref_reward: 0.3541 improvement: -32.29%
2024-12-04 16:09:31,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 2 loss: -0.2545 reward: 0.2545 ref_reward: 0.3541 improvement: -28.11%
2024-12-04 16:09:32,135 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 4 loss: -0.2663 reward: 0.2663 ref_reward: 0.3541 improvement: -24.78%
2024-12-04 16:09:32,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 6 loss: -0.2766 reward: 0.2766 ref_reward: 0.3541 improvement: -21.89%
2024-12-04 16:09:33,236 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 8 loss: -0.2850 reward: 0.2850 ref_reward: 0.3541 improvement: -19.50%
2024-12-04 16:09:33,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 10 loss: -0.2924 reward: 0.2924 ref_reward: 0.3541 improvement: -17.41%
2024-12-04 16:09:34,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 12 loss: -0.2993 reward: 0.2993 ref_reward: 0.3541 improvement: -15.46%
2024-12-04 16:09:34,909 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 14 loss: -0.3059 reward: 0.3059 ref_reward: 0.3541 improvement: -13.59%
2024-12-04 16:09:35,462 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 16 loss: -0.3124 reward: 0.3124 ref_reward: 0.3541 improvement: -11.77%
2024-12-04 16:09:35,938 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 18 loss: -0.3187 reward: 0.3187 ref_reward: 0.3541 improvement: -9.98%
2024-12-04 16:09:36,297 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 20 loss: -0.3249 reward: 0.3249 ref_reward: 0.3541 improvement: -8.25%
2024-12-04 16:09:36,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 22 loss: -0.3308 reward: 0.3308 ref_reward: 0.3541 improvement: -6.58%
2024-12-04 16:09:36,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 24 loss: -0.3364 reward: 0.3364 ref_reward: 0.3541 improvement: -5.00%
2024-12-04 16:09:37,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 26 loss: -0.3417 reward: 0.3417 ref_reward: 0.3541 improvement: -3.51%
2024-12-04 16:09:37,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 28 loss: -0.3466 reward: 0.3466 ref_reward: 0.3541 improvement: -2.12%
2024-12-04 16:09:37,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 30 loss: -0.3510 reward: 0.3510 ref_reward: 0.3541 improvement: -0.87%
2024-12-04 16:09:38,083 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 32 loss: -0.3549 reward: 0.3549 ref_reward: 0.3541 improvement: 0.24%
2024-12-04 16:09:38,465 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 34 loss: -0.3583 reward: 0.3583 ref_reward: 0.3541 improvement: 1.20%
2024-12-04 16:09:38,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 36 loss: -0.3612 reward: 0.3612 ref_reward: 0.3541 improvement: 2.00%
2024-12-04 16:09:39,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 38 loss: -0.3634 reward: 0.3634 ref_reward: 0.3541 improvement: 2.65%
2024-12-04 16:09:40,062 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 40 loss: -0.3652 reward: 0.3652 ref_reward: 0.3541 improvement: 3.15%
2024-12-04 16:09:40,478 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 42 loss: -0.3666 reward: 0.3666 ref_reward: 0.3541 improvement: 3.52%
2024-12-04 16:09:40,801 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 44 loss: -0.3675 reward: 0.3675 ref_reward: 0.3541 improvement: 3.79%
2024-12-04 16:09:41,092 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 46 loss: -0.3682 reward: 0.3682 ref_reward: 0.3541 improvement: 3.98%
2024-12-04 16:09:41,381 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 48 loss: -0.3686 reward: 0.3686 ref_reward: 0.3541 improvement: 4.11%
2024-12-04 16:09:41,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 50 loss: -0.3690 reward: 0.3690 ref_reward: 0.3541 improvement: 4.21%
2024-12-04 16:09:41,964 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 52 loss: -0.3692 reward: 0.3692 ref_reward: 0.3541 improvement: 4.28%
2024-12-04 16:09:42,256 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 54 loss: -0.3695 reward: 0.3695 ref_reward: 0.3541 improvement: 4.35%
2024-12-04 16:09:42,547 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 56 loss: -0.3697 reward: 0.3697 ref_reward: 0.3541 improvement: 4.40%
2024-12-04 16:09:42,838 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 58 loss: -0.3698 reward: 0.3698 ref_reward: 0.3541 improvement: 4.45%
2024-12-04 16:09:43,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 60 loss: -0.3700 reward: 0.3700 ref_reward: 0.3541 improvement: 4.51%
2024-12-04 16:09:43,422 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 62 loss: -0.3702 reward: 0.3702 ref_reward: 0.3541 improvement: 4.56%
2024-12-04 16:09:43,712 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 64 loss: -0.3704 reward: 0.3704 ref_reward: 0.3541 improvement: 4.60%
2024-12-04 16:09:44,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 66 loss: -0.3705 reward: 0.3705 ref_reward: 0.3541 improvement: 4.64%
2024-12-04 16:09:44,292 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 68 loss: -0.3706 reward: 0.3706 ref_reward: 0.3541 improvement: 4.67%
2024-12-04 16:09:44,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 70 loss: -0.3707 reward: 0.3707 ref_reward: 0.3541 improvement: 4.70%
2024-12-04 16:09:44,873 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 72 loss: -0.3708 reward: 0.3708 ref_reward: 0.3541 improvement: 4.72%
2024-12-04 16:09:45,164 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 74 loss: -0.3708 reward: 0.3708 ref_reward: 0.3541 improvement: 4.73%
2024-12-04 16:09:45,452 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 76 loss: -0.3709 reward: 0.3709 ref_reward: 0.3541 improvement: 4.74%
2024-12-04 16:09:45,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 78 loss: -0.3709 reward: 0.3709 ref_reward: 0.3541 improvement: 4.75%
2024-12-04 16:09:46,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 80 loss: -0.3709 reward: 0.3709 ref_reward: 0.3541 improvement: 4.75%
2024-12-04 16:09:46,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 82 loss: -0.3709 reward: 0.3709 ref_reward: 0.3541 improvement: 4.76%
2024-12-04 16:09:46,618 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 84 loss: -0.3710 reward: 0.3710 ref_reward: 0.3541 improvement: 4.77%
2024-12-04 16:09:46,909 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 86 loss: -0.3710 reward: 0.3710 ref_reward: 0.3541 improvement: 4.78%
2024-12-04 16:09:47,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 88 loss: -0.3710 reward: 0.3710 ref_reward: 0.3541 improvement: 4.78%
2024-12-04 16:09:47,489 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 90 loss: -0.3710 reward: 0.3710 ref_reward: 0.3541 improvement: 4.79%
2024-12-04 16:09:47,783 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 92 loss: -0.3710 reward: 0.3710 ref_reward: 0.3541 improvement: 4.79%
2024-12-04 16:09:48,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 94 loss: -0.3711 reward: 0.3711 ref_reward: 0.3541 improvement: 4.80%
2024-12-04 16:09:48,371 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 96 loss: -0.3711 reward: 0.3711 ref_reward: 0.3541 improvement: 4.80%
2024-12-04 16:09:48,666 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 98 loss: -0.3711 reward: 0.3711 ref_reward: 0.3541 improvement: 4.80%
2024-12-04 16:09:49,193 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 0 loss: 1.1174 reward: -1.1174 ref_reward: 0.3861 improvement: -389.43%
2024-12-04 16:09:49,484 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 2 loss: 1.0581 reward: -1.0581 ref_reward: 0.3861 improvement: -374.08%
2024-12-04 16:09:49,775 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 4 loss: 0.9996 reward: -0.9996 ref_reward: 0.3861 improvement: -358.93%
2024-12-04 16:09:50,065 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 6 loss: 0.9391 reward: -0.9391 ref_reward: 0.3861 improvement: -343.24%
2024-12-04 16:09:50,356 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 8 loss: 0.8787 reward: -0.8787 ref_reward: 0.3861 improvement: -327.61%
2024-12-04 16:09:50,648 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 10 loss: 0.8179 reward: -0.8179 ref_reward: 0.3861 improvement: -311.85%
2024-12-04 16:09:50,937 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 12 loss: 0.7546 reward: -0.7546 ref_reward: 0.3861 improvement: -295.47%
2024-12-04 16:09:51,228 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 14 loss: 0.6902 reward: -0.6902 ref_reward: 0.3861 improvement: -278.77%
2024-12-04 16:09:51,520 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 16 loss: 0.6236 reward: -0.6236 ref_reward: 0.3861 improvement: -261.54%
2024-12-04 16:09:51,811 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 18 loss: 0.5550 reward: -0.5550 ref_reward: 0.3861 improvement: -243.77%
2024-12-04 16:09:52,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 20 loss: 0.4847 reward: -0.4847 ref_reward: 0.3861 improvement: -225.55%
2024-12-04 16:09:52,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 22 loss: 0.4129 reward: -0.4129 ref_reward: 0.3861 improvement: -206.96%
2024-12-04 16:09:52,683 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 24 loss: 0.3409 reward: -0.3409 ref_reward: 0.3861 improvement: -188.30%
2024-12-04 16:09:52,974 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 26 loss: 0.2710 reward: -0.2710 ref_reward: 0.3861 improvement: -170.20%
2024-12-04 16:09:53,264 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 28 loss: 0.2023 reward: -0.2023 ref_reward: 0.3861 improvement: -152.41%
2024-12-04 16:09:53,555 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 30 loss: 0.1360 reward: -0.1360 ref_reward: 0.3861 improvement: -135.22%
2024-12-04 16:09:53,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 32 loss: 0.0729 reward: -0.0729 ref_reward: 0.3861 improvement: -118.89%
2024-12-04 16:09:54,135 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 34 loss: 0.0142 reward: -0.0142 ref_reward: 0.3861 improvement: -103.67%
2024-12-04 16:09:54,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 36 loss: -0.0396 reward: 0.0396 ref_reward: 0.3861 improvement: -89.73%
2024-12-04 16:09:54,716 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 38 loss: -0.0880 reward: 0.0880 ref_reward: 0.3861 improvement: -77.20%
2024-12-04 16:09:55,006 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 40 loss: -0.1308 reward: 0.1308 ref_reward: 0.3861 improvement: -66.11%
2024-12-04 16:09:55,298 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 42 loss: -0.1681 reward: 0.1681 ref_reward: 0.3861 improvement: -56.45%
2024-12-04 16:09:55,589 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 44 loss: -0.2002 reward: 0.2002 ref_reward: 0.3861 improvement: -48.14%
2024-12-04 16:09:55,880 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 46 loss: -0.2275 reward: 0.2275 ref_reward: 0.3861 improvement: -41.07%
2024-12-04 16:09:56,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 48 loss: -0.2505 reward: 0.2505 ref_reward: 0.3861 improvement: -35.11%
2024-12-04 16:09:56,461 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 50 loss: -0.2698 reward: 0.2698 ref_reward: 0.3861 improvement: -30.12%
2024-12-04 16:09:56,753 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 52 loss: -0.2858 reward: 0.2858 ref_reward: 0.3861 improvement: -25.96%
2024-12-04 16:09:57,045 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 54 loss: -0.2992 reward: 0.2992 ref_reward: 0.3861 improvement: -22.49%
2024-12-04 16:09:57,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 56 loss: -0.3103 reward: 0.3103 ref_reward: 0.3861 improvement: -19.61%
2024-12-04 16:09:57,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 58 loss: -0.3196 reward: 0.3196 ref_reward: 0.3861 improvement: -17.21%
2024-12-04 16:09:57,918 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 60 loss: -0.3274 reward: 0.3274 ref_reward: 0.3861 improvement: -15.21%
2024-12-04 16:09:58,209 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 62 loss: -0.3338 reward: 0.3338 ref_reward: 0.3861 improvement: -13.53%
2024-12-04 16:09:58,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 64 loss: -0.3393 reward: 0.3393 ref_reward: 0.3861 improvement: -12.11%
2024-12-04 16:09:58,788 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 66 loss: -0.3439 reward: 0.3439 ref_reward: 0.3861 improvement: -10.92%
2024-12-04 16:09:59,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 68 loss: -0.3478 reward: 0.3478 ref_reward: 0.3861 improvement: -9.90%
2024-12-04 16:09:59,371 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 70 loss: -0.3512 reward: 0.3512 ref_reward: 0.3861 improvement: -9.03%
2024-12-04 16:09:59,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 72 loss: -0.3542 reward: 0.3542 ref_reward: 0.3861 improvement: -8.26%
2024-12-04 16:09:59,955 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 74 loss: -0.3568 reward: 0.3568 ref_reward: 0.3861 improvement: -7.58%
2024-12-04 16:10:00,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 76 loss: -0.3591 reward: 0.3591 ref_reward: 0.3861 improvement: -6.97%
2024-12-04 16:10:00,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 78 loss: -0.3612 reward: 0.3612 ref_reward: 0.3861 improvement: -6.44%
2024-12-04 16:10:00,831 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 80 loss: -0.3630 reward: 0.3630 ref_reward: 0.3861 improvement: -5.97%
2024-12-04 16:10:01,126 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 82 loss: -0.3647 reward: 0.3647 ref_reward: 0.3861 improvement: -5.54%
2024-12-04 16:10:01,418 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 84 loss: -0.3661 reward: 0.3661 ref_reward: 0.3861 improvement: -5.16%
2024-12-04 16:10:01,712 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 86 loss: -0.3674 reward: 0.3674 ref_reward: 0.3861 improvement: -4.82%
2024-12-04 16:10:02,003 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 88 loss: -0.3686 reward: 0.3686 ref_reward: 0.3861 improvement: -4.51%
2024-12-04 16:10:02,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 90 loss: -0.3697 reward: 0.3697 ref_reward: 0.3861 improvement: -4.23%
2024-12-04 16:10:02,583 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 92 loss: -0.3707 reward: 0.3707 ref_reward: 0.3861 improvement: -3.98%
2024-12-04 16:10:02,875 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 94 loss: -0.3716 reward: 0.3716 ref_reward: 0.3861 improvement: -3.74%
2024-12-04 16:10:03,165 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 96 loss: -0.3724 reward: 0.3724 ref_reward: 0.3861 improvement: -3.53%
2024-12-04 16:10:03,456 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 98 loss: -0.3732 reward: 0.3732 ref_reward: 0.3861 improvement: -3.33%
2024-12-04 16:10:04,192 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 0 loss: -0.2491 reward: 0.2491 ref_reward: 0.1811 improvement: 37.54%
2024-12-04 16:10:04,481 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 2 loss: -0.2547 reward: 0.2547 ref_reward: 0.1811 improvement: 40.63%
2024-12-04 16:10:04,770 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 4 loss: -0.2594 reward: 0.2594 ref_reward: 0.1811 improvement: 43.20%
2024-12-04 16:10:05,057 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 6 loss: -0.2632 reward: 0.2632 ref_reward: 0.1811 improvement: 45.30%
2024-12-04 16:10:05,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 8 loss: -0.2662 reward: 0.2662 ref_reward: 0.1811 improvement: 46.97%
2024-12-04 16:10:05,631 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 10 loss: -0.2687 reward: 0.2687 ref_reward: 0.1811 improvement: 48.32%
2024-12-04 16:10:05,914 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 12 loss: -0.2706 reward: 0.2706 ref_reward: 0.1811 improvement: 49.39%
2024-12-04 16:10:06,200 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 14 loss: -0.2721 reward: 0.2721 ref_reward: 0.1811 improvement: 50.22%
2024-12-04 16:10:06,489 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 16 loss: -0.2732 reward: 0.2732 ref_reward: 0.1811 improvement: 50.81%
2024-12-04 16:10:06,777 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 18 loss: -0.2739 reward: 0.2739 ref_reward: 0.1811 improvement: 51.22%
2024-12-04 16:10:07,065 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 20 loss: -0.2744 reward: 0.2744 ref_reward: 0.1811 improvement: 51.47%
2024-12-04 16:10:07,354 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 22 loss: -0.2746 reward: 0.2746 ref_reward: 0.1811 improvement: 51.60%
2024-12-04 16:10:07,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 24 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.65%
2024-12-04 16:10:07,931 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 26 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.65%
2024-12-04 16:10:08,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 28 loss: -0.2746 reward: 0.2746 ref_reward: 0.1811 improvement: 51.62%
2024-12-04 16:10:08,506 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 30 loss: -0.2746 reward: 0.2746 ref_reward: 0.1811 improvement: 51.60%
2024-12-04 16:10:08,795 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 32 loss: -0.2746 reward: 0.2746 ref_reward: 0.1811 improvement: 51.58%
2024-12-04 16:10:09,084 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 34 loss: -0.2745 reward: 0.2745 ref_reward: 0.1811 improvement: 51.57%
2024-12-04 16:10:09,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 36 loss: -0.2745 reward: 0.2745 ref_reward: 0.1811 improvement: 51.56%
2024-12-04 16:10:09,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 38 loss: -0.2745 reward: 0.2745 ref_reward: 0.1811 improvement: 51.56%
2024-12-04 16:10:09,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 40 loss: -0.2745 reward: 0.2745 ref_reward: 0.1811 improvement: 51.56%
2024-12-04 16:10:10,237 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 42 loss: -0.2745 reward: 0.2745 ref_reward: 0.1811 improvement: 51.56%
2024-12-04 16:10:10,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 44 loss: -0.2746 reward: 0.2746 ref_reward: 0.1811 improvement: 51.58%
2024-12-04 16:10:10,814 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 46 loss: -0.2746 reward: 0.2746 ref_reward: 0.1811 improvement: 51.60%
2024-12-04 16:10:11,104 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 48 loss: -0.2746 reward: 0.2746 ref_reward: 0.1811 improvement: 51.62%
2024-12-04 16:10:11,393 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 50 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.63%
2024-12-04 16:10:11,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 52 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.65%
2024-12-04 16:10:11,970 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 54 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:12,260 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 56 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:12,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 58 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:12,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 60 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:13,126 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 62 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:13,416 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 64 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:13,710 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 66 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:14,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 68 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:14,290 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 70 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:14,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 72 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:14,951 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 74 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:15,366 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 76 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.66%
2024-12-04 16:10:15,741 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 78 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:16,043 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 80 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:16,335 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 82 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:16,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 84 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:16,914 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 86 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:17,204 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 88 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:17,494 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 90 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:17,788 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 92 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:18,079 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 94 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:18,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 96 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:18,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:216] - INFO: [Policy] Epoch 98 loss: -0.2747 reward: 0.2747 ref_reward: 0.1811 improvement: 51.67%
2024-12-04 16:10:19,891 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 0 loss: 0.7042 grad norm: 0.5207 policy: 0.3292 0.3372
2024-12-04 16:10:20,613 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 5 loss: 0.6582 grad norm: 0.3695 policy: 0.3629 0.3653
2024-12-04 16:10:21,342 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 10 loss: 0.6286 grad norm: 0.2629 policy: 0.3932 0.3845
2024-12-04 16:10:22,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 15 loss: 0.6107 grad norm: 0.1713 policy: 0.4212 0.3960
2024-12-04 16:10:22,801 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 20 loss: 0.6016 grad norm: 0.0861 policy: 0.4463 0.4021
2024-12-04 16:10:23,615 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 25 loss: 0.5989 grad norm: 0.0196 policy: 0.4679 0.4022
2024-12-04 16:10:25,005 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 30 loss: 0.5994 grad norm: 0.0429 policy: 0.4848 0.3976
2024-12-04 16:10:26,359 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 35 loss: 0.6001 grad norm: 0.0613 policy: 0.4945 0.3914
2024-12-04 16:10:27,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 40 loss: 0.5999 grad norm: 0.0560 policy: 0.4959 0.3875
2024-12-04 16:10:27,984 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 45 loss: 0.5993 grad norm: 0.0377 policy: 0.4907 0.3871
2024-12-04 16:10:28,717 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 50 loss: 0.5989 grad norm: 0.0169 policy: 0.4823 0.3897
2024-12-04 16:10:29,453 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0042 policy: 0.4744 0.3934
2024-12-04 16:10:30,756 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 60 loss: 0.5989 grad norm: 0.0118 policy: 0.4694 0.3962
2024-12-04 16:10:32,135 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 65 loss: 0.5989 grad norm: 0.0144 policy: 0.4682 0.3971
2024-12-04 16:10:33,467 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 70 loss: 0.5989 grad norm: 0.0115 policy: 0.4700 0.3964
2024-12-04 16:10:34,535 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0058 policy: 0.4730 0.3949
2024-12-04 16:10:35,298 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0009 policy: 0.4756 0.3937
2024-12-04 16:10:36,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0032 policy: 0.4768 0.3933
2024-12-04 16:10:36,801 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0040 policy: 0.4767 0.3936
2024-12-04 16:10:37,562 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0031 policy: 0.4760 0.3941
2024-12-04 16:10:38,565 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 0 loss: 1.1496 grad norm: 1.0781 policy: 0.3211 0.3291
2024-12-04 16:10:39,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 5 loss: 1.0489 grad norm: 0.9764 policy: 0.3664 0.3442
2024-12-04 16:10:40,062 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 10 loss: 0.9583 grad norm: 0.9650 policy: 0.4136 0.3502
2024-12-04 16:10:40,815 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 15 loss: 0.8784 grad norm: 0.9632 policy: 0.4575 0.3522
2024-12-04 16:10:41,576 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 20 loss: 0.8087 grad norm: 0.9428 policy: 0.4995 0.3503
2024-12-04 16:10:42,326 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 25 loss: 0.7432 grad norm: 0.8854 policy: 0.5507 0.3337
2024-12-04 16:10:43,088 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 30 loss: 0.6854 grad norm: 0.7638 policy: 0.6124 0.3005
2024-12-04 16:10:43,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 35 loss: 0.6405 grad norm: 0.5767 policy: 0.6782 0.2574
2024-12-04 16:10:44,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 40 loss: 0.6128 grad norm: 0.3520 policy: 0.7369 0.2156
2024-12-04 16:10:45,636 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 45 loss: 0.6011 grad norm: 0.1488 policy: 0.7773 0.1868
2024-12-04 16:10:46,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 50 loss: 0.5991 grad norm: 0.0606 policy: 0.7970 0.1742
2024-12-04 16:10:47,139 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 55 loss: 0.6005 grad norm: 0.1279 policy: 0.8023 0.1725
2024-12-04 16:10:47,889 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 60 loss: 0.6016 grad norm: 0.1641 policy: 0.8018 0.1741
2024-12-04 16:10:48,644 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 65 loss: 0.6012 grad norm: 0.1529 policy: 0.8003 0.1749
2024-12-04 16:10:49,399 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 70 loss: 0.6001 grad norm: 0.1127 policy: 0.7978 0.1757
2024-12-04 16:10:50,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 75 loss: 0.5992 grad norm: 0.0659 policy: 0.7929 0.1785
2024-12-04 16:10:51,961 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 80 loss: 0.5989 grad norm: 0.0232 policy: 0.7864 0.1833
2024-12-04 16:10:53,331 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0110 policy: 0.7809 0.1876
2024-12-04 16:10:54,251 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 90 loss: 0.5989 grad norm: 0.0283 policy: 0.7783 0.1896
2024-12-04 16:10:55,011 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 95 loss: 0.5989 grad norm: 0.0309 policy: 0.7787 0.1892
2024-12-04 16:10:56,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 0 loss: 9.1513 grad norm: 0.9833 policy: 0.3115 0.3638
2024-12-04 16:10:56,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 5 loss: 9.0573 grad norm: 0.9764 policy: 0.3637 0.3416
2024-12-04 16:10:57,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 10 loss: 8.9700 grad norm: 1.0087 policy: 0.4154 0.3189
2024-12-04 16:10:58,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 15 loss: 8.8790 grad norm: 1.1087 policy: 0.4721 0.2944
2024-12-04 16:10:59,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 20 loss: 8.7852 grad norm: 1.2103 policy: 0.5297 0.2698
2024-12-04 16:10:59,890 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 25 loss: 8.6801 grad norm: 1.3625 policy: 0.5950 0.2395
2024-12-04 16:11:00,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 30 loss: 8.5573 grad norm: 1.5311 policy: 0.6674 0.2030
2024-12-04 16:11:01,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 35 loss: 8.4140 grad norm: 1.7156 policy: 0.7432 0.1619
2024-12-04 16:11:02,148 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 40 loss: 8.2472 grad norm: 1.9163 policy: 0.8160 0.1197
2024-12-04 16:11:03,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 45 loss: 8.0543 grad norm: 2.1340 policy: 0.8793 0.0810
2024-12-04 16:11:04,542 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 50 loss: 7.8322 grad norm: 2.3692 policy: 0.9282 0.0497
2024-12-04 16:11:05,303 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 55 loss: 7.5778 grad norm: 2.6227 policy: 0.9615 0.0274
2024-12-04 16:11:06,068 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 60 loss: 7.2882 grad norm: 2.8950 policy: 0.9815 0.0135
2024-12-04 16:11:06,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 65 loss: 6.9605 grad norm: 3.1865 policy: 0.9920 0.0060
2024-12-04 16:11:07,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 70 loss: 6.5917 grad norm: 3.4974 policy: 0.9969 0.0023
2024-12-04 16:11:08,279 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 75 loss: 6.1791 grad norm: 3.8280 policy: 0.9990 0.0008
2024-12-04 16:11:09,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 80 loss: 5.7196 grad norm: 4.1782 policy: 0.9997 0.0002
2024-12-04 16:11:10,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 85 loss: 5.2103 grad norm: 4.5483 policy: 0.9999 0.0001
2024-12-04 16:11:11,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 90 loss: 4.6484 grad norm: 4.9380 policy: 1.0000 0.0000
2024-12-04 16:11:11,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 95 loss: 4.0309 grad norm: 5.3469 policy: 1.0000 0.0000
2024-12-04 16:11:12,888 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 0 loss: 0.6346 grad norm: 0.2601 policy: 0.3073 0.3660
2024-12-04 16:11:13,631 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 5 loss: 0.6132 grad norm: 0.1728 policy: 0.2651 0.4223
2024-12-04 16:11:14,370 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 10 loss: 0.6022 grad norm: 0.0875 policy: 0.2256 0.4683
2024-12-04 16:11:15,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 15 loss: 0.5992 grad norm: 0.0270 policy: 0.1943 0.5004
2024-12-04 16:11:15,848 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 20 loss: 0.5999 grad norm: 0.0648 policy: 0.1763 0.5076
2024-12-04 16:11:16,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 25 loss: 0.6001 grad norm: 0.0645 policy: 0.1748 0.4957
2024-12-04 16:11:17,328 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 30 loss: 0.5993 grad norm: 0.0348 policy: 0.1838 0.4831
2024-12-04 16:11:18,059 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 35 loss: 0.5989 grad norm: 0.0129 policy: 0.1947 0.4777
2024-12-04 16:11:18,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 40 loss: 0.5989 grad norm: 0.0175 policy: 0.2013 0.4791
2024-12-04 16:11:19,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 45 loss: 0.5990 grad norm: 0.0189 policy: 0.2021 0.4830
2024-12-04 16:11:20,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 50 loss: 0.5989 grad norm: 0.0129 policy: 0.1989 0.4854
2024-12-04 16:11:21,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 55 loss: 0.5988 grad norm: 0.0034 policy: 0.1946 0.4853
2024-12-04 16:11:21,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0055 policy: 0.1920 0.4842
2024-12-04 16:11:22,475 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0072 policy: 0.1918 0.4840
2024-12-04 16:11:23,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0038 policy: 0.1933 0.4845
2024-12-04 16:11:23,959 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0011 policy: 0.1950 0.4847
2024-12-04 16:11:24,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0029 policy: 0.1957 0.4843
2024-12-04 16:11:25,439 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0024 policy: 0.1953 0.4838
2024-12-04 16:11:26,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0010 policy: 0.1945 0.4839
2024-12-04 16:11:26,917 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:345] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0010 policy: 0.1940 0.4843
2024-12-04 16:11:28,523 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 0 loss: 0.0097 grad norm: 0.3984 
2024-12-04 16:11:29,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 5 loss: 0.0094 grad norm: 0.2920 
2024-12-04 16:11:30,042 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 10 loss: 0.0002 grad norm: 0.0446 
2024-12-04 16:11:30,784 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 15 loss: 0.0020 grad norm: 0.1555 
2024-12-04 16:11:31,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 20 loss: 0.0011 grad norm: 0.0986 
2024-12-04 16:11:32,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 25 loss: 0.0006 grad norm: 0.0743 
2024-12-04 16:11:33,022 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 30 loss: 0.0004 grad norm: 0.0606 
2024-12-04 16:11:33,779 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0104 
2024-12-04 16:11:34,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0162 
2024-12-04 16:11:35,266 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0047 
2024-12-04 16:11:36,007 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0095 
2024-12-04 16:11:36,762 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0023 
2024-12-04 16:11:37,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0052 
2024-12-04 16:11:38,259 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0033 
2024-12-04 16:11:39,001 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0004 
2024-12-04 16:11:39,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0021 
2024-12-04 16:11:40,482 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0018 
2024-12-04 16:11:41,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0010 
2024-12-04 16:11:41,992 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0008 
2024-12-04 16:11:42,741 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0009 
2024-12-04 16:11:43,685 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 0 loss: 1.4357 grad norm: 3.0417 
2024-12-04 16:11:44,440 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 5 loss: 0.0260 grad norm: 0.5775 
2024-12-04 16:11:45,168 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 10 loss: 0.0055 grad norm: 0.1981 
2024-12-04 16:11:45,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 15 loss: 0.0085 grad norm: 0.2238 
2024-12-04 16:11:46,646 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 20 loss: 0.0009 grad norm: 0.0772 
2024-12-04 16:11:47,403 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 25 loss: 0.0018 grad norm: 0.1208 
2024-12-04 16:11:48,147 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 30 loss: 0.0010 grad norm: 0.0863 
2024-12-04 16:11:48,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 35 loss: 0.0005 grad norm: 0.0604 
2024-12-04 16:11:49,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 40 loss: 0.0002 grad norm: 0.0395 
2024-12-04 16:11:50,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 45 loss: 0.0001 grad norm: 0.0249 
2024-12-04 16:11:51,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0304 
2024-12-04 16:11:51,871 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0101 
2024-12-04 16:11:52,617 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 60 loss: 0.0001 grad norm: 0.0196 
2024-12-04 16:11:53,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0076 
2024-12-04 16:11:54,121 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0028 
2024-12-04 16:11:54,876 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0043 
2024-12-04 16:11:55,622 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0018 
2024-12-04 16:11:56,377 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0026 
2024-12-04 16:11:57,137 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0009 
2024-12-04 16:11:57,873 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0018 
2024-12-04 16:11:58,824 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 0 loss: 676.8475 grad norm: 52.1276 
2024-12-04 16:11:59,570 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 5 loss: 0.3632 grad norm: 3.6709 
2024-12-04 16:12:00,309 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 10 loss: 0.3057 grad norm: 3.2829 
2024-12-04 16:12:01,052 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 15 loss: 0.0470 grad norm: 0.9967 
2024-12-04 16:12:01,796 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 20 loss: 0.0068 grad norm: 0.3375 
2024-12-04 16:12:02,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 25 loss: 0.0194 grad norm: 0.5092 
2024-12-04 16:12:03,276 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 30 loss: 0.0128 grad norm: 0.3768 
2024-12-04 16:12:04,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 35 loss: 0.0000 grad norm: 0.0127 
2024-12-04 16:12:04,760 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0130 
2024-12-04 16:12:05,497 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 45 loss: 0.0000 grad norm: 0.0093 
2024-12-04 16:12:06,241 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0041 
2024-12-04 16:12:06,980 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0011 
2024-12-04 16:12:07,732 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0032 
2024-12-04 16:12:08,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0008 
2024-12-04 16:12:09,203 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0011 
2024-12-04 16:12:09,938 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0010 
2024-12-04 16:12:10,684 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0006 
2024-12-04 16:12:11,426 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0001 
2024-12-04 16:12:12,178 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0002 
2024-12-04 16:12:12,923 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0005 
2024-12-04 16:12:13,856 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 0 loss: 0.9924 grad norm: 3.6010 
2024-12-04 16:12:14,601 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 5 loss: 0.0270 grad norm: 0.7180 
2024-12-04 16:12:15,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 10 loss: 0.0137 grad norm: 0.3444 
2024-12-04 16:12:16,092 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 15 loss: 0.0097 grad norm: 0.2924 
2024-12-04 16:12:16,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 20 loss: 0.0005 grad norm: 0.0807 
2024-12-04 16:12:17,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 25 loss: 0.0042 grad norm: 0.2335 
2024-12-04 16:12:18,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 30 loss: 0.0001 grad norm: 0.0304 
2024-12-04 16:12:19,070 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 35 loss: 0.0010 grad norm: 0.0966 
2024-12-04 16:12:19,811 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 40 loss: 0.0003 grad norm: 0.0553 
2024-12-04 16:12:20,563 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 45 loss: 0.0002 grad norm: 0.0487 
2024-12-04 16:12:21,295 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 50 loss: 0.0001 grad norm: 0.0394 
2024-12-04 16:12:22,036 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 55 loss: 0.0001 grad norm: 0.0269 
2024-12-04 16:12:22,788 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 60 loss: 0.0001 grad norm: 0.0243 
2024-12-04 16:12:23,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0203 
2024-12-04 16:12:24,291 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0033 
2024-12-04 16:12:25,025 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0123 
2024-12-04 16:12:25,762 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0006 
2024-12-04 16:12:26,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0075 
2024-12-04 16:12:27,238 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0016 
2024-12-04 16:12:27,978 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:469] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0036 
2024-12-04 16:12:29,391 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-04 16:12:29,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-04 16:12:29,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-04 16:12:29,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-04 16:12:29,746 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-04 16:12:29,835 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-04 16:12:29,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-04 16:12:30,014 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-04 16:12:30,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-04 16:12:30,192 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-04 16:12:30,281 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-04 16:12:30,370 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-04 16:12:30,459 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-04 16:12:30,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-04 16:12:30,637 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-04 16:12:30,727 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-04 16:12:30,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-04 16:12:30,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-04 16:12:30,996 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-04 16:12:31,085 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-04 16:12:31,175 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-04 16:12:31,264 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-04 16:12:31,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-04 16:12:31,442 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-04 16:12:31,531 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-04 16:12:31,620 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-04 16:12:31,709 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-04 16:12:31,798 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-04 16:12:31,888 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-04 16:12:31,977 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-04 16:12:32,066 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-04 16:12:32,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-04 16:12:32,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-04 16:12:32,335 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-04 16:12:32,424 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-04 16:12:32,514 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-04 16:12:32,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-04 16:12:32,697 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-04 16:12:32,786 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-04 16:12:32,875 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-04 16:12:32,964 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-04 16:12:33,056 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-04 16:12:33,145 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-04 16:12:33,234 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-04 16:12:33,324 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-04 16:12:33,413 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-04 16:12:33,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-04 16:12:33,680 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-04 16:12:33,845 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-04 16:12:34,011 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-04 16:12:34,176 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-04 16:12:34,341 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-04 16:12:34,507 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-04 16:12:34,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-04 16:12:34,850 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-04 16:12:35,015 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-04 16:12:35,175 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-04 16:12:35,326 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-04 16:12:35,464 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-04 16:12:35,590 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-04 16:12:35,707 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-04 16:12:35,815 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-04 16:12:35,915 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-04 16:12:36,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-04 16:12:36,099 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-04 16:12:36,188 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-04 16:12:36,277 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-04 16:12:36,366 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-04 16:12:36,455 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-04 16:12:36,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-04 16:12:36,634 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-04 16:12:36,723 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-04 16:12:36,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-04 16:12:36,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-04 16:12:36,990 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-04 16:12:37,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-04 16:12:37,169 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-04 16:12:37,258 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-04 16:12:37,348 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-04 16:12:37,437 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-04 16:12:37,526 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-04 16:12:37,682 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-04 16:12:37,847 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-04 16:12:38,015 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-04 16:12:38,182 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-04 16:12:38,350 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-04 16:12:38,517 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-04 16:12:38,685 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-04 16:12:38,852 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-04 16:12:39,022 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-04 16:12:39,192 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-04 16:12:39,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-04 16:12:39,491 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-04 16:12:39,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-04 16:12:39,746 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-04 16:12:39,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-04 16:12:39,962 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-04 16:12:40,058 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-04 16:12:40,149 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-04 16:12:40,239 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-04 16:12:40,564 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-04 16:12:40,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-04 16:12:40,746 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-04 16:12:40,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-04 16:12:40,926 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-04 16:12:41,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-04 16:12:41,107 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-04 16:12:41,197 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-04 16:12:41,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-04 16:12:41,375 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-04 16:12:41,465 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-04 16:12:41,555 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-04 16:12:41,646 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-04 16:12:41,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-04 16:12:41,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-04 16:12:41,917 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-04 16:12:42,007 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-04 16:12:42,098 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-04 16:12:42,188 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-04 16:12:42,278 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-04 16:12:42,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-04 16:12:42,458 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-04 16:12:42,548 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-04 16:12:42,638 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-04 16:12:42,728 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-04 16:12:42,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-04 16:12:42,908 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-04 16:12:42,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-04 16:12:43,089 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-04 16:12:43,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-04 16:12:43,269 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-04 16:12:43,359 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-04 16:12:43,449 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-04 16:12:43,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-04 16:12:43,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-04 16:12:43,720 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-04 16:12:43,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-04 16:12:43,900 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-04 16:12:43,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-04 16:12:44,081 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-04 16:12:44,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-04 16:12:44,262 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-04 16:12:44,352 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-04 16:12:44,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-04 16:12:44,536 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-04 16:12:44,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-04 16:12:44,718 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-04 16:12:44,808 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-04 16:12:44,898 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-04 16:12:44,990 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-04 16:12:45,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-04 16:12:45,171 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-04 16:12:45,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-04 16:12:45,351 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-04 16:12:45,442 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-04 16:12:45,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-04 16:12:45,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-04 16:12:45,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-04 16:12:45,804 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-04 16:12:45,895 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-04 16:12:45,985 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-04 16:12:46,076 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-04 16:12:46,166 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-04 16:12:46,255 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-04 16:12:46,345 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-04 16:12:46,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-04 16:12:46,525 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-04 16:12:46,615 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-04 16:12:46,706 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-04 16:12:46,796 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-04 16:12:46,886 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-04 16:12:46,976 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-04 16:12:47,067 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-04 16:12:47,158 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-04 16:12:47,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-04 16:12:47,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-04 16:12:47,428 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-04 16:12:47,519 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-04 16:12:47,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-04 16:12:47,702 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-04 16:12:47,793 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-04 16:12:47,884 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-04 16:12:47,975 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-04 16:12:48,065 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-04 16:12:48,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-04 16:12:48,246 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-04 16:12:48,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-04 16:12:48,425 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-04 16:12:48,516 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-04 16:12:48,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-04 16:12:48,769 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-04 16:12:48,936 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-04 16:12:49,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-04 16:12:49,242 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-04 16:12:49,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-04 16:12:49,494 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-04 16:12:49,621 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-04 16:12:49,748 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-04 16:12:49,877 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-04 16:12:50,004 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-04 16:12:50,458 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:50,586 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:50,713 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:50,840 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:50,967 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,334 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,440 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,633 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,741 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,866 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:51,992 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:52,119 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:52,245 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:52,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:52,500 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:52,627 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:52,754 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:52,881 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,007 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,134 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,515 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,768 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,886 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:53,996 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,097 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,192 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,461 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,639 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,729 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,818 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:54,997 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,086 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,176 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,265 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,355 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,445 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,534 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,624 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,713 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,803 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,892 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:55,981 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,071 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,161 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,249 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,428 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,518 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,696 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,786 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,876 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:56,965 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,143 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,232 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,322 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,412 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,501 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,591 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,680 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,770 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:57,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,038 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,128 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,217 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,307 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,394 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,481 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,570 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,659 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,748 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,837 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:58,927 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,016 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,106 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,195 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,284 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,373 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 89: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,462 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 90: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,551 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 91: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,641 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 92: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,730 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 93: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 94: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,908 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 95: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:12:59,998 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 96: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:13:00,088 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 97: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:13:00,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 98: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:13:00,266 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 99: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-04 16:13:00,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 0: ref_distribution = tensor([0.1000, 0.3000, 0.6000], device='cuda:0'), new_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0')
2024-12-04 16:13:00,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 1: ref_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0'), new_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0')
2024-12-04 16:13:00,764 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 2: ref_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0'), new_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0')
2024-12-04 16:13:00,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 3: ref_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0'), new_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0')
2024-12-04 16:13:00,945 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 4: ref_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0'), new_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0')
2024-12-04 16:13:01,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 5: ref_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0'), new_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0')
2024-12-04 16:13:01,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 6: ref_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0'), new_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0')
2024-12-04 16:13:01,215 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 7: ref_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0'), new_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0')
2024-12-04 16:13:01,307 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 8: ref_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0'), new_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0')
2024-12-04 16:13:01,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 9: ref_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0'), new_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0')
2024-12-04 16:13:01,486 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 10: ref_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0'), new_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0')
2024-12-04 16:13:01,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 11: ref_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0'), new_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0')
2024-12-04 16:13:01,670 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 12: ref_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0'), new_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0')
2024-12-04 16:13:01,759 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 13: ref_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0'), new_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0')
2024-12-04 16:13:01,851 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 14: ref_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0'), new_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0')
2024-12-04 16:13:01,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 15: ref_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0'), new_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0')
2024-12-04 16:13:02,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 16: ref_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0'), new_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0')
2024-12-04 16:13:02,120 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 17: ref_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0'), new_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0')
2024-12-04 16:13:02,210 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 18: ref_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0'), new_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0')
2024-12-04 16:13:02,299 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 19: ref_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0'), new_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0')
2024-12-04 16:13:02,389 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 20: ref_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0'), new_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0')
2024-12-04 16:13:02,478 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 21: ref_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0'), new_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0')
2024-12-04 16:13:02,568 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 22: ref_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0'), new_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0')
2024-12-04 16:13:02,657 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 23: ref_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0'), new_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0')
2024-12-04 16:13:02,747 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 24: ref_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0'), new_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0')
2024-12-04 16:13:02,837 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 25: ref_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0'), new_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0')
2024-12-04 16:13:02,927 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 26: ref_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0'), new_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0')
2024-12-04 16:13:03,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 27: ref_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0'), new_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0')
2024-12-04 16:13:03,106 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 28: ref_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0'), new_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0')
2024-12-04 16:13:03,196 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 29: ref_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0'), new_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0')
2024-12-04 16:13:03,289 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 30: ref_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0'), new_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0')
2024-12-04 16:13:03,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 31: ref_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0'), new_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0')
2024-12-04 16:13:03,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 32: ref_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0'), new_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0')
2024-12-04 16:13:03,558 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 33: ref_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0'), new_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0')
2024-12-04 16:13:03,647 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 34: ref_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0'), new_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0')
2024-12-04 16:13:03,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 35: ref_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0'), new_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0')
2024-12-04 16:13:03,826 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 36: ref_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0'), new_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0')
2024-12-04 16:13:03,917 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 37: ref_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0'), new_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0')
2024-12-04 16:13:04,004 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 38: ref_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0'), new_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0')
2024-12-04 16:13:04,093 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 39: ref_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0'), new_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0')
2024-12-04 16:13:04,183 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 40: ref_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0'), new_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0')
2024-12-04 16:13:04,273 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 41: ref_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0'), new_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0')
2024-12-04 16:13:04,363 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 42: ref_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0'), new_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0')
2024-12-04 16:13:04,452 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 43: ref_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0'), new_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0')
2024-12-04 16:13:04,542 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 44: ref_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0'), new_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0')
2024-12-04 16:13:04,632 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 45: ref_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0'), new_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0')
2024-12-04 16:13:04,722 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 46: ref_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0'), new_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0')
2024-12-04 16:13:04,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 47: ref_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0'), new_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0')
2024-12-04 16:13:04,901 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 48: ref_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0'), new_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0')
2024-12-04 16:13:04,991 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 49: ref_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0'), new_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0')
2024-12-04 16:13:05,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 50: ref_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0'), new_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0')
2024-12-04 16:13:05,170 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 51: ref_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0'), new_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0')
2024-12-04 16:13:05,259 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 52: ref_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0'), new_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0')
2024-12-04 16:13:05,349 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 53: ref_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0'), new_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0')
2024-12-04 16:13:05,438 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 54: ref_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0'), new_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0')
2024-12-04 16:13:05,528 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 55: ref_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0'), new_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0')
2024-12-04 16:13:05,617 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 56: ref_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0'), new_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0')
2024-12-04 16:13:05,706 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 57: ref_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0'), new_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0')
2024-12-04 16:13:05,796 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 58: ref_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0'), new_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0')
2024-12-04 16:13:05,885 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 59: ref_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0'), new_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0')
2024-12-04 16:13:05,975 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 60: ref_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0'), new_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0')
2024-12-04 16:13:06,065 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 61: ref_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0'), new_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0')
2024-12-04 16:13:06,154 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 62: ref_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0'), new_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0')
2024-12-04 16:13:06,243 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 63: ref_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0'), new_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0')
2024-12-04 16:13:06,332 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 64: ref_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0'), new_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0')
2024-12-04 16:13:06,422 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 65: ref_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0'), new_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0')
2024-12-04 16:13:06,512 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 66: ref_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0'), new_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0')
2024-12-04 16:13:06,601 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 67: ref_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0'), new_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0')
2024-12-04 16:13:06,690 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 68: ref_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0'), new_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0')
2024-12-04 16:13:06,782 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 69: ref_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0'), new_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0')
2024-12-04 16:13:06,869 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 70: ref_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0'), new_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0')
2024-12-04 16:13:06,959 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 71: ref_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0'), new_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0')
2024-12-04 16:13:07,048 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 72: ref_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0'), new_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0')
2024-12-04 16:13:07,138 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 73: ref_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0'), new_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0')
2024-12-04 16:13:07,227 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 74: ref_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0'), new_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0')
2024-12-04 16:13:07,317 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 75: ref_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0'), new_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0')
2024-12-04 16:13:07,407 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 76: ref_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0'), new_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0')
2024-12-04 16:13:07,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 77: ref_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0'), new_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0')
2024-12-04 16:13:07,586 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 78: ref_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0'), new_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0')
2024-12-04 16:13:07,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 79: ref_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0'), new_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0')
2024-12-04 16:13:07,765 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 80: ref_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0'), new_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0')
2024-12-04 16:13:07,855 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 81: ref_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0'), new_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0')
2024-12-04 16:13:07,945 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 82: ref_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0'), new_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0')
2024-12-04 16:13:08,034 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 83: ref_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0'), new_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0')
2024-12-04 16:13:08,123 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 84: ref_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0'), new_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0')
2024-12-04 16:13:08,213 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 85: ref_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0'), new_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0')
2024-12-04 16:13:08,304 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 86: ref_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0'), new_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0')
2024-12-04 16:13:08,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 87: ref_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0'), new_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0')
2024-12-04 16:13:08,482 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 88: ref_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0'), new_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0')
2024-12-04 16:13:08,571 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 89: ref_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0'), new_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0')
2024-12-04 16:13:08,660 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 90: ref_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0'), new_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0')
2024-12-04 16:13:08,749 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 91: ref_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0'), new_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0')
2024-12-04 16:13:08,839 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 92: ref_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0'), new_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0')
2024-12-04 16:13:08,929 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 93: ref_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0'), new_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0')
2024-12-04 16:13:09,019 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 94: ref_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0'), new_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0')
2024-12-04 16:13:09,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 95: ref_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0'), new_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0')
2024-12-04 16:13:09,197 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 96: ref_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0'), new_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0')
2024-12-04 16:13:09,287 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 97: ref_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0'), new_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0')
2024-12-04 16:13:09,375 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 98: ref_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0'), new_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0')
2024-12-04 16:13:09,464 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:551] - INFO: Iteration 99: ref_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0'), new_distribution = tensor([0.1051, 0.3544, 0.5405], device='cuda:0')
