2024-12-03 15:59:03,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 0 loss: 0.6777 acc: 0.77
2024-12-03 15:59:03,822 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 2 loss: 0.6746 acc: 0.77
2024-12-03 15:59:03,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 4 loss: 0.6717 acc: 0.77
2024-12-03 15:59:03,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 6 loss: 0.6688 acc: 0.77
2024-12-03 15:59:03,838 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 8 loss: 0.6659 acc: 0.77
2024-12-03 15:59:03,843 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 10 loss: 0.6632 acc: 0.77
2024-12-03 15:59:03,849 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 12 loss: 0.6605 acc: 0.77
2024-12-03 15:59:03,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 14 loss: 0.6579 acc: 0.77
2024-12-03 15:59:03,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 16 loss: 0.6554 acc: 0.77
2024-12-03 15:59:03,864 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:122] - INFO: [Reward] Epoch 18 loss: 0.6529 acc: 0.77
2024-12-03 15:59:04,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 0 loss: -0.3024 reward: 0.3024 ref_reward: 0.2734 improvement: 10.62%
2024-12-03 15:59:04,646 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 2 loss: -0.3092 reward: 0.3092 ref_reward: 0.2734 improvement: 13.10%
2024-12-03 15:59:05,184 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 4 loss: -0.3149 reward: 0.3149 ref_reward: 0.2734 improvement: 15.19%
2024-12-03 15:59:05,730 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 6 loss: -0.3202 reward: 0.3202 ref_reward: 0.2734 improvement: 17.10%
2024-12-03 15:59:06,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 8 loss: -0.3253 reward: 0.3253 ref_reward: 0.2734 improvement: 18.99%
2024-12-03 15:59:06,765 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 10 loss: -0.3302 reward: 0.3302 ref_reward: 0.2734 improvement: 20.79%
2024-12-03 15:59:07,123 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 12 loss: -0.3350 reward: 0.3350 ref_reward: 0.2734 improvement: 22.54%
2024-12-03 15:59:07,428 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 14 loss: -0.3396 reward: 0.3396 ref_reward: 0.2734 improvement: 24.22%
2024-12-03 15:59:07,717 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 16 loss: -0.3440 reward: 0.3440 ref_reward: 0.2734 improvement: 25.81%
2024-12-03 15:59:08,008 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 18 loss: -0.3481 reward: 0.3481 ref_reward: 0.2734 improvement: 27.33%
2024-12-03 15:59:08,294 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 20 loss: -0.3520 reward: 0.3520 ref_reward: 0.2734 improvement: 28.76%
2024-12-03 15:59:08,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 22 loss: -0.3557 reward: 0.3557 ref_reward: 0.2734 improvement: 30.11%
2024-12-03 15:59:08,869 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 24 loss: -0.3592 reward: 0.3592 ref_reward: 0.2734 improvement: 31.38%
2024-12-03 15:59:09,155 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 26 loss: -0.3624 reward: 0.3624 ref_reward: 0.2734 improvement: 32.56%
2024-12-03 15:59:09,443 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 28 loss: -0.3654 reward: 0.3654 ref_reward: 0.2734 improvement: 33.65%
2024-12-03 15:59:09,731 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 30 loss: -0.3681 reward: 0.3681 ref_reward: 0.2734 improvement: 34.65%
2024-12-03 15:59:10,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 32 loss: -0.3706 reward: 0.3706 ref_reward: 0.2734 improvement: 35.55%
2024-12-03 15:59:10,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 34 loss: -0.3728 reward: 0.3728 ref_reward: 0.2734 improvement: 36.35%
2024-12-03 15:59:10,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 36 loss: -0.3747 reward: 0.3747 ref_reward: 0.2734 improvement: 37.05%
2024-12-03 15:59:10,883 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 38 loss: -0.3763 reward: 0.3763 ref_reward: 0.2734 improvement: 37.65%
2024-12-03 15:59:11,169 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 40 loss: -0.3777 reward: 0.3777 ref_reward: 0.2734 improvement: 38.17%
2024-12-03 15:59:11,532 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 42 loss: -0.3789 reward: 0.3789 ref_reward: 0.2734 improvement: 38.60%
2024-12-03 15:59:12,069 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 44 loss: -0.3799 reward: 0.3799 ref_reward: 0.2734 improvement: 38.97%
2024-12-03 15:59:12,611 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 46 loss: -0.3808 reward: 0.3808 ref_reward: 0.2734 improvement: 39.28%
2024-12-03 15:59:13,148 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 48 loss: -0.3815 reward: 0.3815 ref_reward: 0.2734 improvement: 39.53%
2024-12-03 15:59:13,693 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 50 loss: -0.3821 reward: 0.3821 ref_reward: 0.2734 improvement: 39.75%
2024-12-03 15:59:14,212 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 52 loss: -0.3826 reward: 0.3826 ref_reward: 0.2734 improvement: 39.92%
2024-12-03 15:59:14,625 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 54 loss: -0.3830 reward: 0.3830 ref_reward: 0.2734 improvement: 40.07%
2024-12-03 15:59:14,942 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 56 loss: -0.3833 reward: 0.3833 ref_reward: 0.2734 improvement: 40.20%
2024-12-03 15:59:15,234 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 58 loss: -0.3836 reward: 0.3836 ref_reward: 0.2734 improvement: 40.30%
2024-12-03 15:59:15,524 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 60 loss: -0.3838 reward: 0.3838 ref_reward: 0.2734 improvement: 40.39%
2024-12-03 15:59:15,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 62 loss: -0.3840 reward: 0.3840 ref_reward: 0.2734 improvement: 40.47%
2024-12-03 15:59:16,104 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 64 loss: -0.3842 reward: 0.3842 ref_reward: 0.2734 improvement: 40.53%
2024-12-03 15:59:16,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 66 loss: -0.3844 reward: 0.3844 ref_reward: 0.2734 improvement: 40.59%
2024-12-03 15:59:16,683 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 68 loss: -0.3845 reward: 0.3845 ref_reward: 0.2734 improvement: 40.64%
2024-12-03 15:59:16,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 70 loss: -0.3846 reward: 0.3846 ref_reward: 0.2734 improvement: 40.68%
2024-12-03 15:59:17,273 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 72 loss: -0.3847 reward: 0.3847 ref_reward: 0.2734 improvement: 40.71%
2024-12-03 15:59:17,599 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 74 loss: -0.3848 reward: 0.3848 ref_reward: 0.2734 improvement: 40.75%
2024-12-03 15:59:17,892 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 76 loss: -0.3849 reward: 0.3849 ref_reward: 0.2734 improvement: 40.77%
2024-12-03 15:59:18,185 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 78 loss: -0.3849 reward: 0.3849 ref_reward: 0.2734 improvement: 40.80%
2024-12-03 15:59:18,473 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 80 loss: -0.3850 reward: 0.3850 ref_reward: 0.2734 improvement: 40.82%
2024-12-03 15:59:18,760 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 82 loss: -0.3851 reward: 0.3851 ref_reward: 0.2734 improvement: 40.84%
2024-12-03 15:59:19,049 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 84 loss: -0.3851 reward: 0.3851 ref_reward: 0.2734 improvement: 40.86%
2024-12-03 15:59:19,336 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 86 loss: -0.3852 reward: 0.3852 ref_reward: 0.2734 improvement: 40.88%
2024-12-03 15:59:19,622 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 88 loss: -0.3852 reward: 0.3852 ref_reward: 0.2734 improvement: 40.89%
2024-12-03 15:59:19,909 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 90 loss: -0.3852 reward: 0.3852 ref_reward: 0.2734 improvement: 40.91%
2024-12-03 15:59:20,196 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 92 loss: -0.3853 reward: 0.3853 ref_reward: 0.2734 improvement: 40.92%
2024-12-03 15:59:20,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 94 loss: -0.3853 reward: 0.3853 ref_reward: 0.2734 improvement: 40.93%
2024-12-03 15:59:20,778 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 96 loss: -0.3853 reward: 0.3853 ref_reward: 0.2734 improvement: 40.94%
2024-12-03 15:59:21,073 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:203] - INFO: [Policy] Epoch 98 loss: -0.3854 reward: 0.3854 ref_reward: 0.2734 improvement: 40.95%
2024-12-03 15:59:21,927 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 0.6601 grad norm: 0.3895 policy: 0.4184 0.3163
2024-12-03 15:59:22,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 0.6255 grad norm: 0.2693 policy: 0.4481 0.3427
2024-12-03 15:59:23,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.6063 grad norm: 0.1535 policy: 0.4700 0.3648
2024-12-03 15:59:24,104 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.5992 grad norm: 0.0415 policy: 0.4829 0.3850
2024-12-03 15:59:24,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.5998 grad norm: 0.0615 policy: 0.4824 0.4037
2024-12-03 15:59:25,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.6011 grad norm: 0.0979 policy: 0.4760 0.4132
2024-12-03 15:59:26,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.6002 grad norm: 0.0761 policy: 0.4728 0.4099
2024-12-03 15:59:27,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.5990 grad norm: 0.0291 policy: 0.4734 0.3996
2024-12-03 15:59:27,982 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.5988 grad norm: 0.0108 policy: 0.4749 0.3906
2024-12-03 15:59:28,726 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.5990 grad norm: 0.0291 policy: 0.4756 0.3869
2024-12-03 15:59:29,515 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.5990 grad norm: 0.0274 policy: 0.4753 0.3883
2024-12-03 15:59:30,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.5989 grad norm: 0.0137 policy: 0.4747 0.3921
2024-12-03 15:59:31,041 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0019 policy: 0.4746 0.3955
2024-12-03 15:59:31,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0103 policy: 0.4750 0.3966
2024-12-03 15:59:32,625 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0099 policy: 0.4753 0.3960
2024-12-03 15:59:33,375 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0042 policy: 0.4752 0.3947
2024-12-03 15:59:34,208 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0017 policy: 0.4749 0.3938
2024-12-03 15:59:35,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0042 policy: 0.4748 0.3934
2024-12-03 15:59:35,731 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0032 policy: 0.4750 0.3936
2024-12-03 15:59:36,495 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0007 policy: 0.4751 0.3941
2024-12-03 15:59:37,661 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 1.1505 grad norm: 1.0963 policy: 0.3430 0.2943
2024-12-03 15:59:38,388 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 1.0561 grad norm: 0.9113 policy: 0.3891 0.3027
2024-12-03 15:59:39,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.9881 grad norm: 0.8351 policy: 0.4280 0.3026
2024-12-03 15:59:39,843 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.9338 grad norm: 0.8253 policy: 0.4625 0.2996
2024-12-03 15:59:40,572 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.8868 grad norm: 0.8394 policy: 0.4910 0.2990
2024-12-03 15:59:41,300 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.8409 grad norm: 0.8642 policy: 0.5191 0.2986
2024-12-03 15:59:42,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.7941 grad norm: 0.8694 policy: 0.5480 0.2976
2024-12-03 15:59:42,760 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.7481 grad norm: 0.8427 policy: 0.5777 0.2946
2024-12-03 15:59:43,487 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.7059 grad norm: 0.7842 policy: 0.6082 0.2883
2024-12-03 15:59:44,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.6692 grad norm: 0.6890 policy: 0.6399 0.2777
2024-12-03 15:59:44,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.6400 grad norm: 0.5636 policy: 0.6732 0.2616
2024-12-03 15:59:45,678 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.6192 grad norm: 0.4185 policy: 0.7074 0.2407
2024-12-03 15:59:46,404 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.6065 grad norm: 0.2671 policy: 0.7398 0.2180
2024-12-03 15:59:47,136 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.6005 grad norm: 0.1273 policy: 0.7668 0.1977
2024-12-03 15:59:47,870 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5989 grad norm: 0.0248 policy: 0.7857 0.1832
2024-12-03 15:59:48,598 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5991 grad norm: 0.0596 policy: 0.7958 0.1757
2024-12-03 15:59:49,327 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5996 grad norm: 0.0897 policy: 0.7986 0.1741
2024-12-03 15:59:50,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5997 grad norm: 0.0929 policy: 0.7968 0.1761
2024-12-03 15:59:50,785 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5995 grad norm: 0.0806 policy: 0.7931 0.1793
2024-12-03 15:59:51,512 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5992 grad norm: 0.0607 policy: 0.7895 0.1821
2024-12-03 15:59:52,458 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 9.0930 grad norm: 1.1583 policy: 0.3516 0.3312
2024-12-03 15:59:53,184 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 8.9809 grad norm: 1.1178 policy: 0.4166 0.3027
2024-12-03 15:59:53,918 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 8.8799 grad norm: 1.2113 policy: 0.4812 0.2694
2024-12-03 15:59:54,731 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 8.7728 grad norm: 1.3928 policy: 0.5536 0.2310
2024-12-03 15:59:56,110 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 8.6468 grad norm: 1.5956 policy: 0.6355 0.1878
2024-12-03 15:59:57,116 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 8.5003 grad norm: 1.8278 policy: 0.7221 0.1420
2024-12-03 15:59:57,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 8.3270 grad norm: 2.0802 policy: 0.8063 0.0982
2024-12-03 15:59:58,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 8.1235 grad norm: 2.3526 policy: 0.8784 0.0611
2024-12-03 15:59:59,338 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 7.8867 grad norm: 2.6449 policy: 0.9320 0.0339
2024-12-03 16:00:00,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 7.6131 grad norm: 2.9572 policy: 0.9663 0.0167
2024-12-03 16:00:00,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 7.2995 grad norm: 3.2894 policy: 0.9852 0.0073
2024-12-03 16:00:01,577 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 6.9426 grad norm: 3.6418 policy: 0.9943 0.0028
2024-12-03 16:00:02,320 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 6.5389 grad norm: 4.0143 policy: 0.9981 0.0009
2024-12-03 16:00:03,066 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 6.0851 grad norm: 4.4071 policy: 0.9994 0.0003
2024-12-03 16:00:03,813 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 5.5778 grad norm: 4.8200 policy: 0.9999 0.0001
2024-12-03 16:00:04,560 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 5.0139 grad norm: 5.2530 policy: 1.0000 0.0000
2024-12-03 16:00:05,304 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 4.3898 grad norm: 5.7058 policy: 1.0000 0.0000
2024-12-03 16:00:06,048 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 3.7026 grad norm: 6.1774 policy: 1.0000 0.0000
2024-12-03 16:00:07,436 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 2.9495 grad norm: 6.6573 policy: 1.0000 0.0000
2024-12-03 16:00:08,527 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 2.1336 grad norm: 7.0461 policy: 1.0000 0.0000
2024-12-03 16:00:09,517 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 0 loss: 0.6443 grad norm: 0.3066 policy: 0.3209 0.3444
2024-12-03 16:00:10,269 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 5 loss: 0.6217 grad norm: 0.2287 policy: 0.2835 0.3930
2024-12-03 16:00:11,021 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 10 loss: 0.6071 grad norm: 0.1489 policy: 0.2465 0.4419
2024-12-03 16:00:11,770 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 15 loss: 0.6001 grad norm: 0.0589 policy: 0.2106 0.4816
2024-12-03 16:00:12,515 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 20 loss: 0.5992 grad norm: 0.0381 policy: 0.1832 0.5032
2024-12-03 16:00:13,262 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 25 loss: 0.6004 grad norm: 0.0806 policy: 0.1714 0.5050
2024-12-03 16:00:14,007 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 30 loss: 0.6002 grad norm: 0.0719 policy: 0.1745 0.4969
2024-12-03 16:00:14,756 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 35 loss: 0.5992 grad norm: 0.0347 policy: 0.1855 0.4881
2024-12-03 16:00:15,504 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 40 loss: 0.5988 grad norm: 0.0024 policy: 0.1966 0.4821
2024-12-03 16:00:16,246 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 45 loss: 0.5990 grad norm: 0.0211 policy: 0.2027 0.4794
2024-12-03 16:00:16,989 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 50 loss: 0.5990 grad norm: 0.0239 policy: 0.2026 0.4792
2024-12-03 16:00:17,736 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 55 loss: 0.5989 grad norm: 0.0152 policy: 0.1987 0.4809
2024-12-03 16:00:18,476 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 60 loss: 0.5988 grad norm: 0.0029 policy: 0.1943 0.4835
2024-12-03 16:00:19,223 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 65 loss: 0.5988 grad norm: 0.0071 policy: 0.1918 0.4859
2024-12-03 16:00:19,974 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 70 loss: 0.5988 grad norm: 0.0092 policy: 0.1918 0.4867
2024-12-03 16:00:20,731 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 75 loss: 0.5988 grad norm: 0.0054 policy: 0.1933 0.4857
2024-12-03 16:00:21,479 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 80 loss: 0.5988 grad norm: 0.0004 policy: 0.1949 0.4841
2024-12-03 16:00:22,227 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 85 loss: 0.5988 grad norm: 0.0034 policy: 0.1956 0.4831
2024-12-03 16:00:22,972 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 90 loss: 0.5988 grad norm: 0.0033 policy: 0.1954 0.4833
2024-12-03 16:00:23,719 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:331] - INFO: [Policy] Epoch: 95 loss: 0.5988 grad norm: 0.0012 policy: 0.1947 0.4841
2024-12-03 16:00:25,356 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 0.0287 grad norm: 0.6606 
2024-12-03 16:00:26,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0064 grad norm: 0.2534 
2024-12-03 16:00:26,864 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0015 grad norm: 0.1044 
2024-12-03 16:00:27,620 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0038 grad norm: 0.1703 
2024-12-03 16:00:28,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0001 grad norm: 0.0285 
2024-12-03 16:00:29,133 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0013 grad norm: 0.0819 
2024-12-03 16:00:29,889 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0000 grad norm: 0.0162 
2024-12-03 16:00:30,642 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0004 grad norm: 0.0482 
2024-12-03 16:00:31,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0000 grad norm: 0.0113 
2024-12-03 16:00:32,153 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0002 grad norm: 0.0290 
2024-12-03 16:00:32,908 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0000 grad norm: 0.0070 
2024-12-03 16:00:33,665 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0001 grad norm: 0.0174 
2024-12-03 16:00:34,421 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0000 grad norm: 0.0067 
2024-12-03 16:00:35,177 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0074 
2024-12-03 16:00:35,935 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0010 
2024-12-03 16:00:36,692 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0041 
2024-12-03 16:00:37,447 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0023 
2024-12-03 16:00:38,201 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0013 
2024-12-03 16:00:38,954 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0021 
2024-12-03 16:00:39,713 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0005 
2024-12-03 16:00:40,702 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 1.2630 grad norm: 3.3703 
2024-12-03 16:00:41,451 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0376 grad norm: 0.8054 
2024-12-03 16:00:42,207 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0185 grad norm: 0.4648 
2024-12-03 16:00:42,963 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0115 grad norm: 0.3450 
2024-12-03 16:00:43,719 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0019 grad norm: 0.1737 
2024-12-03 16:00:44,475 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0037 grad norm: 0.2135 
2024-12-03 16:00:45,230 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0002 grad norm: 0.0509 
2024-12-03 16:00:45,987 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0011 grad norm: 0.1075 
2024-12-03 16:00:46,745 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0005 grad norm: 0.0784 
2024-12-03 16:00:47,502 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0004 grad norm: 0.0722 
2024-12-03 16:00:48,258 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0002 grad norm: 0.0518 
2024-12-03 16:00:49,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0001 grad norm: 0.0394 
2024-12-03 16:00:49,767 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0001 grad norm: 0.0338 
2024-12-03 16:00:50,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0188 
2024-12-03 16:00:51,279 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0053 
2024-12-03 16:00:52,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0104 
2024-12-03 16:00:52,792 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0012 
2024-12-03 16:00:53,547 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0066 
2024-12-03 16:00:54,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0027 
2024-12-03 16:00:55,063 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0035 
2024-12-03 16:00:56,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 670.7255 grad norm: 64.6175 
2024-12-03 16:00:56,795 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.7152 grad norm: 6.8348 
2024-12-03 16:00:57,552 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.2917 grad norm: 3.7163 
2024-12-03 16:00:58,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0085 grad norm: 0.4709 
2024-12-03 16:00:59,057 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0262 grad norm: 0.7309 
2024-12-03 16:00:59,811 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0280 grad norm: 0.7376 
2024-12-03 16:01:00,566 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0030 grad norm: 0.2599 
2024-12-03 16:01:01,326 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0064 grad norm: 0.4337 
2024-12-03 16:01:02,080 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0012 grad norm: 0.1702 
2024-12-03 16:01:02,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0015 grad norm: 0.1902 
2024-12-03 16:01:03,595 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0006 grad norm: 0.1221 
2024-12-03 16:01:04,353 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0006 grad norm: 0.1190 
2024-12-03 16:01:05,112 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0001 grad norm: 0.0453 
2024-12-03 16:01:05,867 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0003 grad norm: 0.0815 
2024-12-03 16:01:06,626 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0296 
2024-12-03 16:01:07,380 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0001 grad norm: 0.0405 
2024-12-03 16:01:08,135 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0233 
2024-12-03 16:01:08,893 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0288 
2024-12-03 16:01:09,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0003 
2024-12-03 16:01:10,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0174 
2024-12-03 16:01:11,372 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 0 loss: 1.3772 grad norm: 4.6064 
2024-12-03 16:01:12,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 5 loss: 0.0452 grad norm: 0.9081 
2024-12-03 16:01:12,881 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 10 loss: 0.0184 grad norm: 0.3898 
2024-12-03 16:01:13,634 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 15 loss: 0.0206 grad norm: 0.3879 
2024-12-03 16:01:14,391 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 20 loss: 0.0004 grad norm: 0.0612 
2024-12-03 16:01:15,146 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 25 loss: 0.0055 grad norm: 0.2523 
2024-12-03 16:01:15,908 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 30 loss: 0.0013 grad norm: 0.1118 
2024-12-03 16:01:16,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 35 loss: 0.0016 grad norm: 0.1198 
2024-12-03 16:01:17,416 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 40 loss: 0.0008 grad norm: 0.0837 
2024-12-03 16:01:18,172 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 45 loss: 0.0002 grad norm: 0.0488 
2024-12-03 16:01:18,919 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 50 loss: 0.0004 grad norm: 0.0630 
2024-12-03 16:01:19,676 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 55 loss: 0.0000 grad norm: 0.0201 
2024-12-03 16:01:20,430 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 60 loss: 0.0002 grad norm: 0.0394 
2024-12-03 16:01:21,183 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 65 loss: 0.0000 grad norm: 0.0172 
2024-12-03 16:01:21,934 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 70 loss: 0.0000 grad norm: 0.0054 
2024-12-03 16:01:22,684 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 75 loss: 0.0000 grad norm: 0.0103 
2024-12-03 16:01:23,436 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 80 loss: 0.0000 grad norm: 0.0032 
2024-12-03 16:01:24,191 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 85 loss: 0.0000 grad norm: 0.0062 
2024-12-03 16:01:24,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 90 loss: 0.0000 grad norm: 0.0015 
2024-12-03 16:01:25,700 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:455] - INFO: [Policy] Epoch: 95 loss: 0.0000 grad norm: 0.0037 
2024-12-03 16:01:27,160 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.3333, 0.3333, 0.3333], device='cuda:0'), new_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0')
2024-12-03 16:01:27,255 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.3336, 0.3334, 0.3330], device='cuda:0'), new_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0')
2024-12-03 16:01:27,347 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.3338, 0.3335, 0.3327], device='cuda:0'), new_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0')
2024-12-03 16:01:27,439 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.3340, 0.3336, 0.3324], device='cuda:0'), new_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0')
2024-12-03 16:01:27,531 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.3342, 0.3337, 0.3321], device='cuda:0'), new_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0')
2024-12-03 16:01:27,622 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.3344, 0.3338, 0.3318], device='cuda:0'), new_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0')
2024-12-03 16:01:27,714 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.3346, 0.3339, 0.3315], device='cuda:0'), new_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0')
2024-12-03 16:01:27,805 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.3349, 0.3340, 0.3312], device='cuda:0'), new_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0')
2024-12-03 16:01:27,896 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.3351, 0.3341, 0.3309], device='cuda:0'), new_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0')
2024-12-03 16:01:27,989 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.3353, 0.3341, 0.3306], device='cuda:0'), new_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0')
2024-12-03 16:01:28,078 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.3355, 0.3342, 0.3302], device='cuda:0'), new_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0')
2024-12-03 16:01:28,169 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.3357, 0.3343, 0.3299], device='cuda:0'), new_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0')
2024-12-03 16:01:28,261 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.3360, 0.3344, 0.3296], device='cuda:0'), new_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0')
2024-12-03 16:01:28,352 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.3362, 0.3345, 0.3293], device='cuda:0'), new_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0')
2024-12-03 16:01:28,444 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.3364, 0.3346, 0.3290], device='cuda:0'), new_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0')
2024-12-03 16:01:28,535 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.3366, 0.3347, 0.3287], device='cuda:0'), new_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0')
2024-12-03 16:01:28,626 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.3369, 0.3347, 0.3284], device='cuda:0'), new_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0')
2024-12-03 16:01:28,718 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.3371, 0.3348, 0.3281], device='cuda:0'), new_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0')
2024-12-03 16:01:28,810 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.3373, 0.3349, 0.3278], device='cuda:0'), new_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0')
2024-12-03 16:01:28,899 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.3375, 0.3350, 0.3275], device='cuda:0'), new_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0')
2024-12-03 16:01:28,990 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.3377, 0.3351, 0.3272], device='cuda:0'), new_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0')
2024-12-03 16:01:29,082 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.3380, 0.3352, 0.3269], device='cuda:0'), new_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0')
2024-12-03 16:01:29,173 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.3382, 0.3352, 0.3266], device='cuda:0'), new_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0')
2024-12-03 16:01:29,264 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.3384, 0.3353, 0.3263], device='cuda:0'), new_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0')
2024-12-03 16:01:29,356 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.3386, 0.3354, 0.3260], device='cuda:0'), new_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0')
2024-12-03 16:01:29,447 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.3389, 0.3355, 0.3257], device='cuda:0'), new_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0')
2024-12-03 16:01:29,538 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.3391, 0.3356, 0.3253], device='cuda:0'), new_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0')
2024-12-03 16:01:29,630 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.3393, 0.3356, 0.3250], device='cuda:0'), new_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0')
2024-12-03 16:01:29,721 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.3395, 0.3357, 0.3247], device='cuda:0'), new_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0')
2024-12-03 16:01:29,812 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.3398, 0.3358, 0.3244], device='cuda:0'), new_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0')
2024-12-03 16:01:29,904 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.3400, 0.3359, 0.3241], device='cuda:0'), new_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0')
2024-12-03 16:01:29,995 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.3402, 0.3360, 0.3238], device='cuda:0'), new_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0')
2024-12-03 16:01:30,087 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.3404, 0.3360, 0.3235], device='cuda:0'), new_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0')
2024-12-03 16:01:30,178 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.3407, 0.3361, 0.3232], device='cuda:0'), new_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0')
2024-12-03 16:01:30,270 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.3409, 0.3362, 0.3229], device='cuda:0'), new_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0')
2024-12-03 16:01:30,361 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.3411, 0.3363, 0.3226], device='cuda:0'), new_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0')
2024-12-03 16:01:30,453 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.3414, 0.3363, 0.3223], device='cuda:0'), new_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0')
2024-12-03 16:01:30,545 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.3416, 0.3364, 0.3220], device='cuda:0'), new_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0')
2024-12-03 16:01:30,636 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.3418, 0.3365, 0.3217], device='cuda:0'), new_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0')
2024-12-03 16:01:30,728 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.3420, 0.3365, 0.3214], device='cuda:0'), new_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0')
2024-12-03 16:01:30,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.3423, 0.3366, 0.3211], device='cuda:0'), new_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0')
2024-12-03 16:01:30,911 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.3425, 0.3367, 0.3208], device='cuda:0'), new_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0')
2024-12-03 16:01:31,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.3427, 0.3368, 0.3205], device='cuda:0'), new_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0')
2024-12-03 16:01:31,094 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.3430, 0.3368, 0.3202], device='cuda:0'), new_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0')
2024-12-03 16:01:31,185 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.3432, 0.3369, 0.3199], device='cuda:0'), new_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0')
2024-12-03 16:01:31,277 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.3434, 0.3370, 0.3196], device='cuda:0'), new_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0')
2024-12-03 16:01:31,368 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.3436, 0.3370, 0.3193], device='cuda:0'), new_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0')
2024-12-03 16:01:31,460 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.3439, 0.3371, 0.3190], device='cuda:0'), new_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0')
2024-12-03 16:01:31,551 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.3441, 0.3372, 0.3187], device='cuda:0'), new_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0')
2024-12-03 16:01:31,643 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.3443, 0.3372, 0.3184], device='cuda:0'), new_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0')
2024-12-03 16:01:31,734 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.3446, 0.3373, 0.3181], device='cuda:0'), new_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0')
2024-12-03 16:01:31,826 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.3448, 0.3374, 0.3178], device='cuda:0'), new_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0')
2024-12-03 16:01:31,917 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.3450, 0.3374, 0.3175], device='cuda:0'), new_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0')
2024-12-03 16:01:32,009 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.3453, 0.3375, 0.3172], device='cuda:0'), new_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0')
2024-12-03 16:01:32,100 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.3455, 0.3376, 0.3169], device='cuda:0'), new_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0')
2024-12-03 16:01:32,193 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.3457, 0.3376, 0.3166], device='cuda:0'), new_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0')
2024-12-03 16:01:32,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.3460, 0.3377, 0.3163], device='cuda:0'), new_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0')
2024-12-03 16:01:32,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.3462, 0.3378, 0.3160], device='cuda:0'), new_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0')
2024-12-03 16:01:32,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.3464, 0.3378, 0.3157], device='cuda:0'), new_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0')
2024-12-03 16:01:32,561 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.3467, 0.3379, 0.3154], device='cuda:0'), new_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0')
2024-12-03 16:01:32,650 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.3469, 0.3380, 0.3151], device='cuda:0'), new_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0')
2024-12-03 16:01:32,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.3471, 0.3380, 0.3149], device='cuda:0'), new_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0')
2024-12-03 16:01:32,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.3474, 0.3381, 0.3146], device='cuda:0'), new_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0')
2024-12-03 16:01:32,925 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.3476, 0.3381, 0.3143], device='cuda:0'), new_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0')
2024-12-03 16:01:33,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.3478, 0.3382, 0.3140], device='cuda:0'), new_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0')
2024-12-03 16:01:33,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.3481, 0.3383, 0.3137], device='cuda:0'), new_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0')
2024-12-03 16:01:33,202 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.3483, 0.3383, 0.3134], device='cuda:0'), new_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0')
2024-12-03 16:01:33,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.3485, 0.3384, 0.3131], device='cuda:0'), new_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0')
2024-12-03 16:01:33,429 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.3488, 0.3384, 0.3128], device='cuda:0'), new_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0')
2024-12-03 16:01:33,556 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.3490, 0.3385, 0.3125], device='cuda:0'), new_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0')
2024-12-03 16:01:33,684 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.3493, 0.3385, 0.3122], device='cuda:0'), new_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0')
2024-12-03 16:01:33,861 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.3495, 0.3386, 0.3119], device='cuda:0'), new_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0')
2024-12-03 16:01:34,008 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.3497, 0.3386, 0.3116], device='cuda:0'), new_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0')
2024-12-03 16:01:34,137 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.3500, 0.3387, 0.3113], device='cuda:0'), new_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0')
2024-12-03 16:01:34,266 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.3502, 0.3388, 0.3110], device='cuda:0'), new_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0')
2024-12-03 16:01:34,395 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.3504, 0.3388, 0.3107], device='cuda:0'), new_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0')
2024-12-03 16:01:34,524 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.3507, 0.3389, 0.3104], device='cuda:0'), new_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0')
2024-12-03 16:01:34,660 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.3509, 0.3389, 0.3102], device='cuda:0'), new_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0')
2024-12-03 16:01:34,789 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.3512, 0.3390, 0.3099], device='cuda:0'), new_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0')
2024-12-03 16:01:34,919 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.3514, 0.3390, 0.3096], device='cuda:0'), new_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0')
2024-12-03 16:01:35,048 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.3516, 0.3391, 0.3093], device='cuda:0'), new_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0')
2024-12-03 16:01:35,179 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.3519, 0.3391, 0.3090], device='cuda:0'), new_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0')
2024-12-03 16:01:35,307 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.3521, 0.3392, 0.3087], device='cuda:0'), new_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0')
2024-12-03 16:01:35,435 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.3524, 0.3392, 0.3084], device='cuda:0'), new_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0')
2024-12-03 16:01:35,564 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.3526, 0.3393, 0.3081], device='cuda:0'), new_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0')
2024-12-03 16:01:35,693 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.3528, 0.3393, 0.3078], device='cuda:0'), new_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0')
2024-12-03 16:01:35,822 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.3531, 0.3394, 0.3075], device='cuda:0'), new_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0')
2024-12-03 16:01:35,951 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.3533, 0.3394, 0.3073], device='cuda:0'), new_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0')
2024-12-03 16:01:36,074 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.3536, 0.3395, 0.3070], device='cuda:0'), new_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0')
2024-12-03 16:01:36,189 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.3538, 0.3395, 0.3067], device='cuda:0'), new_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0')
2024-12-03 16:01:36,298 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.3541, 0.3396, 0.3064], device='cuda:0'), new_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0')
2024-12-03 16:01:36,396 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.3543, 0.3396, 0.3061], device='cuda:0'), new_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0')
2024-12-03 16:01:36,488 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.3545, 0.3396, 0.3058], device='cuda:0'), new_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0')
2024-12-03 16:01:36,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.3548, 0.3397, 0.3055], device='cuda:0'), new_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0')
2024-12-03 16:01:36,671 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.3550, 0.3397, 0.3052], device='cuda:0'), new_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0')
2024-12-03 16:01:36,762 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.3553, 0.3398, 0.3050], device='cuda:0'), new_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0')
2024-12-03 16:01:36,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.3555, 0.3398, 0.3047], device='cuda:0'), new_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0')
2024-12-03 16:01:36,945 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.3558, 0.3399, 0.3044], device='cuda:0'), new_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0')
2024-12-03 16:01:37,037 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.3560, 0.3399, 0.3041], device='cuda:0'), new_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0')
2024-12-03 16:01:37,128 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.3562, 0.3399, 0.3038], device='cuda:0'), new_distribution = tensor([0.3565, 0.3400, 0.3035], device='cuda:0')
2024-12-03 16:01:37,442 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.7000, 0.2000, 0.1000], device='cuda:0'), new_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0')
2024-12-03 16:01:37,535 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.7003, 0.1997, 0.1000], device='cuda:0'), new_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0')
2024-12-03 16:01:37,626 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.7006, 0.1994, 0.0999], device='cuda:0'), new_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0')
2024-12-03 16:01:37,717 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.7009, 0.1991, 0.0999], device='cuda:0'), new_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0')
2024-12-03 16:01:37,809 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.7013, 0.1988, 0.0999], device='cuda:0'), new_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0')
2024-12-03 16:01:37,900 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.7016, 0.1986, 0.0999], device='cuda:0'), new_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0')
2024-12-03 16:01:37,992 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.7019, 0.1983, 0.0998], device='cuda:0'), new_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0')
2024-12-03 16:01:38,083 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.7022, 0.1980, 0.0998], device='cuda:0'), new_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0')
2024-12-03 16:01:38,174 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.7025, 0.1977, 0.0998], device='cuda:0'), new_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0')
2024-12-03 16:01:38,266 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.7028, 0.1974, 0.0998], device='cuda:0'), new_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0')
2024-12-03 16:01:38,357 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.7031, 0.1971, 0.0997], device='cuda:0'), new_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0')
2024-12-03 16:01:38,448 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.7035, 0.1968, 0.0997], device='cuda:0'), new_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0')
2024-12-03 16:01:38,540 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.7038, 0.1965, 0.0997], device='cuda:0'), new_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0')
2024-12-03 16:01:38,631 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.7041, 0.1963, 0.0997], device='cuda:0'), new_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0')
2024-12-03 16:01:38,724 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.7044, 0.1960, 0.0996], device='cuda:0'), new_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0')
2024-12-03 16:01:38,816 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.7047, 0.1957, 0.0996], device='cuda:0'), new_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0')
2024-12-03 16:01:38,907 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.7050, 0.1954, 0.0996], device='cuda:0'), new_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0')
2024-12-03 16:01:38,999 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.7053, 0.1951, 0.0996], device='cuda:0'), new_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0')
2024-12-03 16:01:39,092 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.7056, 0.1948, 0.0995], device='cuda:0'), new_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0')
2024-12-03 16:01:39,183 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.7059, 0.1945, 0.0995], device='cuda:0'), new_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0')
2024-12-03 16:01:39,277 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.7062, 0.1943, 0.0995], device='cuda:0'), new_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0')
2024-12-03 16:01:39,369 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.7066, 0.1940, 0.0995], device='cuda:0'), new_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0')
2024-12-03 16:01:39,461 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.7069, 0.1937, 0.0994], device='cuda:0'), new_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0')
2024-12-03 16:01:39,552 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.7072, 0.1934, 0.0994], device='cuda:0'), new_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0')
2024-12-03 16:01:39,644 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.7075, 0.1931, 0.0994], device='cuda:0'), new_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0')
2024-12-03 16:01:39,737 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.7078, 0.1928, 0.0994], device='cuda:0'), new_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0')
2024-12-03 16:01:39,828 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.7081, 0.1926, 0.0994], device='cuda:0'), new_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0')
2024-12-03 16:01:39,919 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.7084, 0.1923, 0.0993], device='cuda:0'), new_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0')
2024-12-03 16:01:40,010 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.7087, 0.1920, 0.0993], device='cuda:0'), new_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0')
2024-12-03 16:01:40,102 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.7090, 0.1917, 0.0993], device='cuda:0'), new_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0')
2024-12-03 16:01:40,194 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.7093, 0.1914, 0.0993], device='cuda:0'), new_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0')
2024-12-03 16:01:40,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.7096, 0.1912, 0.0992], device='cuda:0'), new_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0')
2024-12-03 16:01:40,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.7099, 0.1909, 0.0992], device='cuda:0'), new_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0')
2024-12-03 16:01:40,468 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.7102, 0.1906, 0.0992], device='cuda:0'), new_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0')
2024-12-03 16:01:40,559 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.7105, 0.1903, 0.0992], device='cuda:0'), new_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0')
2024-12-03 16:01:40,651 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.7108, 0.1900, 0.0991], device='cuda:0'), new_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0')
2024-12-03 16:01:40,742 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.7111, 0.1897, 0.0991], device='cuda:0'), new_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0')
2024-12-03 16:01:40,834 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.7114, 0.1895, 0.0991], device='cuda:0'), new_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0')
2024-12-03 16:01:40,926 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.7117, 0.1892, 0.0991], device='cuda:0'), new_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0')
2024-12-03 16:01:41,017 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.7120, 0.1889, 0.0991], device='cuda:0'), new_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0')
2024-12-03 16:01:41,108 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.7123, 0.1886, 0.0990], device='cuda:0'), new_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0')
2024-12-03 16:01:41,200 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.7126, 0.1883, 0.0990], device='cuda:0'), new_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0')
2024-12-03 16:01:41,291 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.7129, 0.1881, 0.0990], device='cuda:0'), new_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0')
2024-12-03 16:01:41,382 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.7132, 0.1878, 0.0990], device='cuda:0'), new_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0')
2024-12-03 16:01:41,473 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.7135, 0.1875, 0.0990], device='cuda:0'), new_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0')
2024-12-03 16:01:41,564 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.7138, 0.1872, 0.0989], device='cuda:0'), new_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0')
2024-12-03 16:01:41,656 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.7141, 0.1870, 0.0989], device='cuda:0'), new_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0')
2024-12-03 16:01:41,747 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.7144, 0.1867, 0.0989], device='cuda:0'), new_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0')
2024-12-03 16:01:41,839 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.7147, 0.1864, 0.0989], device='cuda:0'), new_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0')
2024-12-03 16:01:41,930 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.7150, 0.1861, 0.0988], device='cuda:0'), new_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0')
2024-12-03 16:01:42,022 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.7153, 0.1858, 0.0988], device='cuda:0'), new_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0')
2024-12-03 16:01:42,113 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.7156, 0.1856, 0.0988], device='cuda:0'), new_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0')
2024-12-03 16:01:42,204 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.7159, 0.1853, 0.0988], device='cuda:0'), new_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0')
2024-12-03 16:01:42,295 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.7162, 0.1850, 0.0988], device='cuda:0'), new_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0')
2024-12-03 16:01:42,387 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.7165, 0.1847, 0.0987], device='cuda:0'), new_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0')
2024-12-03 16:01:42,478 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.7168, 0.1845, 0.0987], device='cuda:0'), new_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0')
2024-12-03 16:01:42,570 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.7171, 0.1842, 0.0987], device='cuda:0'), new_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0')
2024-12-03 16:01:42,661 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.7174, 0.1839, 0.0987], device='cuda:0'), new_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0')
2024-12-03 16:01:42,752 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.7177, 0.1836, 0.0987], device='cuda:0'), new_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0')
2024-12-03 16:01:42,844 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.7180, 0.1834, 0.0986], device='cuda:0'), new_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0')
2024-12-03 16:01:42,936 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.7183, 0.1831, 0.0986], device='cuda:0'), new_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0')
2024-12-03 16:01:43,027 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.7186, 0.1828, 0.0986], device='cuda:0'), new_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0')
2024-12-03 16:01:43,118 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.7189, 0.1825, 0.0986], device='cuda:0'), new_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0')
2024-12-03 16:01:43,210 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.7192, 0.1823, 0.0986], device='cuda:0'), new_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0')
2024-12-03 16:01:43,302 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.7195, 0.1820, 0.0985], device='cuda:0'), new_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0')
2024-12-03 16:01:43,393 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.7197, 0.1817, 0.0985], device='cuda:0'), new_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0')
2024-12-03 16:01:43,484 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.7200, 0.1815, 0.0985], device='cuda:0'), new_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0')
2024-12-03 16:01:43,575 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.7203, 0.1812, 0.0985], device='cuda:0'), new_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0')
2024-12-03 16:01:43,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.7206, 0.1809, 0.0985], device='cuda:0'), new_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0')
2024-12-03 16:01:43,758 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.7209, 0.1806, 0.0985], device='cuda:0'), new_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0')
2024-12-03 16:01:43,850 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.7212, 0.1804, 0.0984], device='cuda:0'), new_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0')
2024-12-03 16:01:43,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.7215, 0.1801, 0.0984], device='cuda:0'), new_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0')
2024-12-03 16:01:44,032 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.7218, 0.1798, 0.0984], device='cuda:0'), new_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0')
2024-12-03 16:01:44,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.7221, 0.1795, 0.0984], device='cuda:0'), new_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0')
2024-12-03 16:01:44,216 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.7224, 0.1793, 0.0984], device='cuda:0'), new_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0')
2024-12-03 16:01:44,307 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.7226, 0.1790, 0.0983], device='cuda:0'), new_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0')
2024-12-03 16:01:44,399 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.7229, 0.1787, 0.0983], device='cuda:0'), new_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0')
2024-12-03 16:01:44,490 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.7232, 0.1785, 0.0983], device='cuda:0'), new_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0')
2024-12-03 16:01:44,582 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.7235, 0.1782, 0.0983], device='cuda:0'), new_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0')
2024-12-03 16:01:44,673 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.7238, 0.1779, 0.0983], device='cuda:0'), new_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0')
2024-12-03 16:01:44,764 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.7241, 0.1777, 0.0983], device='cuda:0'), new_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0')
2024-12-03 16:01:44,855 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.7244, 0.1774, 0.0982], device='cuda:0'), new_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0')
2024-12-03 16:01:44,944 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.7247, 0.1771, 0.0982], device='cuda:0'), new_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0')
2024-12-03 16:01:45,035 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.7249, 0.1769, 0.0982], device='cuda:0'), new_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0')
2024-12-03 16:01:45,127 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.7252, 0.1766, 0.0982], device='cuda:0'), new_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0')
2024-12-03 16:01:45,218 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.7255, 0.1763, 0.0982], device='cuda:0'), new_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0')
2024-12-03 16:01:45,310 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.7258, 0.1761, 0.0982], device='cuda:0'), new_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0')
2024-12-03 16:01:45,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.7261, 0.1758, 0.0981], device='cuda:0'), new_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0')
2024-12-03 16:01:45,493 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.7264, 0.1755, 0.0981], device='cuda:0'), new_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0')
2024-12-03 16:01:45,584 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.7266, 0.1753, 0.0981], device='cuda:0'), new_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0')
2024-12-03 16:01:45,675 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.7269, 0.1750, 0.0981], device='cuda:0'), new_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0')
2024-12-03 16:01:45,766 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.7272, 0.1747, 0.0981], device='cuda:0'), new_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0')
2024-12-03 16:01:45,858 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.7275, 0.1745, 0.0981], device='cuda:0'), new_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0')
2024-12-03 16:01:45,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.7278, 0.1742, 0.0980], device='cuda:0'), new_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0')
2024-12-03 16:01:46,041 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.7280, 0.1739, 0.0980], device='cuda:0'), new_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0')
2024-12-03 16:01:46,132 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.7283, 0.1737, 0.0980], device='cuda:0'), new_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0')
2024-12-03 16:01:46,223 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.7286, 0.1734, 0.0980], device='cuda:0'), new_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0')
2024-12-03 16:01:46,314 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.7289, 0.1731, 0.0980], device='cuda:0'), new_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0')
2024-12-03 16:01:46,405 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.7292, 0.1729, 0.0980], device='cuda:0'), new_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0')
2024-12-03 16:01:46,496 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.7294, 0.1726, 0.0980], device='cuda:0'), new_distribution = tensor([0.7297, 0.1723, 0.0979], device='cuda:0')
2024-12-03 16:01:46,819 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:46,911 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,002 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,093 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,184 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,275 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,367 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,458 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,550 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,641 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,733 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,827 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:47,918 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,010 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,101 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,195 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,286 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,467 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,555 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,647 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,738 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,829 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:48,920 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,012 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,103 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,194 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,285 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,376 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,467 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,558 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,649 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,741 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,833 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:49,924 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,015 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,107 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,199 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,290 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,382 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,477 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,570 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,662 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,753 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,845 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:50,936 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,027 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,118 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,209 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,300 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,391 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,483 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,574 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,665 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,757 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,848 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:51,939 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,031 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,122 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,214 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,305 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,488 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,579 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,668 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,759 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,850 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:52,941 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,032 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,124 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,215 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,306 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,397 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,489 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,580 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,672 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,763 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,854 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:53,945 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,037 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,128 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,219 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,310 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,493 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,584 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,675 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,767 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,859 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:54,948 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,039 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,130 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,222 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,312 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,403 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,494 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,586 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,678 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,769 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:55,860 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([1., 0., 0.], device='cuda:0'), new_distribution = tensor([1., 0., 0.], device='cuda:0')
2024-12-03 16:01:56,190 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 0: ref_distribution = tensor([0.1000, 0.3000, 0.6000], device='cuda:0'), new_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0')
2024-12-03 16:01:56,283 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 1: ref_distribution = tensor([0.1000, 0.3005, 0.5994], device='cuda:0'), new_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0')
2024-12-03 16:01:56,374 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 2: ref_distribution = tensor([0.1001, 0.3011, 0.5989], device='cuda:0'), new_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0')
2024-12-03 16:01:56,465 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 3: ref_distribution = tensor([0.1001, 0.3016, 0.5983], device='cuda:0'), new_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0')
2024-12-03 16:01:56,557 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 4: ref_distribution = tensor([0.1002, 0.3021, 0.5977], device='cuda:0'), new_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0')
2024-12-03 16:01:56,648 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 5: ref_distribution = tensor([0.1002, 0.3027, 0.5971], device='cuda:0'), new_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0')
2024-12-03 16:01:56,740 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 6: ref_distribution = tensor([0.1002, 0.3032, 0.5966], device='cuda:0'), new_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0')
2024-12-03 16:01:56,831 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 7: ref_distribution = tensor([0.1003, 0.3037, 0.5960], device='cuda:0'), new_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0')
2024-12-03 16:01:56,922 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 8: ref_distribution = tensor([0.1003, 0.3043, 0.5954], device='cuda:0'), new_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0')
2024-12-03 16:01:57,014 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 9: ref_distribution = tensor([0.1004, 0.3048, 0.5948], device='cuda:0'), new_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0')
2024-12-03 16:01:57,105 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 10: ref_distribution = tensor([0.1004, 0.3053, 0.5942], device='cuda:0'), new_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0')
2024-12-03 16:01:57,196 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 11: ref_distribution = tensor([0.1005, 0.3059, 0.5937], device='cuda:0'), new_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0')
2024-12-03 16:01:57,288 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 12: ref_distribution = tensor([0.1005, 0.3064, 0.5931], device='cuda:0'), new_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0')
2024-12-03 16:01:57,379 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 13: ref_distribution = tensor([0.1005, 0.3069, 0.5925], device='cuda:0'), new_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0')
2024-12-03 16:01:57,470 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 14: ref_distribution = tensor([0.1006, 0.3075, 0.5919], device='cuda:0'), new_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0')
2024-12-03 16:01:57,562 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 15: ref_distribution = tensor([0.1006, 0.3080, 0.5913], device='cuda:0'), new_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0')
2024-12-03 16:01:57,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 16: ref_distribution = tensor([0.1007, 0.3086, 0.5908], device='cuda:0'), new_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0')
2024-12-03 16:01:57,744 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 17: ref_distribution = tensor([0.1007, 0.3091, 0.5902], device='cuda:0'), new_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0')
2024-12-03 16:01:57,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 18: ref_distribution = tensor([0.1008, 0.3096, 0.5896], device='cuda:0'), new_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0')
2024-12-03 16:01:57,927 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 19: ref_distribution = tensor([0.1008, 0.3102, 0.5890], device='cuda:0'), new_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0')
2024-12-03 16:01:58,020 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 20: ref_distribution = tensor([0.1009, 0.3107, 0.5884], device='cuda:0'), new_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0')
2024-12-03 16:01:58,109 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 21: ref_distribution = tensor([0.1009, 0.3112, 0.5879], device='cuda:0'), new_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0')
2024-12-03 16:01:58,201 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 22: ref_distribution = tensor([0.1009, 0.3118, 0.5873], device='cuda:0'), new_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0')
2024-12-03 16:01:58,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 23: ref_distribution = tensor([0.1010, 0.3123, 0.5867], device='cuda:0'), new_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0')
2024-12-03 16:01:58,382 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 24: ref_distribution = tensor([0.1010, 0.3129, 0.5861], device='cuda:0'), new_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0')
2024-12-03 16:01:58,473 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 25: ref_distribution = tensor([0.1011, 0.3134, 0.5855], device='cuda:0'), new_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0')
2024-12-03 16:01:58,562 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 26: ref_distribution = tensor([0.1011, 0.3139, 0.5849], device='cuda:0'), new_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0')
2024-12-03 16:01:58,653 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 27: ref_distribution = tensor([0.1012, 0.3145, 0.5843], device='cuda:0'), new_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0')
2024-12-03 16:01:58,745 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 28: ref_distribution = tensor([0.1012, 0.3150, 0.5838], device='cuda:0'), new_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0')
2024-12-03 16:01:58,836 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 29: ref_distribution = tensor([0.1013, 0.3156, 0.5832], device='cuda:0'), new_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0')
2024-12-03 16:01:58,927 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 30: ref_distribution = tensor([0.1013, 0.3161, 0.5826], device='cuda:0'), new_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0')
2024-12-03 16:01:59,019 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 31: ref_distribution = tensor([0.1014, 0.3166, 0.5820], device='cuda:0'), new_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0')
2024-12-03 16:01:59,111 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 32: ref_distribution = tensor([0.1014, 0.3172, 0.5814], device='cuda:0'), new_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0')
2024-12-03 16:01:59,202 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 33: ref_distribution = tensor([0.1015, 0.3177, 0.5808], device='cuda:0'), new_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0')
2024-12-03 16:01:59,293 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 34: ref_distribution = tensor([0.1015, 0.3183, 0.5802], device='cuda:0'), new_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0')
2024-12-03 16:01:59,385 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 35: ref_distribution = tensor([0.1016, 0.3188, 0.5796], device='cuda:0'), new_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0')
2024-12-03 16:01:59,476 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 36: ref_distribution = tensor([0.1016, 0.3193, 0.5791], device='cuda:0'), new_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0')
2024-12-03 16:01:59,567 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 37: ref_distribution = tensor([0.1016, 0.3199, 0.5785], device='cuda:0'), new_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0')
2024-12-03 16:01:59,659 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 38: ref_distribution = tensor([0.1017, 0.3204, 0.5779], device='cuda:0'), new_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0')
2024-12-03 16:01:59,750 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 39: ref_distribution = tensor([0.1017, 0.3210, 0.5773], device='cuda:0'), new_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0')
2024-12-03 16:01:59,842 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 40: ref_distribution = tensor([0.1018, 0.3215, 0.5767], device='cuda:0'), new_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0')
2024-12-03 16:01:59,936 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 41: ref_distribution = tensor([0.1018, 0.3221, 0.5761], device='cuda:0'), new_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0')
2024-12-03 16:02:00,027 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 42: ref_distribution = tensor([0.1019, 0.3226, 0.5755], device='cuda:0'), new_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0')
2024-12-03 16:02:00,118 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 43: ref_distribution = tensor([0.1019, 0.3231, 0.5749], device='cuda:0'), new_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0')
2024-12-03 16:02:00,210 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 44: ref_distribution = tensor([0.1020, 0.3237, 0.5743], device='cuda:0'), new_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0')
2024-12-03 16:02:00,301 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 45: ref_distribution = tensor([0.1020, 0.3242, 0.5737], device='cuda:0'), new_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0')
2024-12-03 16:02:00,392 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 46: ref_distribution = tensor([0.1021, 0.3248, 0.5731], device='cuda:0'), new_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0')
2024-12-03 16:02:00,484 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 47: ref_distribution = tensor([0.1021, 0.3253, 0.5725], device='cuda:0'), new_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0')
2024-12-03 16:02:00,575 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 48: ref_distribution = tensor([0.1022, 0.3259, 0.5719], device='cuda:0'), new_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0')
2024-12-03 16:02:00,667 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 49: ref_distribution = tensor([0.1022, 0.3264, 0.5713], device='cuda:0'), new_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0')
2024-12-03 16:02:00,759 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 50: ref_distribution = tensor([0.1023, 0.3270, 0.5707], device='cuda:0'), new_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0')
2024-12-03 16:02:00,851 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 51: ref_distribution = tensor([0.1023, 0.3275, 0.5702], device='cuda:0'), new_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0')
2024-12-03 16:02:00,942 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 52: ref_distribution = tensor([0.1024, 0.3281, 0.5696], device='cuda:0'), new_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0')
2024-12-03 16:02:01,034 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 53: ref_distribution = tensor([0.1024, 0.3286, 0.5690], device='cuda:0'), new_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0')
2024-12-03 16:02:01,125 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 54: ref_distribution = tensor([0.1025, 0.3291, 0.5684], device='cuda:0'), new_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0')
2024-12-03 16:02:01,217 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 55: ref_distribution = tensor([0.1025, 0.3297, 0.5678], device='cuda:0'), new_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0')
2024-12-03 16:02:01,309 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 56: ref_distribution = tensor([0.1026, 0.3302, 0.5672], device='cuda:0'), new_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0')
2024-12-03 16:02:01,401 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 57: ref_distribution = tensor([0.1027, 0.3308, 0.5666], device='cuda:0'), new_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0')
2024-12-03 16:02:01,492 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 58: ref_distribution = tensor([0.1027, 0.3313, 0.5660], device='cuda:0'), new_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0')
2024-12-03 16:02:01,583 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 59: ref_distribution = tensor([0.1028, 0.3319, 0.5654], device='cuda:0'), new_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0')
2024-12-03 16:02:01,674 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 60: ref_distribution = tensor([0.1028, 0.3324, 0.5648], device='cuda:0'), new_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0')
2024-12-03 16:02:01,766 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 61: ref_distribution = tensor([0.1029, 0.3330, 0.5642], device='cuda:0'), new_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0')
2024-12-03 16:02:01,857 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 62: ref_distribution = tensor([0.1029, 0.3335, 0.5636], device='cuda:0'), new_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0')
2024-12-03 16:02:01,949 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 63: ref_distribution = tensor([0.1030, 0.3341, 0.5630], device='cuda:0'), new_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0')
2024-12-03 16:02:02,041 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 64: ref_distribution = tensor([0.1030, 0.3346, 0.5624], device='cuda:0'), new_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0')
2024-12-03 16:02:02,132 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 65: ref_distribution = tensor([0.1031, 0.3352, 0.5618], device='cuda:0'), new_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0')
2024-12-03 16:02:02,224 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 66: ref_distribution = tensor([0.1031, 0.3357, 0.5612], device='cuda:0'), new_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0')
2024-12-03 16:02:02,315 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 67: ref_distribution = tensor([0.1032, 0.3363, 0.5606], device='cuda:0'), new_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0')
2024-12-03 16:02:02,408 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 68: ref_distribution = tensor([0.1032, 0.3368, 0.5600], device='cuda:0'), new_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0')
2024-12-03 16:02:02,505 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 69: ref_distribution = tensor([0.1033, 0.3374, 0.5593], device='cuda:0'), new_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0')
2024-12-03 16:02:02,597 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 70: ref_distribution = tensor([0.1033, 0.3379, 0.5587], device='cuda:0'), new_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0')
2024-12-03 16:02:02,688 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 71: ref_distribution = tensor([0.1034, 0.3385, 0.5581], device='cuda:0'), new_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0')
2024-12-03 16:02:02,780 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 72: ref_distribution = tensor([0.1035, 0.3390, 0.5575], device='cuda:0'), new_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0')
2024-12-03 16:02:02,871 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 73: ref_distribution = tensor([0.1035, 0.3396, 0.5569], device='cuda:0'), new_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0')
2024-12-03 16:02:02,963 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 74: ref_distribution = tensor([0.1036, 0.3401, 0.5563], device='cuda:0'), new_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0')
2024-12-03 16:02:03,054 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 75: ref_distribution = tensor([0.1036, 0.3407, 0.5557], device='cuda:0'), new_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0')
2024-12-03 16:02:03,146 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 76: ref_distribution = tensor([0.1037, 0.3412, 0.5551], device='cuda:0'), new_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0')
2024-12-03 16:02:03,237 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 77: ref_distribution = tensor([0.1037, 0.3418, 0.5545], device='cuda:0'), new_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0')
2024-12-03 16:02:03,330 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 78: ref_distribution = tensor([0.1038, 0.3423, 0.5539], device='cuda:0'), new_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0')
2024-12-03 16:02:03,422 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 79: ref_distribution = tensor([0.1038, 0.3429, 0.5533], device='cuda:0'), new_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0')
2024-12-03 16:02:03,513 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 80: ref_distribution = tensor([0.1039, 0.3434, 0.5527], device='cuda:0'), new_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0')
2024-12-03 16:02:03,607 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 81: ref_distribution = tensor([0.1040, 0.3440, 0.5521], device='cuda:0'), new_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0')
2024-12-03 16:02:03,698 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 82: ref_distribution = tensor([0.1040, 0.3445, 0.5515], device='cuda:0'), new_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0')
2024-12-03 16:02:03,790 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 83: ref_distribution = tensor([0.1041, 0.3451, 0.5509], device='cuda:0'), new_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0')
2024-12-03 16:02:03,881 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 84: ref_distribution = tensor([0.1041, 0.3456, 0.5503], device='cuda:0'), new_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0')
2024-12-03 16:02:03,973 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 85: ref_distribution = tensor([0.1042, 0.3462, 0.5496], device='cuda:0'), new_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0')
2024-12-03 16:02:04,064 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 86: ref_distribution = tensor([0.1043, 0.3467, 0.5490], device='cuda:0'), new_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0')
2024-12-03 16:02:04,156 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 87: ref_distribution = tensor([0.1043, 0.3473, 0.5484], device='cuda:0'), new_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0')
2024-12-03 16:02:04,248 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 88: ref_distribution = tensor([0.1044, 0.3478, 0.5478], device='cuda:0'), new_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0')
2024-12-03 16:02:04,339 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 89: ref_distribution = tensor([0.1044, 0.3484, 0.5472], device='cuda:0'), new_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0')
2024-12-03 16:02:04,430 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 90: ref_distribution = tensor([0.1045, 0.3489, 0.5466], device='cuda:0'), new_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0')
2024-12-03 16:02:04,522 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 91: ref_distribution = tensor([0.1045, 0.3495, 0.5460], device='cuda:0'), new_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0')
2024-12-03 16:02:04,614 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 92: ref_distribution = tensor([0.1046, 0.3500, 0.5454], device='cuda:0'), new_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0')
2024-12-03 16:02:04,705 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 93: ref_distribution = tensor([0.1047, 0.3506, 0.5448], device='cuda:0'), new_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0')
2024-12-03 16:02:04,797 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 94: ref_distribution = tensor([0.1047, 0.3511, 0.5441], device='cuda:0'), new_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0')
2024-12-03 16:02:04,891 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 95: ref_distribution = tensor([0.1048, 0.3517, 0.5435], device='cuda:0'), new_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0')
2024-12-03 16:02:04,984 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 96: ref_distribution = tensor([0.1048, 0.3522, 0.5429], device='cuda:0'), new_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0')
2024-12-03 16:02:05,075 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 97: ref_distribution = tensor([0.1049, 0.3528, 0.5423], device='cuda:0'), new_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0')
2024-12-03 16:02:05,167 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 98: ref_distribution = tensor([0.1050, 0.3533, 0.5417], device='cuda:0'), new_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0')
2024-12-03 16:02:05,258 - /home/hanwen/policy_optimization/exp/algorithm_single_state.py[line:537] - INFO: Iteration 99: ref_distribution = tensor([0.1050, 0.3539, 0.5411], device='cuda:0'), new_distribution = tensor([0.1051, 0.3544, 0.5405], device='cuda:0')
